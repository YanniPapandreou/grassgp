{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c39d4f-889c-4cc8-a817-83552554ffb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPSR - Cylinder Example\n",
    "\n",
    "!!! change `compute_barycenter` to newer version !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4ca4b-c1d9-4c0b-a6ae-5e70d0d969d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax\n",
    "# jax.config.update('jax_array', False)\n",
    "# # again, this only works on startup!\n",
    "# from jax.config import config\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import time\n",
    "from hydra_zen import instantiate, make_config, builds\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import random, vmap\n",
    "import jax.numpy.linalg as lin\n",
    "\n",
    "from dataclasses import field\n",
    "from typing import Callable, Tuple\n",
    "import chex\n",
    "from chex import assert_shape, assert_rank\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import numpyro.handlers as handlers\n",
    "from numpyro.infer import SVI, Trace_ELBO, autoguide, init_to_value\n",
    "\n",
    "from grassgp.inference import run_inference\n",
    "\n",
    "from grassgp.utils import get_save_path, subspace_angle, to_dictconf, unvec, vec, kron_chol\n",
    "# from grassgp.utils import safe_save_jax_array_dict as safe_save\n",
    "# from grassgp.utils import load_and_convert_to_samples_dict as load_data\n",
    "from grassgp.grassmann import valid_grass_point, convert_to_projs, grass_log, grass_exp, valid_grass_tangent, grass_dist\n",
    "from grassgp.kernels import rbf\n",
    "# from grassgp.models import GrassGP\n",
    "# from grassgp.models_optimised import MatGP as MatGP_optimised\n",
    "# from grassgp.models import MatGP\n",
    "from grassgp.means import zero_mean\n",
    "from grassgp.plot_utils import flatten_samples, plot_grass_dists, plot_AS_dir_preds\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218d655-8d8b-4181-8305-b4416add1e08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_svi_for_map(rng_key, model, maxiter, step_size, *args):\n",
    "    start = time.time()\n",
    "    guide = autoguide.AutoDelta(model)\n",
    "    optimzer = numpyro.optim.Adam(step_size)\n",
    "    svi = SVI(model, guide, optimzer, Trace_ELBO())\n",
    "    svi_results = svi.run(rng_key, maxiter, *args)\n",
    "    print('\\nSVI elapsed time:', time.time() - start)\n",
    "    return svi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6c757-3924-4164-a062-de6d1fa2e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class MatGP:\n",
    "    d_in: int\n",
    "    d_out: Tuple[int, int]\n",
    "    mu: Callable = field(repr=False)\n",
    "    k: Callable = field(repr=False)\n",
    "    Omega: chex.ArrayDevice = field(repr=False)\n",
    "    cov_jitter: float = field(default=1e-8, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_shape(self.Omega, (d_n, d_n),\n",
    "                    custom_message=f\"Omega has shape {self.Omega.shape}; expected shape {(d_n, d_n)}\")\n",
    "\n",
    "    def model(self, s: chex.ArrayDevice, use_kron_chol: bool = True) -> chex.ArrayDevice:\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_rank(s, self.d_in)\n",
    "        N = s.shape[0]\n",
    "\n",
    "        # compute mean matrix M = [mu(s[1]), mu(s[2]), ..., mu(s[N])]\n",
    "        M = np.hstack(vmap(self.mu)(s))\n",
    "        assert_shape(M, (d, n*N))\n",
    "\n",
    "        # compute kernel matrix\n",
    "        K = self.k(s, s)\n",
    "        assert_shape(K, (N, N))\n",
    "\n",
    "        # compute covariance matrix and cholesky factor\n",
    "        if use_kron_chol:\n",
    "            Chol = kron_chol(K + self.cov_jitter * np.eye(N), self.Omega)\n",
    "        else:\n",
    "            Cov = np.kron(K + self.cov_jitter * np.eye(N), self.Omega)\n",
    "            Chol = lin.cholesky(Cov)\n",
    "\n",
    "        # sample vec_Vs\n",
    "        # Z = numpyro.sample(\"Z\", dist.MultivariateNormal(covariance_matrix=np.eye(N*d_n)))\n",
    "        Z = numpyro.sample(\"Z\", dist.Normal().expand([N*d_n]))\n",
    "        vec_Vs = numpyro.deterministic(\"vec_Vs\", vec(M) + Chol @ Z)\n",
    "\n",
    "        # form Vs\n",
    "        Vs = numpyro.deterministic(\"Vs\", vmap(lambda params: unvec(params, d, n))(np.array(vec_Vs.split(N))))\n",
    "        return Vs\n",
    "\n",
    "    def sample(self, seed: int, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        model = self.model\n",
    "        seeded_model = handlers.seed(model, rng_seed=seed)\n",
    "        return seeded_model(s)\n",
    "\n",
    "    def predict(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        d, n = self.d_out\n",
    "        d_in = self.d_in\n",
    "        d_n = d * n\n",
    "        N_train = s_train.shape[0]\n",
    "        N_test = s_test.shape[0]\n",
    "        if d_in > 1:\n",
    "            assert s_train.shape[1] == d_in\n",
    "            assert s_test.shape[1] == d_in\n",
    "\n",
    "        # compute means\n",
    "        M_train = np.hstack(vmap(self.mu)(s_train))\n",
    "        M_test = np.hstack(vmap(self.mu)(s_test))\n",
    "        assert_shape(M_train, (d, n*N_train))\n",
    "        assert_shape(M_test, (d, n*N_test))\n",
    "\n",
    "        # compute kernels between train and test locs\n",
    "        K_train_train = self.k(s_train, s_train)\n",
    "        assert_shape(K_train_train, (N_train, N_train))\n",
    "        K_train_test = self.k(s_train, s_test)\n",
    "        assert_shape(K_train_test, (N_train, N_test))\n",
    "        K_test_train = K_train_test.T\n",
    "        K_test_test = self.k(s_test, s_test)\n",
    "        assert_shape(K_test_test, (N_test, N_test))\n",
    "\n",
    "        # compute posterior mean and cov\n",
    "        K_test_train_Omega = np.kron(K_test_train, self.Omega)\n",
    "        K_train_test_Omega = np.kron(K_train_test, self.Omega)\n",
    "        K_test_test_Omega = np.kron(K_test_test, self.Omega)\n",
    "        # FIX: change for singular Omega\n",
    "        # print(f\"Rank of Omega = {lin.matrix_rank(self.Omega)}. Shape of Omega = {self.Omega.shape}\")\n",
    "        # if lin.matrix_rank(self.Omega) == d_n:\n",
    "        #     mean_sols = kron_solve(K_train_train, self.Omega, vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "        #     cov_sols = vmap(lambda v: kron_solve(K_train_train, self.Omega, v), in_axes=1, out_axes=1)(K_train_test_Omega)\n",
    "        # else:\n",
    "        #     K_train_train_inv = lin.inv(K_train_train)\n",
    "        #     Omega_pinv = lin.pinv(self.Omega)\n",
    "        #     K_train_train_Omega_pinv = np.kron(K_train_train_inv, Omega_pinv)\n",
    "        #     mean_sols = K_train_train_Omega_pinv @ (vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "        #     cov_sols = K_train_train_Omega_pinv @ K_train_test_Omega\n",
    "        K_train_train_inv = lin.inv(K_train_train)\n",
    "        Omega_pinv = lin.pinv(self.Omega)\n",
    "        K_train_train_Omega_pinv = np.kron(K_train_train_inv, Omega_pinv)\n",
    "        mean_sols = K_train_train_Omega_pinv @ (vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "        cov_sols = K_train_train_Omega_pinv @ K_train_test_Omega\n",
    "        \n",
    "        vec_post_mean = vec(M_test) + K_test_train_Omega @ mean_sols\n",
    "        assert_shape(vec_post_mean, (d*n*N_test,),\n",
    "                     custom_message=f\"vec_post_mean should have shape {(d*n*N_test,)}; obtained {vec_post_mean.shape}\")\n",
    "\n",
    "        # cov_sols = vmap(lambda v: kron_solve(K_train_train, self.Omega, v), in_axes=1, out_axes=1)(K_train_test_Omega)\n",
    "        post_cov = K_test_test_Omega - K_test_train_Omega @ cov_sols\n",
    "        assert_shape(post_cov, (d*n*N_test, d*n*N_test),\n",
    "                     custom_message=f\"post_cov should have shape {(d*n*N_test,d*n*N_test)}; obtained {post_cov.shape}\")\n",
    "\n",
    "        # sample predictions\n",
    "        post_cov += jitter * np.eye(d*n*N_test)\n",
    "        \n",
    "        # FIX: change for singular post_cov\n",
    "        # print(f\"Rank of posterior cov = {lin.matrix_rank(post_cov)}. Shape of posterior cov = {post_cov.shape}\")\n",
    "        vec_pred = dist.MultivariateNormal(loc=vec_post_mean, covariance_matrix=post_cov).sample(key)\n",
    "        assert_shape(vec_pred, (d*n*N_test,),\n",
    "                     custom_message=f\"vec_pred should have shape {(d*n*N_test,)}; obtained {vec_pred.shape}\")\n",
    "\n",
    "        # unvec mean and preds and return\n",
    "        post_mean = vmap(lambda params: unvec(params, d, n))(np.array(vec_post_mean.split(N_test)))\n",
    "        pred = vmap(lambda params: unvec(params, d, n))(np.array(vec_pred.split(N_test)))\n",
    "        return post_mean, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac80efe0-9856-4fc3-9e3f-f814b6af41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @chex.dataclass\n",
    "# class MatGP_optimised:\n",
    "#     d_in: int\n",
    "#     d_out: Tuple[int, int]\n",
    "#     mu: Callable = field(repr=False)\n",
    "#     k: Callable = field(repr=False)\n",
    "#     Omega_diag_chol: chex.ArrayDevice = field(repr=False)\n",
    "#     cov_jitter: float = field(default=1e-8, repr=False)\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         d, n = self.d_out\n",
    "#         d_n = d * n\n",
    "#         assert_shape(self.Omega_diag_chol, (d_n,),\n",
    "#                     custom_message=f\"Omega_diag_chol has shape {self.Omega_diag_chol.shape}; expected shape {(d_n,)}\")\n",
    "\n",
    "#     def model(self, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "#         d, n = self.d_out\n",
    "#         d_n = d * n\n",
    "#         assert_rank(s, self.d_in)\n",
    "#         N = s.shape[0]\n",
    "\n",
    "#         # compute mean matrix M = [mu(s[1]), mu(s[2]), ..., mu(s[N])]\n",
    "#         M = np.hstack(vmap(self.mu)(s))\n",
    "#         assert_shape(M, (d, n*N))\n",
    "#         # ! TODO: check this out\n",
    "#         vec_M = vec(M)\n",
    "\n",
    "#         # compute kernel matrix\n",
    "#         K = self.k(s, s)\n",
    "#         assert_shape(K, (N, N))\n",
    "            \n",
    "#         K_chol = lin.cholesky(K + self.cov_jitter * np.eye(N))\n",
    "#         # Omega_diag_chol = np.sqrt(self.Omega_diag)\n",
    "\n",
    "#         # sample vec_Vs\n",
    "#         # Z = numpyro.sample(\"Z\", dist.MultivariateNormal(covariance_matrix=np.eye(N*d_n)))\n",
    "#         Z = numpyro.sample(\"Z\", dist.Normal().expand([N*d_n]))\n",
    "#         unvec_Z = unvec(Z, d_n, N)\n",
    "#         # vec_Vs = numpyro.deterministic(\"vec_Vs\", vec(M + np.einsum('i,ij->ij', self.Omega_diag_chol, unvec_Z @ K_chol.T)))\n",
    "#         vec_Vs = numpyro.deterministic(\"vec_Vs\", vec_M + vec(np.einsum('i,ij->ij', self.Omega_diag_chol, unvec_Z @ K_chol.T)))\n",
    "\n",
    "#         # form Vs\n",
    "#         Vs = numpyro.deterministic(\"Vs\", vmap(lambda params: unvec(params, d, n))(np.array(vec_Vs.split(N))))\n",
    "#         return Vs\n",
    "\n",
    "#     def sample(self, seed: int, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "#         model = self.model\n",
    "#         seeded_model = handlers.seed(model, rng_seed=seed)\n",
    "#         return seeded_model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa607a-8c70-4ea8-9cf7-8b8486329b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @chex.dataclass\n",
    "# class GrassGP:\n",
    "#     d_in: int\n",
    "#     d_out: Tuple[int, int]\n",
    "#     mu: Callable = field(repr=False)\n",
    "#     k: Callable = field(repr=False)\n",
    "#     Omega_diag_chol: chex.ArrayDevice = field(repr=False)\n",
    "#     U: chex.ArrayDevice\n",
    "#     cov_jitter: float = field(default=1e-4, repr=False)\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         d, n = self.d_out\n",
    "#         d_n = d * n\n",
    "#         assert_shape(self.Omega_diag_chol, (d_n,),\n",
    "#                     custom_message=f\"Omega_diag_chol has shape {self.Omega_diag_chol.shape}; expected shape {(d_n,)}\")\n",
    "#         assert_shape(self.U, (d, n),\n",
    "#                     custom_message=f\"U has shape {self.U.shape}; expected shape {(d, n)}\")\n",
    "#         tol = 1e-06\n",
    "#         # assert valid_grass_point(self.U), f\"U is not a valid point on Grassmann manifold G({d},{n}) at tolerance level {tol = }\"\n",
    "\n",
    "#     @property\n",
    "#     def V(self) -> MatGP:\n",
    "#         mat_gp = MatGP_optimised(d_in=self.d_in, d_out=self.d_out, mu=self.mu, k=self.k, Omega_diag_chol=self.Omega_diag_chol, cov_jitter=self.cov_jitter)\n",
    "#         return mat_gp\n",
    "\n",
    "#     def tangent_model(self, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "#         d, n = self.d_out\n",
    "#         N = s.shape[0]\n",
    "#         Vs = self.V.model(s)\n",
    "#         I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "#         Deltas = numpyro.deterministic(\"Deltas\", np.einsum('ij,ljk->lik', I_UUT, Vs))\n",
    "#         assert_shape(Deltas, (N, d, n),\n",
    "#                     custom_message=f\"Deltas has shape {Deltas.shape}; expected shape {(N, d, n)}\")\n",
    "#         return Deltas\n",
    "\n",
    "#     def sample_tangents(self, seed: int, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "#         tangent_model = self.tangent_model\n",
    "#         seeded_model = handlers.seed(tangent_model, rng_seed=seed)\n",
    "#         Deltas = seeded_model(s)\n",
    "#         assert vmap(lambda Delta: valid_grass_tangent(self.U, Delta))(Deltas).all()\n",
    "#         return Deltas\n",
    "\n",
    "#     def sample_grass(self, seed: int, s: chex.ArrayDevice, reortho: bool = False) -> chex.ArrayDevice:\n",
    "#         Deltas = self.sample_tangents(seed, s)\n",
    "#         Ws = convert_to_projs(Deltas, self.U, reorthonormalize=reortho)\n",
    "#         return Ws\n",
    "\n",
    "#     def predict_tangents(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "#         d, _ = self.d_out\n",
    "#         I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "#         V_mu = lambda s: I_UUT @ self.mu(s)\n",
    "#         V_Omega = I_UUT @ np.diag(self.Omega_diag_chol) @ I_UUT.T\n",
    "#         V = MatGP(d_in=self.d_in, d_out=self.d_out, mu=V_mu, k=self.k, Omega=V_Omega, cov_jitter=self.cov_jitter)\n",
    "#         Deltas_mean, Deltas_pred = V.predict(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "#         return Deltas_mean, Deltas_pred\n",
    "\n",
    "#     def predict_grass(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8, reortho: bool = False) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "#         Deltas_mean, Deltas_pred = self.predict_tangents(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "#         Ws_mean = convert_to_projs(Deltas_mean, self.U, reorthonormalize=reortho)\n",
    "#         Ws_pred = convert_to_projs(Deltas_pred, self.U, reorthonormalize=reortho)\n",
    "#         return Ws_mean, Ws_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff75f2-8682-45b0-b96a-d1580e7aa80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class GrassGP:\n",
    "    d_in: int\n",
    "    d_out: Tuple[int, int]\n",
    "    mu: Callable = field(repr=False)\n",
    "    k: Callable = field(repr=False)\n",
    "    Omega: chex.ArrayDevice = field(repr=False)\n",
    "    U: chex.ArrayDevice\n",
    "    cov_jitter: float = field(default=1e-4, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_shape(self.Omega, (d_n, d_n),\n",
    "                    custom_message=f\"Omega has shape {self.Omega.shape}; expected shape {(d_n, d_n)}\")\n",
    "        assert_shape(self.U, (d, n),\n",
    "                    custom_message=f\"U has shape {self.U.shape}; expected shape {(d, n)}\")\n",
    "        tol = 1e-06\n",
    "        # assert valid_grass_point(self.U), f\"U is not a valid point on Grassmann manifold G({d},{n}) at tolerance level {tol = }\"\n",
    "\n",
    "    @property\n",
    "    def V(self) -> MatGP:\n",
    "        mat_gp = MatGP(d_in=self.d_in, d_out=self.d_out, mu=self.mu, k=self.k, Omega=self.Omega, cov_jitter=self.cov_jitter)\n",
    "        return mat_gp\n",
    "\n",
    "    def tangent_model(self, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        d, n = self.d_out\n",
    "        N = s.shape[0]\n",
    "        Vs = self.V.model(s)\n",
    "        I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "        Deltas = numpyro.deterministic(\"Deltas\", np.einsum('ij,ljk->lik', I_UUT, Vs))\n",
    "        assert_shape(Deltas, (N, d, n),\n",
    "                    custom_message=f\"Deltas has shape {Deltas.shape}; expected shape {(N, d, n)}\")\n",
    "        return Deltas\n",
    "\n",
    "    def sample_tangents(self, seed: int, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        tangent_model = self.tangent_model\n",
    "        seeded_model = handlers.seed(tangent_model, rng_seed=seed)\n",
    "        Deltas = seeded_model(s)\n",
    "        assert vmap(lambda Delta: valid_grass_tangent(self.U, Delta))(Deltas).all()\n",
    "        return Deltas\n",
    "\n",
    "    def sample_grass(self, seed: int, s: chex.ArrayDevice, reortho: bool = False) -> chex.ArrayDevice:\n",
    "        Deltas = self.sample_tangents(seed, s)\n",
    "        Ws = convert_to_projs(Deltas, self.U, reorthonormalize=reortho)\n",
    "        return Ws\n",
    "\n",
    "    def predict_tangents(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        d, _ = self.d_out\n",
    "        I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "        V_mu = lambda s: I_UUT @ self.mu(s)\n",
    "        V_Omega = I_UUT @ self.Omega @ I_UUT.T\n",
    "        V = MatGP(d_in=self.d_in, d_out=self.d_out, mu=V_mu, k=self.k, Omega=V_Omega, cov_jitter=self.cov_jitter)\n",
    "        Deltas_mean, Deltas_pred = V.predict(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "        return Deltas_mean, Deltas_pred\n",
    "\n",
    "    def predict_grass(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8, reortho: bool = False) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        Deltas_mean, Deltas_pred = self.predict_tangents(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "        Ws_mean = convert_to_projs(Deltas_mean, self.U, reorthonormalize=reortho)\n",
    "        Ws_pred = convert_to_projs(Deltas_pred, self.U, reorthonormalize=reortho)\n",
    "        return Ws_mean, Ws_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca144952-ca64-4b8c-bd99-3ac1e914e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "N = 40\n",
    "s_test = np.linspace(0, 1, N)\n",
    "k = 2 * np.pi\n",
    "x = np.cos(k * s_test).reshape(-1, 1)\n",
    "y = np.sin(k * s_test).reshape(-1, 1)\n",
    "Ws_test = np.hstack((x,y))[:,:,None]\n",
    "assert vmap(valid_grass_point)(Ws_test).all()\n",
    "d, n = Ws_test.shape[1:]\n",
    "\n",
    "# plot dataset\n",
    "for i in range(d):\n",
    "    plt.plot(s_test, Ws_test[:,i,0])\n",
    "    plt.title(f'{i+1}th component of projection')\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$s$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f0a5c-262d-4a5a-a1bb-b93095b3de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample data\n",
    "s_gap = 3\n",
    "s_train = s_test[::s_gap].copy()\n",
    "print(f\"Number of training points: {s_train.shape[0]}\")\n",
    "Ws_train = Ws_test[::s_gap,:,:].copy()\n",
    "\n",
    "for i in range(d):\n",
    "    plt.plot(s_test, Ws_test[:,i,0])\n",
    "    plt.scatter(s_train, Ws_train[:,i,0], c='r')\n",
    "    plt.title(f'{i+1}th component of projection')\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$s$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64836240-54d6-4650-aafa-c8aefab14359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subspace_angle_to_grass_pt(theta):\n",
    "    x = np.cos(theta).reshape(-1,1)\n",
    "    y = np.sin(theta).reshape(-1,1)\n",
    "    W = np.hstack((x,y))[:,:,None]\n",
    "    W = W[0]\n",
    "    return W\n",
    "\n",
    "def loss(theta, Ws):\n",
    "    W = subspace_angle_to_grass_pt(theta)\n",
    "    return vmap(lambda x: grass_dist(W, x)**2)(Ws).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9331a9-c7e5-4852-96d2-64b756e22078",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0, np.pi, 1000)\n",
    "losses = vmap(lambda theta: loss(theta, Ws_train))(thetas)\n",
    "\n",
    "theta_argmin = thetas[losses.argmin()]\n",
    "anchor_point = subspace_angle_to_grass_pt(theta_argmin)\n",
    "assert valid_grass_point(anchor_point)\n",
    "\n",
    "plt.plot(thetas,losses)\n",
    "plt.scatter(theta_argmin, losses.min(), color=\"red\", label='loss of anchor point')\n",
    "plt.grid()\n",
    "plt.xlabel(\"subspace angle\")\n",
    "plt.title(\"Plot of loss vs subspace angle to determine Karcher mean\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylim((-1.25,1.25))\n",
    "ax.set_xlim((-1.25,1.25))\n",
    "ax.scatter(Ws_train[:,0,0], Ws_train[:,1,0], color=\"blue\", alpha=0.25, label='training points')\n",
    "ax.scatter(anchor_point[0,0], anchor_point[1,0], color=\"red\", marker=\"*\", label=\"anchor point\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Plot of training points with anchor point on circle rep of Gr(2,1)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbb03e-46be-486a-b424-4c7482f3b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of training data and full data\n",
    "log_Ws_train = vmap(lambda W: grass_log(anchor_point, W))(Ws_train)\n",
    "log_Ws_test = vmap(lambda W: grass_log(anchor_point, W))(Ws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ee7b0-bd41-48c4-8e9c-08a9064a4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([subspace_angle(w) for w in Ws_test])\n",
    "alphas_train = np.array([subspace_angle(w) for w in Ws_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdea86-9266-4238-b3a6-4c6f51f91451",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'anchor_point': anchor_point.tolist(),\n",
    "    'd_in': 1,\n",
    "    'Omega' : None,\n",
    "    'k_include_noise': True,\n",
    "    'var' : 1.0,\n",
    "    'length' : None, \n",
    "    'noise' : None,\n",
    "    'require_noise' : False,\n",
    "    'jitter' : 1e-06,\n",
    "    'cov_jitter' : 1e-4,\n",
    "    'L_jitter' : 1e-8,\n",
    "    'reorthonormalize' : False,\n",
    "    'b' : 0.5,\n",
    "    # 'ell': 0.0075\n",
    "    'ell': 0.01\n",
    "}\n",
    "\n",
    "def model(s, log_Ws, grass_config = model_config):\n",
    "    U = np.array(grass_config['anchor_point'])\n",
    "    d, n = U.shape\n",
    "    N = s.shape[0]\n",
    "    d_n = d * n\n",
    "    # N_params = N * d_n\n",
    "    if log_Ws is not None:\n",
    "        assert log_Ws.shape == (N, d, n)\n",
    "    \n",
    "    # get/sample Omega\n",
    "    if grass_config['Omega'] is None:\n",
    "        sigmas = numpyro.sample('sigmas', dist.LogNormal(0.0, 1.0).expand([d_n]))\n",
    "        L_factor = numpyro.sample('L_factor', dist.LKJ(d_n, 1.0))\n",
    "        L = numpyro.deterministic('L', L_factor + grass_config['L_jitter'] * np.eye(d_n))\n",
    "        Omega = numpyro.deterministic('Omega', np.outer(sigmas, sigmas) * L)\n",
    "    else:\n",
    "        Omega = np.array(grass_config['Omega'])\n",
    "        \n",
    "    # get/sample kernel params\n",
    "    if grass_config['var'] is None:\n",
    "        # sample var\n",
    "        var = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, grass_config['b']))\n",
    "    else:\n",
    "        var = grass_config['var']\n",
    "\n",
    "    if grass_config['length'] is None:\n",
    "        # sample length\n",
    "        length = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, grass_config['b']))\n",
    "    else:\n",
    "        length = grass_config['length']\n",
    "\n",
    "    if grass_config['require_noise']:\n",
    "        if grass_config['noise'] is None:\n",
    "            # sample noise\n",
    "            noise = numpyro.sample(\"kernel_noise\", dist.LogNormal(0.0, grass_config['b']))\n",
    "        else:\n",
    "            noise = grass_config['noise']\n",
    "    else:\n",
    "        noise = 0.0\n",
    "    \n",
    "    kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "    # create kernel function\n",
    "    k = lambda t, s: rbf(t, s, kernel_params, jitter=grass_config['jitter'], include_noise=grass_config['k_include_noise'])\n",
    "    # create mean function\n",
    "    mu = lambda s: zero_mean(s, d, n)\n",
    "\n",
    "    # initialize GrassGP\n",
    "    grass_gp = GrassGP(d_in=grass_config['d_in'], d_out=(d,n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=grass_config['cov_jitter'])\n",
    "    \n",
    "    # sample Deltas\n",
    "    Deltas = grass_gp.tangent_model(s)\n",
    "    \n",
    "    # # # # ! check what power this should be\n",
    "    # likelihood\n",
    "    ell = grass_config['ell']\n",
    "    with numpyro.plate(\"N\", N):\n",
    "        numpyro.sample(\"log_W\", dist.continuous.MatrixNormal(loc=Deltas, scale_tril_row=ell * np.eye(d),scale_tril_column=np.eye(n)), obs=log_Ws)\n",
    "\n",
    "TangentSpaceModelConf = builds(model, grass_config=model_config, zen_partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cfabe-15e2-4f1d-8cce-dd1fc18b3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVIConfig = make_config(\n",
    "    seed = 123514354575,\n",
    "    maxiter = 15000,\n",
    "    step_size = 0.001\n",
    ")\n",
    "\n",
    "TrainConfig = make_config(\n",
    "    seed = 9870687,\n",
    "    n_warmup = 2000,\n",
    "    n_samples = 7000,\n",
    "    n_chains = 1,\n",
    "    n_thinning = 2\n",
    ")\n",
    "\n",
    "Config = make_config(\n",
    "    model = TangentSpaceModelConf,\n",
    "    svi = SVIConfig,\n",
    "    train = TrainConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8cd5a-86f7-4ae8-8cf6-76ab601bd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    # instantiate grass model\n",
    "    model = instantiate(cfg.model)\n",
    "    \n",
    "    # run SVI to get MAP esimtate to initialise MCMC\n",
    "    svi_key = random.PRNGKey(cfg.svi.seed)\n",
    "    maxiter = cfg.svi.maxiter\n",
    "    step_size = cfg.svi.step_size\n",
    "    print(f\"n_train = {s_train.shape[0]}\")\n",
    "    print(\"Running SVI for MAP estimate to initialise MCMC\")\n",
    "    svi_results = run_svi_for_map(svi_key, model, maxiter, step_size, s_train, log_Ws_train)\n",
    "    \n",
    "    # plot svi losses\n",
    "    plt.plot(svi_results.losses)\n",
    "    plt.show()\n",
    "    \n",
    "    # get initialisation from SVI results\n",
    "    map_est = svi_results.params\n",
    "    strip_val = len('_auto_loc')\n",
    "    init_values = {key[:-strip_val]:value for (key, value) in map_est.items()}\n",
    "    \n",
    "    # run HMC\n",
    "    train_key = random.PRNGKey(cfg.train.seed)\n",
    "    mcmc_config = {'num_warmup' : cfg.train.n_warmup, 'num_samples' : cfg.train.n_samples, 'num_chains' : cfg.train.n_chains, 'thinning' : cfg.train.n_thinning, 'init_strategy' : init_to_value(values=init_values)}\n",
    "    print(\"HMC starting.\")\n",
    "    mcmc = run_inference(train_key, mcmc_config, model, s_train, log_Ws_train)    \n",
    "    # original_stdout = sys.stdout\n",
    "    # with open('hmc_log.txt', 'w') as f:\n",
    "    #     sys.stdout = f\n",
    "    #     mcmc.print_summary()\n",
    "    #     sys.stdout = original_stdout\n",
    "    \n",
    "    samples = mcmc.get_samples()\n",
    "    inference_data = samples.copy()\n",
    "    for param, initial_val in init_values.items():\n",
    "        inference_data[f\"{param}-initial_value\"] = initial_val\n",
    "    \n",
    "    # head = os.getcwd()\n",
    "    # main_name = \"inference_data\"\n",
    "    # path = get_save_path(head, main_name)\n",
    "    # try:\n",
    "    #     safe_save(path, inference_data)\n",
    "    # except FileExistsError:\n",
    "    #     print(\"File exists so not saving.\")\n",
    "    return inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43c224-c589-4e66-a6d4-a6b603f7bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpyro.render_model(instantiate(Config.model), model_args=(s_train,log_Ws_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e18b29-abdf-4838-abc8-f93bd86d78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = train(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610229dc-b76d-42c2-8998-6b0aab5690da",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dict(filter(lambda elem: 'initial_value' not in elem[0], inference_data.items()))\n",
    "initial_values = dict(filter(lambda elem: 'initial_value' in elem[0], inference_data.items()))\n",
    "assert set(samples.keys()).union(initial_values.keys()) == set(inference_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79727d-f145-4725-9710-686cac041d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/samples_less_train_pts.pickle\", 'rb') as f:\n",
    "#     samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2cff6-244f-49af-87f5-a3ffda934089",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/samples_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3b460-bf3f-4ee1-aaff-96d65a86b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_samples = flatten_samples(samples, ignore=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f815d5a-ed9a-49cf-a91b-368b61b02933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trace_plot_vars = ['kernel_length']\n",
    "for key in my_samples.keys():\n",
    "    if 'Omega' in key:\n",
    "        trace_plot_vars.append(key)\n",
    "    if 'sigmas' in key:\n",
    "        trace_plot_vars.append(key)\n",
    "        \n",
    "my_samples[trace_plot_vars].plot(subplots=True, figsize=(10,40), sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa4887-2d84-475b-a36d-5e38a790439e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for var in trace_plot_vars:\n",
    "    sm.graphics.tsa.plot_acf(my_samples[var], lags=100)\n",
    "    plt.title(f\"acf for {var}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af82359-4e52-4b01-b696-66ab7d256aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-5\n",
    "\n",
    "samples_Ws_train = vmap(lambda Deltas: convert_to_projs(Deltas, anchor_point, reorthonormalize=False))(samples['Deltas'])\n",
    "\n",
    "for ws in samples_Ws_train:\n",
    "    assert vmap(lambda w: valid_grass_point(w, tol=tol))(ws).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966a811-0424-4f66-8051-c76d28498928",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_barycenters = []\n",
    "for i in tqdm(range(s_train.shape[0])):\n",
    "    points = samples_Ws_train[:,i,:,:]\n",
    "    mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    mcmc_theta_argmin = thetas[mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(mcmc_theta_argmin)\n",
    "    mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, mcmc_losses)\n",
    "    # plt.scatter(mcmc_theta_argmin, mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dedc06c-b010-4750-9b76-bfe2e6ec0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_barycenters = np.array(mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702e02e-4e0b-4686-8657-a131132a4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/mcmc_barycenters_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(mcmc_barycenters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9107337-c872-4d90-8ebf-acb4ec1c7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/mcmc_barycenters_less_train_pts.pickle\", 'rb') as f:\n",
    "#     mcmc_barycenters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4acd3-1ec8-42ff-9fb8-71e2191f36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_errors = vmap(grass_dist)(Ws_train, mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0de81-950f-4439-b3bf-e16d281e4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_train,in_sample_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1c1ad-6303-460e-8eaa-451c816a3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_train = []\n",
    "for i in tqdm(range(s_train.shape[0])):\n",
    "    fixed = mcmc_barycenters[i]\n",
    "    dists = vmap(lambda W: grass_dist(W, fixed))(samples_Ws_train[:,i,:,:])\n",
    "    dists_Sq = dists**2\n",
    "    sd_s_train.append(np.sqrt(dists_Sq.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368e48f-8003-4988-beb8-275f000eba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_train = np.array(sd_s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14311c1-676c-4352-9a36-00217eb421f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = {'s': s_train, 'errors': in_sample_errors, 'sd': sd_s_train}\n",
    "in_sample_errors_df = pd.DataFrame(data=pd_data)\n",
    "in_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d549e4b-d706-4f0f-aed2-575ee3d59198",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_errors_df.drop([\"s\"],axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1262c7c-4e83-4fd9-aa4c-e098095dc32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_grass_dists(samples_Ws_train, Ws_train, s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcea9a-1843-4ba0-849a-327e1b9e01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_alphas_train = np.array([[subspace_angle(w)for w in Ws_sample] for Ws_sample in samples_Ws_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3c613-b808-4e49-a941-06948c075d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/samples_alphas_train_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(samples_alphas_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415a112-2686-41dd-9bb1-78a40a09043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/samples_alphas_train_less_train_pts.pickle\", 'rb') as f:\n",
    "#     samples_alphas_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cd56a-96d4-4e0e-88a5-e9806da07154",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "percentiles = np.percentile(samples_alphas_train, np.array(percentile_levels), axis=0)\n",
    "lower = percentiles[0,:]\n",
    "upper = percentiles[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba953f9-ab91-42d7-8ce9-ab85b0d2eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_test, alphas, c='black', alpha=0.5, label='full data')\n",
    "plt.scatter(s_train, alphas_train, label='training data', c='g')\n",
    "plt.scatter(s_train, samples_alphas_train.mean(axis=0), label='mean samples', c='r')\n",
    "plt.fill_between(s_train, lower, upper,  color='lightblue', alpha=0.75,label=f'{conf_level}% credible interval')\n",
    "plt.xlabel(r\"$s$\")\n",
    "plt.ylabel(\"subspace angle\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76152d-664c-4851-82f9-c709ba095aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tangents(\n",
    "    key: chex.ArrayDevice,\n",
    "    s_test: chex.ArrayDevice,\n",
    "    s_train: chex.ArrayDevice,\n",
    "    Vs_train: chex.ArrayDevice,\n",
    "    dict_cfg,\n",
    "    samples: dict,\n",
    "    jitter: float = 1e-8\n",
    ") -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "    \n",
    "    d_in = dict_cfg.model.grass_config.d_in\n",
    "    U = np.array(dict_cfg.model.grass_config.anchor_point)\n",
    "    d, n = U.shape\n",
    "    cov_jitter = dict_cfg.model.grass_config.cov_jitter\n",
    "    k_include_noise = dict_cfg.model.grass_config.k_include_noise\n",
    "    kern_jitter = dict_cfg.model.grass_config.jitter\n",
    "    n_samples = dict_cfg.train.n_samples // dict_cfg.train.n_thinning\n",
    "    assert n_samples == samples['Deltas'].shape[0]\n",
    "    \n",
    "    def predict(\n",
    "        key: chex.ArrayDevice,\n",
    "        Omega: chex.ArrayDevice,\n",
    "        var: float,\n",
    "        length: float,\n",
    "        noise: float,\n",
    "    ) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        # iniatilize GrassGP\n",
    "        kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=kern_jitter, include_noise=k_include_noise)\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "        grass_gp = GrassGP(d_in=d_in, d_out=(d, n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=cov_jitter)\n",
    "\n",
    "        # predict\n",
    "        Deltas_mean, Deltas_pred = grass_gp.predict_tangents(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "        return Deltas_mean, Deltas_pred\n",
    "\n",
    "    # initialize vmap args\n",
    "    vmap_args = (random.split(key, n_samples),)\n",
    "    \n",
    "    cfg_Omega = dict_cfg.model.grass_config.Omega\n",
    "    cfg_var = dict_cfg.model.grass_config.var\n",
    "    cfg_length = dict_cfg.model.grass_config.length\n",
    "    cfg_noise = dict_cfg.model.grass_config.noise\n",
    "    cfg_require_noise = dict_cfg.model.grass_config.require_noise\n",
    "    \n",
    "    if cfg_Omega is None:\n",
    "        vmap_args += (samples['Omega'],)\n",
    "    else:\n",
    "        cfg_Omega = np.array(cfg_Omega)\n",
    "        vmap_args += (np.repeat(cfg_Omega[None,:,:], n_samples, axis=0),)\n",
    "    \n",
    "    if cfg_var is None:\n",
    "        vmap_args += (samples['kernel_var'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_var * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_length is None:\n",
    "        vmap_args += (samples['kernel_length'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_length * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_require_noise:\n",
    "        if cfg_noise is None:\n",
    "            vmap_args += (samples['kernel_noise'],)\n",
    "        else:\n",
    "            vmap_args += (cfg_noise * np.ones(n_samples),)\n",
    "    else:\n",
    "        vmap_args += (np.zeros(n_samples),)\n",
    "    \n",
    "    assert len(vmap_args) == 5\n",
    "    Deltas_means, Deltas_preds = vmap(predict)(*vmap_args)\n",
    "    return Deltas_means, Deltas_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f0515-e647-4b8b-8b28-352cf6a4f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = to_dictconf(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada6cdd-a322-4aa8-a0dc-df2d829d0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_key = random.PRNGKey(6578)\n",
    "Deltas_means, Deltas_preds = predict_tangents(pred_key, s_test, s_train, log_Ws_train, config, samples)\n",
    "assert np.isnan(Deltas_means).sum() == 0\n",
    "assert np.isnan(Deltas_preds).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cf27d-b95d-43cb-a4ab-edb18529b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/Deltas_means_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(Deltas_means, f)\n",
    "    \n",
    "with open(\"results/Deltas_preds_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(Deltas_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483941bf-ab03-4f76-95a3-dbf8e8173f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/Deltas_means_less_train_pts.pickle\", 'rb') as f:\n",
    "#     Deltas_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/Deltas_preds_less_train_pts.pickle\", 'rb') as f:\n",
    "#     Deltas_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48065a9b-7d9d-4a7e-8c74-5bc11c850034",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "for i in range(d):\n",
    "    obs = log_Ws_train[:,i,0]\n",
    "    means = Deltas_means[:,:,i,0]\n",
    "    means_avg = np.mean(means, axis=0)\n",
    "    preds = Deltas_preds[:,:,i,0]\n",
    "    percentiles = np.percentile(preds, np.array(percentile_levels), axis=0)\n",
    "    lower = percentiles[0,:]\n",
    "    upper = percentiles[1,:]\n",
    "    plt.plot(s_test, log_Ws_test[:,i,0], label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "    plt.scatter(s_train, log_Ws_train[:,i,0], label='training data', c='g')\n",
    "    plt.plot(s_test, means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "    plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "    plt.xlabel(r\"$s$\")\n",
    "    plt.legend()\n",
    "    plt.vlines(s_train, 0.99*lower.min(), 1.01*upper.max(), colors='green', linestyles='dashed')\n",
    "    plt.title(f\"{i+1}th component of tangents\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb78b22-472e-4de7-810d-80fce42dd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_grass(\n",
    "    key: chex.ArrayDevice,\n",
    "    s_test: chex.ArrayDevice,\n",
    "    s_train: chex.ArrayDevice,\n",
    "    Vs_train: chex.ArrayDevice,\n",
    "    dict_cfg,\n",
    "    samples: dict,\n",
    "    jitter: float = 1e-8,\n",
    "    reortho: bool = False\n",
    ") -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "    \n",
    "    d_in = dict_cfg.model.grass_config.d_in\n",
    "    U = np.array(dict_cfg.model.grass_config.anchor_point)\n",
    "    d, n = U.shape\n",
    "    cov_jitter = dict_cfg.model.grass_config.cov_jitter\n",
    "    k_include_noise = dict_cfg.model.grass_config.k_include_noise\n",
    "    kern_jitter = dict_cfg.model.grass_config.jitter\n",
    "    n_samples = dict_cfg.train.n_samples // dict_cfg.train.n_thinning\n",
    "    assert n_samples == samples['Deltas'].shape[0]\n",
    "    \n",
    "    def predict(\n",
    "        key: chex.ArrayDevice,\n",
    "        Omega: chex.ArrayDevice,\n",
    "        var: float,\n",
    "        length: float,\n",
    "        noise: float,\n",
    "    ) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        # iniatilize GrassGP\n",
    "        kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=kern_jitter, include_noise=k_include_noise)\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "        grass_gp = GrassGP(d_in=d_in, d_out=(d, n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=cov_jitter)\n",
    "\n",
    "        # predict\n",
    "        Ws_mean, Ws_pred = grass_gp.predict_grass(key, s_test, s_train, Vs_train, jitter=jitter, reortho=reortho)\n",
    "        return Ws_mean, Ws_pred\n",
    "\n",
    "    # initialize vmap args\n",
    "    vmap_args = (random.split(key, n_samples),)\n",
    "    \n",
    "    cfg_Omega = dict_cfg.model.grass_config.Omega\n",
    "    cfg_var = dict_cfg.model.grass_config.var\n",
    "    cfg_length = dict_cfg.model.grass_config.length\n",
    "    cfg_noise = dict_cfg.model.grass_config.noise\n",
    "    cfg_require_noise = dict_cfg.model.grass_config.require_noise\n",
    "    \n",
    "    if cfg_Omega is None:\n",
    "        vmap_args += (samples['Omega'],)\n",
    "    else:\n",
    "        cfg_Omega = np.array(cfg_Omega)\n",
    "        vmap_args += (np.repeat(cfg_Omega[None,:,:], n_samples, axis=0),)\n",
    "    \n",
    "    if cfg_var is None:\n",
    "        vmap_args += (samples['kernel_var'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_var * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_length is None:\n",
    "        vmap_args += (samples['kernel_length'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_length * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_require_noise:\n",
    "        if cfg_noise is None:\n",
    "            vmap_args += (samples['kernel_noise'],)\n",
    "        else:\n",
    "            vmap_args += (cfg_noise * np.ones(n_samples),)\n",
    "    else:\n",
    "        vmap_args += (np.zeros(n_samples),)\n",
    "    \n",
    "    assert len(vmap_args) == 5\n",
    "    Ws_means, Ws_preds = vmap(predict)(*vmap_args)\n",
    "    return Ws_means, Ws_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff357e-0b30-47d0-bc19-98c6189bf270",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_key_grass = random.PRNGKey(7695)\n",
    "Ws_means, Ws_preds = predict_grass(pred_key_grass, s_test, s_train, log_Ws_train, config, samples)\n",
    "assert np.isnan(Ws_means).sum() == 0\n",
    "assert np.isnan(Ws_preds).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a0e05-c2ed-4fd6-bf53-7808026f9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/Ws_means_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(Ws_means, f)\n",
    "    \n",
    "with open(\"results/Ws_preds_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(Ws_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45b802-c109-4fa0-b30a-d6a72ebb0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/Ws_means_less_train_pts.pickle\", 'rb') as f:\n",
    "#     Ws_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/Ws_preds_less_train_pts.pickle\", 'rb') as f:\n",
    "#     Ws_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2098cf-294a-403d-82ca-efb56d62afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "for i in range(d):\n",
    "    obs = Ws_train[:,i,0]\n",
    "    means = Ws_means[:,:,i,0]\n",
    "    means_avg = np.mean(means, axis=0)\n",
    "    preds = Ws_preds[:,:,i,0]\n",
    "    percentiles = np.percentile(preds, np.array(percentile_levels), axis=0)\n",
    "    lower = percentiles[0,:]\n",
    "    upper = percentiles[1,:]\n",
    "    plt.plot(s_test, Ws_test[:,i,0], label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "    plt.scatter(s_train, Ws_train[:,i,0], label='training data', c='g')\n",
    "    plt.plot(s_test, means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "    plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "    plt.xlabel(r\"$s$\")\n",
    "    plt.legend()\n",
    "    plt.vlines(s_train, 0.99*lower.min(), 1.01*upper.max(), colors='green', linestyles='dashed')\n",
    "    plt.title(f\"{i+1}th component of projections\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ed6ce-ac1c-4091-92d0-4605358aa935",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_means = np.array([[subspace_angle(w) for w in mean] for mean in Ws_means])\n",
    "alphas_preds = np.array([[subspace_angle(w) for w in pred] for pred in Ws_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a27aad-653c-481b-bdee-c8145856a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/alphas_means_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(alphas_means, f)\n",
    "    \n",
    "with open(\"results/alphas_preds_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(alphas_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c0617-fa14-40ee-b298-374eced076cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/alphas_means_less_train_pts.pickle\", 'rb') as f:\n",
    "#     alphas_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/alphas_preds_less_train_pts.pickle\", 'rb') as f:\n",
    "#     alphas_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507eecf3-60fd-44eb-835e-36f27f0d74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "alphas_means_avg = np.mean(alphas_means, axis=0)\n",
    "percentiles = np.percentile(alphas_preds, np.array(percentile_levels), axis=0)\n",
    "lower = percentiles[0,:]\n",
    "upper = percentiles[1,:]\n",
    "plt.plot(s_test, alphas, label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "plt.scatter(s_train, alphas_train, label='training data', c='g')\n",
    "plt.plot(s_test, alphas_means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "plt.xlabel(r\"$s$\")\n",
    "plt.ylabel(\"subspace angle\")\n",
    "plt.legend()\n",
    "plt.vlines(s_train, 0, np.pi, colors='green', linestyles='dashed')\n",
    "plt.title(f\"predictions for subspace angles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075fee3-5fac-4366-9173-0e6466323b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4eaba-68a3-41ed-a3d1-f32c76c35e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means_mcmc_barycenters = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    points = Ws_means[:,i,:,:]\n",
    "    test_means_mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    test_means_mcmc_theta_argmin = thetas[test_means_mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(test_means_mcmc_theta_argmin)\n",
    "    test_means_mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, test_means_mcmc_losses)\n",
    "    # plt.scatter(test_means_mcmc_theta_argmin, test_means_mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2ee71-955d-4781-b9cc-aaa1f0d59103",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_mcmc_barycenters = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    points = Ws_preds[:,i,:,:]\n",
    "    test_preds_mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    test_preds_mcmc_theta_argmin = thetas[test_preds_mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(test_preds_mcmc_theta_argmin)\n",
    "    test_preds_mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, test_preds_mcmc_losses)\n",
    "    # plt.scatter(test_preds_mcmc_theta_argmin, test_preds_mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d55e8-2f9b-4771-a8f4-5140113811fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means_mcmc_barycenters = np.array(test_means_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e1a61-5730-41fd-84c6-04973d30a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_mcmc_barycenters = np.array(test_preds_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f1de9-d47d-4bc4-9b54-8f390559858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/test_means_mcmc_barycenters_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(test_means_mcmc_barycenters, f)\n",
    "\n",
    "with open(\"results/test_preds_mcmc_barycenters_less_train_pts.pickle\", 'wb') as f:\n",
    "    pickle.dump(test_preds_mcmc_barycenters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612156c9-72eb-4286-a208-0f17ff5a66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/test_means_mcmc_barycenters_less_train_pts.pickle\", 'rb') as f:\n",
    "#     test_means_mcmc_barycenters = pickle.load(f)\n",
    "\n",
    "# with open(\"results/test_preds_mcmc_barycenters_less_train_pts.pickle\", 'rb') as f:\n",
    "#     test_preds_mcmc_barycenters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac94934-9ebb-42a7-b2a3-413bf29a1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_mean_errors = vmap(grass_dist)(Ws_test, test_means_mcmc_barycenters)\n",
    "out_sample_pred_errors = vmap(grass_dist)(Ws_test, test_preds_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409a150-b7b2-49f5-a7d3-bccb89119357",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_test,out_sample_mean_errors, label='error using means')\n",
    "plt.plot(s_test,out_sample_pred_errors, label='error using preds')\n",
    "plt.vlines(s_train, 0, 1, colors=\"green\", linestyles=\"dashed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5447ef-07f1-4f1d-9375-472c71677068",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sd_s_test = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    fixed = test_preds_mcmc_barycenters[i]\n",
    "    dists = vmap(lambda W: grass_dist(W, fixed))(Ws_preds[:,i,:,:])\n",
    "    dists_Sq = dists**2\n",
    "    sd_s_test.append(np.sqrt(dists_Sq.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f8440-c457-4495-9d55-8a6cd1afed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_test = np.array(sd_s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38e4ac-adce-40b3-bf0f-4866c2d245a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd_data = {'s': s_test, 'errors_mean': out_sample_mean_errors, 'errors_pred': out_sample_pred_errors, 'sd': sd_s_test}\n",
    "out_sample_errors_df = pd.DataFrame(data=test_pd_data)\n",
    "out_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cf762-1ddd-4f12-8558-9215a07ec456",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_errors_df.drop(['s'], axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc3cc1-620e-4c96-82ea-0d760ee65be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_AS_dir_preds(Ws_preds, Ws_test, s_test, s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4a526-100f-4f89-b232-9ea2724866bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_grass_dists(Ws_preds, Ws_test, s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b375b6b-6798-4ab2-b94c-0a0cc1fda9c6",
   "metadata": {},
   "source": [
    "# Increase training points - keep old anchor point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5effca-8f66-42ee-9aa6-9c81eeef3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample data\n",
    "s_gap = 2\n",
    "s_train = s_test[::s_gap].copy()\n",
    "print(f\"Number of training points: {s_train.shape[0]}\")\n",
    "Ws_train = Ws_test[::s_gap,:,:].copy()\n",
    "\n",
    "for i in range(d):\n",
    "    plt.plot(s_test, Ws_test[:,i,0])\n",
    "    plt.scatter(s_train, Ws_train[:,i,0], c='r')\n",
    "    plt.title(f'{i+1}th component of projection')\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$s$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552eab0d-bd7b-4923-bc03-8d90316ac515",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e235d909-6b37-4366-a207-cccdcd23ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of training data and full data\n",
    "log_Ws_train = vmap(lambda W: grass_log(anchor_point, W))(Ws_train)\n",
    "log_Ws_test = vmap(lambda W: grass_log(anchor_point, W))(Ws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06308e0e-210f-48e3-9aff-210d79af0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([subspace_angle(w) for w in Ws_test])\n",
    "alphas_train = np.array([subspace_angle(w) for w in Ws_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dce1a0-5a2d-420c-9055-3d59cbb754c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'anchor_point': anchor_point.tolist(),\n",
    "    'd_in': 1,\n",
    "    'Omega' : None,\n",
    "    'k_include_noise': True,\n",
    "    'var' : 1.0,\n",
    "    'length' : None, \n",
    "    'noise' : None,\n",
    "    'require_noise' : False,\n",
    "    'jitter' : 1e-06,\n",
    "    'cov_jitter' : 1e-4,\n",
    "    'L_jitter' : 1e-8,\n",
    "    'reorthonormalize' : False,\n",
    "    'b' : 0.5,\n",
    "    # 'ell': 0.0075\n",
    "    'ell': 0.01\n",
    "}\n",
    "\n",
    "def model(s, log_Ws, grass_config = model_config):\n",
    "    U = np.array(grass_config['anchor_point'])\n",
    "    d, n = U.shape\n",
    "    N = s.shape[0]\n",
    "    d_n = d * n\n",
    "    # N_params = N * d_n\n",
    "    if log_Ws is not None:\n",
    "        assert log_Ws.shape == (N, d, n)\n",
    "    \n",
    "    # get/sample Omega\n",
    "    if grass_config['Omega'] is None:\n",
    "        sigmas = numpyro.sample('sigmas', dist.LogNormal(0.0, 1.0).expand([d_n]))\n",
    "        L_factor = numpyro.sample('L_factor', dist.LKJ(d_n, 1.0))\n",
    "        L = numpyro.deterministic('L', L_factor + grass_config['L_jitter'] * np.eye(d_n))\n",
    "        Omega = numpyro.deterministic('Omega', np.outer(sigmas, sigmas) * L)\n",
    "    else:\n",
    "        Omega = np.array(grass_config['Omega'])\n",
    "        \n",
    "    # get/sample kernel params\n",
    "    if grass_config['var'] is None:\n",
    "        # sample var\n",
    "        var = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, grass_config['b']))\n",
    "    else:\n",
    "        var = grass_config['var']\n",
    "\n",
    "    if grass_config['length'] is None:\n",
    "        # sample length\n",
    "        length = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, grass_config['b']))\n",
    "    else:\n",
    "        length = grass_config['length']\n",
    "\n",
    "    if grass_config['require_noise']:\n",
    "        if grass_config['noise'] is None:\n",
    "            # sample noise\n",
    "            noise = numpyro.sample(\"kernel_noise\", dist.LogNormal(0.0, grass_config['b']))\n",
    "        else:\n",
    "            noise = grass_config['noise']\n",
    "    else:\n",
    "        noise = 0.0\n",
    "    \n",
    "    kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "    # create kernel function\n",
    "    k = lambda t, s: rbf(t, s, kernel_params, jitter=grass_config['jitter'], include_noise=grass_config['k_include_noise'])\n",
    "    # create mean function\n",
    "    mu = lambda s: zero_mean(s, d, n)\n",
    "\n",
    "    # initialize GrassGP\n",
    "    grass_gp = GrassGP(d_in=grass_config['d_in'], d_out=(d,n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=grass_config['cov_jitter'])\n",
    "    \n",
    "    # sample Deltas\n",
    "    Deltas = grass_gp.tangent_model(s)\n",
    "    \n",
    "    # # # # ! check what power this should be\n",
    "    # likelihood\n",
    "    ell = grass_config['ell']\n",
    "    with numpyro.plate(\"N\", N):\n",
    "        numpyro.sample(\"log_W\", dist.continuous.MatrixNormal(loc=Deltas, scale_tril_row=ell * np.eye(d),scale_tril_column=np.eye(n)), obs=log_Ws)\n",
    "\n",
    "TangentSpaceModelConf = builds(model, grass_config=model_config, zen_partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a9d40-ca71-43ef-9e04-ee36c34808eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVIConfig = make_config(\n",
    "    seed = 123514354575,\n",
    "    maxiter = 15000,\n",
    "    step_size = 0.001\n",
    ")\n",
    "\n",
    "TrainConfig = make_config(\n",
    "    seed = 9870687,\n",
    "    n_warmup = 2000,\n",
    "    n_samples = 7000,\n",
    "    n_chains = 1,\n",
    "    n_thinning = 2\n",
    ")\n",
    "\n",
    "Config = make_config(\n",
    "    model = TangentSpaceModelConf,\n",
    "    svi = SVIConfig,\n",
    "    train = TrainConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a81181-7479-464e-92e4-418d0ff9af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    # instantiate grass model\n",
    "    model = instantiate(cfg.model)\n",
    "    \n",
    "    # run SVI to get MAP esimtate to initialise MCMC\n",
    "    svi_key = random.PRNGKey(cfg.svi.seed)\n",
    "    maxiter = cfg.svi.maxiter\n",
    "    step_size = cfg.svi.step_size\n",
    "    print(f\"n_train = {s_train.shape[0]}\")\n",
    "    print(\"Running SVI for MAP estimate to initialise MCMC\")\n",
    "    svi_results = run_svi_for_map(svi_key, model, maxiter, step_size, s_train, log_Ws_train)\n",
    "    \n",
    "    # plot svi losses\n",
    "    plt.plot(svi_results.losses)\n",
    "    plt.show()\n",
    "    \n",
    "    # get initialisation from SVI results\n",
    "    map_est = svi_results.params\n",
    "    strip_val = len('_auto_loc')\n",
    "    init_values = {key[:-strip_val]:value for (key, value) in map_est.items()}\n",
    "    \n",
    "    # run HMC\n",
    "    train_key = random.PRNGKey(cfg.train.seed)\n",
    "    mcmc_config = {'num_warmup' : cfg.train.n_warmup, 'num_samples' : cfg.train.n_samples, 'num_chains' : cfg.train.n_chains, 'thinning' : cfg.train.n_thinning, 'init_strategy' : init_to_value(values=init_values)}\n",
    "    print(\"HMC starting.\")\n",
    "    mcmc = run_inference(train_key, mcmc_config, model, s_train, log_Ws_train)    \n",
    "    # original_stdout = sys.stdout\n",
    "    # with open('hmc_log.txt', 'w') as f:\n",
    "    #     sys.stdout = f\n",
    "    #     mcmc.print_summary()\n",
    "    #     sys.stdout = original_stdout\n",
    "    \n",
    "    samples = mcmc.get_samples()\n",
    "    inference_data = samples.copy()\n",
    "    for param, initial_val in init_values.items():\n",
    "        inference_data[f\"{param}-initial_value\"] = initial_val\n",
    "    \n",
    "    # head = os.getcwd()\n",
    "    # main_name = \"inference_data\"\n",
    "    # path = get_save_path(head, main_name)\n",
    "    # try:\n",
    "    #     safe_save(path, inference_data)\n",
    "    # except FileExistsError:\n",
    "    #     print(\"File exists so not saving.\")\n",
    "    return inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7134af2-6b46-4519-b50d-f49fb2f16368",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpyro.render_model(instantiate(Config.model), model_args=(s_train,log_Ws_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d27c4-b630-4c41-980f-af2cf81aa9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = train(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755c6d9-a036-419e-9f14-254c69d1c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dict(filter(lambda elem: 'initial_value' not in elem[0], inference_data.items()))\n",
    "initial_values = dict(filter(lambda elem: 'initial_value' in elem[0], inference_data.items()))\n",
    "assert set(samples.keys()).union(initial_values.keys()) == set(inference_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482c9b3-84eb-42d0-bf11-6a874b4d3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/samples_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fdd63-17bd-470a-96c3-e219fb0edd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/samples_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af832179-e1c0-44c7-8de7-6098d8c3e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_samples = flatten_samples(samples, ignore=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8edb35-eb72-4bd8-b6fb-c4c30a8c06a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trace_plot_vars = ['kernel_length']\n",
    "for key in my_samples.keys():\n",
    "    if 'Omega' in key:\n",
    "        trace_plot_vars.append(key)\n",
    "    if 'sigmas' in key:\n",
    "        trace_plot_vars.append(key)\n",
    "        \n",
    "my_samples[trace_plot_vars].plot(subplots=True, figsize=(10,40), sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a05c3d-2a3c-42f2-a703-09b8a57f68b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for var in trace_plot_vars:\n",
    "    sm.graphics.tsa.plot_acf(my_samples[var], lags=100)\n",
    "    plt.title(f\"acf for {var}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d26d1f-12ff-45ed-b3f7-70870c1dcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-5\n",
    "\n",
    "samples_Ws_train = vmap(lambda Deltas: convert_to_projs(Deltas, anchor_point, reorthonormalize=False))(samples['Deltas'])\n",
    "\n",
    "for ws in samples_Ws_train:\n",
    "    assert vmap(lambda w: valid_grass_point(w, tol=tol))(ws).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25aa9fc-03a6-4d24-af67-a7de907308e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_barycenters = []\n",
    "for i in tqdm(range(s_train.shape[0])):\n",
    "    points = samples_Ws_train[:,i,:,:]\n",
    "    mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    mcmc_theta_argmin = thetas[mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(mcmc_theta_argmin)\n",
    "    mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, mcmc_losses)\n",
    "    # plt.scatter(mcmc_theta_argmin, mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aabec0-ac56-47fc-bfa2-752bdd074c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_barycenters = np.array(mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1156f-0b51-4107-b113-75ca217c5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/mcmc_barycenters_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(mcmc_barycenters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc5823-fbc5-4949-a04a-ef07104d28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/mcmc_barycenters_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     mcmc_barycenters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2490a77-4f12-4826-9354-101c36396c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_errors = vmap(grass_dist)(Ws_train, mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e0887-5f4b-49c4-8211-652f56aaf8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_train,in_sample_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774d108-c80c-4b15-8f67-05184dafc184",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_train = []\n",
    "for i in tqdm(range(s_train.shape[0])):\n",
    "    fixed = mcmc_barycenters[i]\n",
    "    dists = vmap(lambda W: grass_dist(W, fixed))(samples_Ws_train[:,i,:,:])\n",
    "    dists_Sq = dists**2\n",
    "    sd_s_train.append(np.sqrt(dists_Sq.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75027b2-e747-491a-b9c5-881e1b2233fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_train = np.array(sd_s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9b83c-a374-41b7-968f-3981fa0f5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = {'s': s_train, 'errors': in_sample_errors, 'sd': sd_s_train}\n",
    "in_sample_errors_df = pd.DataFrame(data=pd_data)\n",
    "in_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea05784-04c9-414f-82f8-dbcc6ea3b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_errors_df.drop([\"s\"],axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c2710-8b60-4a7d-b1df-e357412db42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_grass_dists(samples_Ws_train, Ws_train, s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11f9bb-a205-49e4-b5c4-967dea50f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_alphas_train = np.array([[subspace_angle(w)for w in Ws_sample] for Ws_sample in samples_Ws_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c1a5f-ad7b-4505-918a-588f5756886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/samples_alphas_train_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(samples_alphas_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45564148-80b9-4734-a979-8444f3151e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/samples_alphas_train_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     samples_alphas_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0aa06b-4434-4295-9237-eae8b1d215c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "percentiles = np.percentile(samples_alphas_train, np.array(percentile_levels), axis=0)\n",
    "lower = percentiles[0,:]\n",
    "upper = percentiles[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f341f70-f0d7-4f7b-91e3-b018ffe9622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_test, alphas, c='black', alpha=0.5, label='full data')\n",
    "plt.scatter(s_train, alphas_train, label='training data', c='g')\n",
    "plt.scatter(s_train, samples_alphas_train.mean(axis=0), label='mean samples', c='r')\n",
    "plt.fill_between(s_train, lower, upper,  color='lightblue', alpha=0.75,label=f'{conf_level}% credible interval')\n",
    "plt.xlabel(r\"$s$\")\n",
    "plt.ylabel(\"subspace angle\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf8779-6021-424c-96e6-62c576a2cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tangents(\n",
    "    key: chex.ArrayDevice,\n",
    "    s_test: chex.ArrayDevice,\n",
    "    s_train: chex.ArrayDevice,\n",
    "    Vs_train: chex.ArrayDevice,\n",
    "    dict_cfg,\n",
    "    samples: dict,\n",
    "    jitter: float = 1e-8\n",
    ") -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "    \n",
    "    d_in = dict_cfg.model.grass_config.d_in\n",
    "    U = np.array(dict_cfg.model.grass_config.anchor_point)\n",
    "    d, n = U.shape\n",
    "    cov_jitter = dict_cfg.model.grass_config.cov_jitter\n",
    "    k_include_noise = dict_cfg.model.grass_config.k_include_noise\n",
    "    kern_jitter = dict_cfg.model.grass_config.jitter\n",
    "    n_samples = dict_cfg.train.n_samples // dict_cfg.train.n_thinning\n",
    "    assert n_samples == samples['Deltas'].shape[0]\n",
    "    \n",
    "    def predict(\n",
    "        key: chex.ArrayDevice,\n",
    "        Omega: chex.ArrayDevice,\n",
    "        var: float,\n",
    "        length: float,\n",
    "        noise: float,\n",
    "    ) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        # iniatilize GrassGP\n",
    "        kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=kern_jitter, include_noise=k_include_noise)\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "        grass_gp = GrassGP(d_in=d_in, d_out=(d, n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=cov_jitter)\n",
    "\n",
    "        # predict\n",
    "        Deltas_mean, Deltas_pred = grass_gp.predict_tangents(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "        return Deltas_mean, Deltas_pred\n",
    "\n",
    "    # initialize vmap args\n",
    "    vmap_args = (random.split(key, n_samples),)\n",
    "    \n",
    "    cfg_Omega = dict_cfg.model.grass_config.Omega\n",
    "    cfg_var = dict_cfg.model.grass_config.var\n",
    "    cfg_length = dict_cfg.model.grass_config.length\n",
    "    cfg_noise = dict_cfg.model.grass_config.noise\n",
    "    cfg_require_noise = dict_cfg.model.grass_config.require_noise\n",
    "    \n",
    "    if cfg_Omega is None:\n",
    "        vmap_args += (samples['Omega'],)\n",
    "    else:\n",
    "        cfg_Omega = np.array(cfg_Omega)\n",
    "        vmap_args += (np.repeat(cfg_Omega[None,:,:], n_samples, axis=0),)\n",
    "    \n",
    "    if cfg_var is None:\n",
    "        vmap_args += (samples['kernel_var'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_var * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_length is None:\n",
    "        vmap_args += (samples['kernel_length'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_length * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_require_noise:\n",
    "        if cfg_noise is None:\n",
    "            vmap_args += (samples['kernel_noise'],)\n",
    "        else:\n",
    "            vmap_args += (cfg_noise * np.ones(n_samples),)\n",
    "    else:\n",
    "        vmap_args += (np.zeros(n_samples),)\n",
    "    \n",
    "    assert len(vmap_args) == 5\n",
    "    Deltas_means, Deltas_preds = vmap(predict)(*vmap_args)\n",
    "    return Deltas_means, Deltas_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2265d2e-cbc6-42a4-a292-11f4f4f98bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = to_dictconf(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3829ebf-5bc0-42f6-9de3-0ac72f2b4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_key = random.PRNGKey(6578)\n",
    "Deltas_means, Deltas_preds = predict_tangents(pred_key, s_test, s_train, log_Ws_train, config, samples)\n",
    "assert np.isnan(Deltas_means).sum() == 0\n",
    "assert np.isnan(Deltas_preds).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98403fc7-9498-40be-982a-e76d9ea76195",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/Deltas_means_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Deltas_means, f)\n",
    "    \n",
    "with open(\"results/Deltas_preds_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Deltas_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41547b60-e32f-467d-b776-6984de9d66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/Deltas_means_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     Deltas_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/Deltas_preds_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     Deltas_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef543c-8b6e-4ac7-b938-e183e21e629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "for i in range(d):\n",
    "    obs = log_Ws_train[:,i,0]\n",
    "    means = Deltas_means[:,:,i,0]\n",
    "    means_avg = np.mean(means, axis=0)\n",
    "    preds = Deltas_preds[:,:,i,0]\n",
    "    percentiles = np.percentile(preds, np.array(percentile_levels), axis=0)\n",
    "    lower = percentiles[0,:]\n",
    "    upper = percentiles[1,:]\n",
    "    plt.plot(s_test, log_Ws_test[:,i,0], label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "    plt.scatter(s_train, log_Ws_train[:,i,0], label='training data', c='g')\n",
    "    plt.plot(s_test, means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "    plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "    plt.xlabel(r\"$s$\")\n",
    "    plt.legend()\n",
    "    plt.vlines(s_train, 0.99*lower.min(), 1.01*upper.max(), colors='green', linestyles='dashed')\n",
    "    plt.title(f\"{i+1}th component of tangents\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18083e-dc07-4455-bef4-fce1163b88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_grass(\n",
    "    key: chex.ArrayDevice,\n",
    "    s_test: chex.ArrayDevice,\n",
    "    s_train: chex.ArrayDevice,\n",
    "    Vs_train: chex.ArrayDevice,\n",
    "    dict_cfg,\n",
    "    samples: dict,\n",
    "    jitter: float = 1e-8,\n",
    "    reortho: bool = False\n",
    ") -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "    \n",
    "    d_in = dict_cfg.model.grass_config.d_in\n",
    "    U = np.array(dict_cfg.model.grass_config.anchor_point)\n",
    "    d, n = U.shape\n",
    "    cov_jitter = dict_cfg.model.grass_config.cov_jitter\n",
    "    k_include_noise = dict_cfg.model.grass_config.k_include_noise\n",
    "    kern_jitter = dict_cfg.model.grass_config.jitter\n",
    "    n_samples = dict_cfg.train.n_samples // dict_cfg.train.n_thinning\n",
    "    assert n_samples == samples['Deltas'].shape[0]\n",
    "    \n",
    "    def predict(\n",
    "        key: chex.ArrayDevice,\n",
    "        Omega: chex.ArrayDevice,\n",
    "        var: float,\n",
    "        length: float,\n",
    "        noise: float,\n",
    "    ) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        # iniatilize GrassGP\n",
    "        kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=kern_jitter, include_noise=k_include_noise)\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "        grass_gp = GrassGP(d_in=d_in, d_out=(d, n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=cov_jitter)\n",
    "\n",
    "        # predict\n",
    "        Ws_mean, Ws_pred = grass_gp.predict_grass(key, s_test, s_train, Vs_train, jitter=jitter, reortho=reortho)\n",
    "        return Ws_mean, Ws_pred\n",
    "\n",
    "    # initialize vmap args\n",
    "    vmap_args = (random.split(key, n_samples),)\n",
    "    \n",
    "    cfg_Omega = dict_cfg.model.grass_config.Omega\n",
    "    cfg_var = dict_cfg.model.grass_config.var\n",
    "    cfg_length = dict_cfg.model.grass_config.length\n",
    "    cfg_noise = dict_cfg.model.grass_config.noise\n",
    "    cfg_require_noise = dict_cfg.model.grass_config.require_noise\n",
    "    \n",
    "    if cfg_Omega is None:\n",
    "        vmap_args += (samples['Omega'],)\n",
    "    else:\n",
    "        cfg_Omega = np.array(cfg_Omega)\n",
    "        vmap_args += (np.repeat(cfg_Omega[None,:,:], n_samples, axis=0),)\n",
    "    \n",
    "    if cfg_var is None:\n",
    "        vmap_args += (samples['kernel_var'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_var * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_length is None:\n",
    "        vmap_args += (samples['kernel_length'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_length * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_require_noise:\n",
    "        if cfg_noise is None:\n",
    "            vmap_args += (samples['kernel_noise'],)\n",
    "        else:\n",
    "            vmap_args += (cfg_noise * np.ones(n_samples),)\n",
    "    else:\n",
    "        vmap_args += (np.zeros(n_samples),)\n",
    "    \n",
    "    assert len(vmap_args) == 5\n",
    "    Ws_means, Ws_preds = vmap(predict)(*vmap_args)\n",
    "    return Ws_means, Ws_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d071dd-8cc8-4811-a942-bd5581e28639",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_key_grass = random.PRNGKey(7695)\n",
    "Ws_means, Ws_preds = predict_grass(pred_key_grass, s_test, s_train, log_Ws_train, config, samples)\n",
    "assert np.isnan(Ws_means).sum() == 0\n",
    "assert np.isnan(Ws_preds).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e19cbc-1e8f-4090-9df5-acea14c08ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/Ws_means_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Ws_means, f)\n",
    "    \n",
    "with open(\"results/Ws_preds_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Ws_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf93908-951e-4801-808b-6acb72068d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/Ws_means_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     Ws_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/Ws_preds_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     Ws_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26219c03-5895-4147-a928-78f27ead0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "for i in range(d):\n",
    "    obs = Ws_train[:,i,0]\n",
    "    means = Ws_means[:,:,i,0]\n",
    "    means_avg = np.mean(means, axis=0)\n",
    "    preds = Ws_preds[:,:,i,0]\n",
    "    percentiles = np.percentile(preds, np.array(percentile_levels), axis=0)\n",
    "    lower = percentiles[0,:]\n",
    "    upper = percentiles[1,:]\n",
    "    plt.plot(s_test, Ws_test[:,i,0], label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "    plt.scatter(s_train, Ws_train[:,i,0], label='training data', c='g')\n",
    "    plt.plot(s_test, means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "    plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "    plt.xlabel(r\"$s$\")\n",
    "    plt.legend()\n",
    "    plt.vlines(s_train, 0.99*lower.min(), 1.01*upper.max(), colors='green', linestyles='dashed')\n",
    "    plt.title(f\"{i+1}th component of projections\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72b2ce-9d65-4138-aa49-6bbbb92a7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_means = np.array([[subspace_angle(w) for w in mean] for mean in Ws_means])\n",
    "alphas_preds = np.array([[subspace_angle(w) for w in pred] for pred in Ws_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402976cc-e1a1-4100-b336-0ea5f0cbfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/alphas_means_more_train_pts.pickle_same_anchor\", 'wb') as f:\n",
    "    pickle.dump(alphas_means, f)\n",
    "    \n",
    "with open(\"results/alphas_preds_more_train_pts.pickle_same_anchor\", 'wb') as f:\n",
    "    pickle.dump(alphas_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54373e-1997-4bef-be4b-1be021537e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/alphas_means_more_train_pts.pickle_same_anchor\", 'rb') as f:\n",
    "#     alphas_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/alphas_preds_more_train_pts.pickle_same_anchor\", 'rb') as f:\n",
    "#     alphas_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171b084-1825-424f-88c2-f0c93bd817bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "alphas_means_avg = np.mean(alphas_means, axis=0)\n",
    "percentiles = np.percentile(alphas_preds, np.array(percentile_levels), axis=0)\n",
    "lower = percentiles[0,:]\n",
    "upper = percentiles[1,:]\n",
    "plt.plot(s_test, alphas, label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "plt.scatter(s_train, alphas_train, label='training data', c='g')\n",
    "plt.plot(s_test, alphas_means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "plt.xlabel(r\"$s$\")\n",
    "plt.ylabel(\"subspace angle\")\n",
    "plt.legend()\n",
    "plt.vlines(s_train, 0, np.pi, colors='green', linestyles='dashed')\n",
    "plt.title(f\"predictions for subspace angles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbafc7-5d61-4857-8fd4-0b5c61305439",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230318d7-8b36-48f4-93e8-eb713c595f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means_mcmc_barycenters = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    points = Ws_means[:,i,:,:]\n",
    "    test_means_mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    test_means_mcmc_theta_argmin = thetas[test_means_mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(test_means_mcmc_theta_argmin)\n",
    "    test_means_mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, test_means_mcmc_losses)\n",
    "    # plt.scatter(test_means_mcmc_theta_argmin, test_means_mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d5d16-9043-4d6d-b840-84cb963f3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_mcmc_barycenters = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    points = Ws_preds[:,i,:,:]\n",
    "    test_preds_mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    test_preds_mcmc_theta_argmin = thetas[test_preds_mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(test_preds_mcmc_theta_argmin)\n",
    "    test_preds_mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, test_preds_mcmc_losses)\n",
    "    # plt.scatter(test_preds_mcmc_theta_argmin, test_preds_mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e9340-f718-4d0d-af65-8b7756b13178",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means_mcmc_barycenters = np.array(test_means_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b6381-50bc-49b1-a752-d0e6552a8c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_mcmc_barycenters = np.array(test_preds_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0554857-78fb-4020-98d4-a5daf19356c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/test_means_mcmc_barycenters_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(test_means_mcmc_barycenters, f)\n",
    "\n",
    "with open(\"results/test_preds_mcmc_barycenters_more_train_pts_same_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(test_preds_mcmc_barycenters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3116c-80e6-45c8-8282-5c629a830217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/test_means_mcmc_barycenters_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     test_means_mcmc_barycenters = pickle.load(f)\n",
    "\n",
    "# with open(\"results/test_preds_mcmc_barycenters_more_train_pts_same_anchor.pickle\", 'rb') as f:\n",
    "#     test_preds_mcmc_barycenters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d145d-87f7-4a38-ba45-f09ee14618fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_mean_errors = vmap(grass_dist)(Ws_test, test_means_mcmc_barycenters)\n",
    "out_sample_pred_errors = vmap(grass_dist)(Ws_test, test_preds_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843d770-4f85-4756-9429-15a62b79fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_test,out_sample_mean_errors, label='error using means')\n",
    "plt.plot(s_test,out_sample_pred_errors, label='error using preds')\n",
    "plt.vlines(s_train, 0, 1, colors=\"green\", linestyles=\"dashed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf057f04-d643-477d-a114-6e63548e7023",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sd_s_test = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    fixed = test_preds_mcmc_barycenters[i]\n",
    "    dists = vmap(lambda W: grass_dist(W, fixed))(Ws_preds[:,i,:,:])\n",
    "    dists_Sq = dists**2\n",
    "    sd_s_test.append(np.sqrt(dists_Sq.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5622153-7419-41b6-ba28-59d4a3b2dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_test = np.array(sd_s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496c326-6c2a-461a-ada3-d4a86cb5b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd_data = {'s': s_test, 'errors_mean': out_sample_mean_errors, 'errors_pred': out_sample_pred_errors, 'sd': sd_s_test}\n",
    "out_sample_errors_df = pd.DataFrame(data=test_pd_data)\n",
    "out_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045fc17-6f40-4c6c-8e8f-9e1a81cf3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_errors_df.drop(['s'], axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db0739-6bc7-4c88-833e-624657bbe471",
   "metadata": {},
   "source": [
    "# Increase training points - new anchor point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6f193-323c-4cf1-914e-3e15738f2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample data\n",
    "s_gap = 2\n",
    "s_train = s_test[::s_gap].copy()\n",
    "print(f\"Number of training points: {s_train.shape[0]}\")\n",
    "Ws_train = Ws_test[::s_gap,:,:].copy()\n",
    "\n",
    "for i in range(d):\n",
    "    plt.plot(s_test, Ws_test[:,i,0])\n",
    "    plt.scatter(s_train, Ws_train[:,i,0], c='r')\n",
    "    plt.title(f'{i+1}th component of projection')\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$s$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfccd3-cc23-4c8c-b864-abe35d258474",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0, np.pi, 1000)\n",
    "losses = vmap(lambda theta: loss(theta, Ws_train))(thetas)\n",
    "\n",
    "theta_argmin = thetas[losses.argmin()]\n",
    "anchor_point = subspace_angle_to_grass_pt(theta_argmin)\n",
    "assert valid_grass_point(anchor_point)\n",
    "\n",
    "plt.plot(thetas,losses)\n",
    "plt.scatter(theta_argmin, losses.min(), color=\"red\", label='loss of anchor point')\n",
    "plt.grid()\n",
    "plt.xlabel(\"subspace angle\")\n",
    "plt.title(\"Plot of loss vs subspace angle to determine Karcher mean\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylim((-1.25,1.25))\n",
    "ax.set_xlim((-1.25,1.25))\n",
    "ax.scatter(Ws_train[:,0,0], Ws_train[:,1,0], color=\"blue\", alpha=0.25, label='training points')\n",
    "ax.scatter(anchor_point[0,0], anchor_point[1,0], color=\"red\", marker=\"*\", label=\"anchor point\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Plot of training points with anchor point on circle rep of Gr(2,1)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85dc1f6-b136-41e9-aa36-0ce4da1d6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of training data and full data\n",
    "log_Ws_train = vmap(lambda W: grass_log(anchor_point, W))(Ws_train)\n",
    "log_Ws_test = vmap(lambda W: grass_log(anchor_point, W))(Ws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651dd4a1-958d-499c-bfbc-04706293b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([subspace_angle(w) for w in Ws_test])\n",
    "alphas_train = np.array([subspace_angle(w) for w in Ws_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf51209-dde0-49da-a142-c0b32a6bce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'anchor_point': anchor_point.tolist(),\n",
    "    'd_in': 1,\n",
    "    'Omega' : None,\n",
    "    'k_include_noise': True,\n",
    "    'var' : 1.0,\n",
    "    'length' : None, \n",
    "    'noise' : None,\n",
    "    'require_noise' : False,\n",
    "    'jitter' : 1e-06,\n",
    "    'cov_jitter' : 1e-4,\n",
    "    'L_jitter' : 1e-8,\n",
    "    'reorthonormalize' : False,\n",
    "    'b' : 0.5,\n",
    "    # 'ell': 0.0075\n",
    "    'ell': 0.01\n",
    "}\n",
    "\n",
    "def model(s, log_Ws, grass_config = model_config):\n",
    "    U = np.array(grass_config['anchor_point'])\n",
    "    d, n = U.shape\n",
    "    N = s.shape[0]\n",
    "    d_n = d * n\n",
    "    # N_params = N * d_n\n",
    "    if log_Ws is not None:\n",
    "        assert log_Ws.shape == (N, d, n)\n",
    "    \n",
    "    # get/sample Omega\n",
    "    if grass_config['Omega'] is None:\n",
    "        sigmas = numpyro.sample('sigmas', dist.LogNormal(0.0, 1.0).expand([d_n]))\n",
    "        L_factor = numpyro.sample('L_factor', dist.LKJ(d_n, 1.0))\n",
    "        L = numpyro.deterministic('L', L_factor + grass_config['L_jitter'] * np.eye(d_n))\n",
    "        Omega = numpyro.deterministic('Omega', np.outer(sigmas, sigmas) * L)\n",
    "    else:\n",
    "        Omega = np.array(grass_config['Omega'])\n",
    "        \n",
    "    # get/sample kernel params\n",
    "    if grass_config['var'] is None:\n",
    "        # sample var\n",
    "        var = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, grass_config['b']))\n",
    "    else:\n",
    "        var = grass_config['var']\n",
    "\n",
    "    if grass_config['length'] is None:\n",
    "        # sample length\n",
    "        length = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, grass_config['b']))\n",
    "    else:\n",
    "        length = grass_config['length']\n",
    "\n",
    "    if grass_config['require_noise']:\n",
    "        if grass_config['noise'] is None:\n",
    "            # sample noise\n",
    "            noise = numpyro.sample(\"kernel_noise\", dist.LogNormal(0.0, grass_config['b']))\n",
    "        else:\n",
    "            noise = grass_config['noise']\n",
    "    else:\n",
    "        noise = 0.0\n",
    "    \n",
    "    kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "    # create kernel function\n",
    "    k = lambda t, s: rbf(t, s, kernel_params, jitter=grass_config['jitter'], include_noise=grass_config['k_include_noise'])\n",
    "    # create mean function\n",
    "    mu = lambda s: zero_mean(s, d, n)\n",
    "\n",
    "    # initialize GrassGP\n",
    "    grass_gp = GrassGP(d_in=grass_config['d_in'], d_out=(d,n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=grass_config['cov_jitter'])\n",
    "    \n",
    "    # sample Deltas\n",
    "    Deltas = grass_gp.tangent_model(s)\n",
    "    \n",
    "    # # # # ! check what power this should be\n",
    "    # likelihood\n",
    "    ell = grass_config['ell']\n",
    "    with numpyro.plate(\"N\", N):\n",
    "        numpyro.sample(\"log_W\", dist.continuous.MatrixNormal(loc=Deltas, scale_tril_row=ell * np.eye(d),scale_tril_column=np.eye(n)), obs=log_Ws)\n",
    "\n",
    "TangentSpaceModelConf = builds(model, grass_config=model_config, zen_partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8d310-d73c-4807-93f2-dbecde726250",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVIConfig = make_config(\n",
    "    seed = 123514354575,\n",
    "    maxiter = 15000,\n",
    "    step_size = 0.001\n",
    ")\n",
    "\n",
    "TrainConfig = make_config(\n",
    "    seed = 9870687,\n",
    "    n_warmup = 2000,\n",
    "    n_samples = 7000,\n",
    "    n_chains = 1,\n",
    "    n_thinning = 2\n",
    ")\n",
    "\n",
    "Config = make_config(\n",
    "    model = TangentSpaceModelConf,\n",
    "    svi = SVIConfig,\n",
    "    train = TrainConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebe3d1-f518-4dd1-b5f2-a2d23d975d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    # instantiate grass model\n",
    "    model = instantiate(cfg.model)\n",
    "    \n",
    "    # run SVI to get MAP esimtate to initialise MCMC\n",
    "    svi_key = random.PRNGKey(cfg.svi.seed)\n",
    "    maxiter = cfg.svi.maxiter\n",
    "    step_size = cfg.svi.step_size\n",
    "    print(f\"n_train = {s_train.shape[0]}\")\n",
    "    print(\"Running SVI for MAP estimate to initialise MCMC\")\n",
    "    svi_results = run_svi_for_map(svi_key, model, maxiter, step_size, s_train, log_Ws_train)\n",
    "    \n",
    "    # plot svi losses\n",
    "    plt.plot(svi_results.losses)\n",
    "    plt.show()\n",
    "    \n",
    "    # get initialisation from SVI results\n",
    "    map_est = svi_results.params\n",
    "    strip_val = len('_auto_loc')\n",
    "    init_values = {key[:-strip_val]:value for (key, value) in map_est.items()}\n",
    "    \n",
    "    # run HMC\n",
    "    train_key = random.PRNGKey(cfg.train.seed)\n",
    "    mcmc_config = {'num_warmup' : cfg.train.n_warmup, 'num_samples' : cfg.train.n_samples, 'num_chains' : cfg.train.n_chains, 'thinning' : cfg.train.n_thinning, 'init_strategy' : init_to_value(values=init_values)}\n",
    "    print(\"HMC starting.\")\n",
    "    mcmc = run_inference(train_key, mcmc_config, model, s_train, log_Ws_train)    \n",
    "    # original_stdout = sys.stdout\n",
    "    # with open('hmc_log.txt', 'w') as f:\n",
    "    #     sys.stdout = f\n",
    "    #     mcmc.print_summary()\n",
    "    #     sys.stdout = original_stdout\n",
    "    \n",
    "    samples = mcmc.get_samples()\n",
    "    inference_data = samples.copy()\n",
    "    for param, initial_val in init_values.items():\n",
    "        inference_data[f\"{param}-initial_value\"] = initial_val\n",
    "    \n",
    "    # head = os.getcwd()\n",
    "    # main_name = \"inference_data\"\n",
    "    # path = get_save_path(head, main_name)\n",
    "    # try:\n",
    "    #     safe_save(path, inference_data)\n",
    "    # except FileExistsError:\n",
    "    #     print(\"File exists so not saving.\")\n",
    "    return inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16385a12-bdcf-47bc-9e78-794c410f880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpyro.render_model(instantiate(Config.model), model_args=(s_train,log_Ws_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c1907-86db-4547-82a7-6629fa0fb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = train(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e51637-8458-4aa4-af3a-7d93d51a7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dict(filter(lambda elem: 'initial_value' not in elem[0], inference_data.items()))\n",
    "initial_values = dict(filter(lambda elem: 'initial_value' in elem[0], inference_data.items()))\n",
    "assert set(samples.keys()).union(initial_values.keys()) == set(inference_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58d1b0-a9dc-4cd8-a269-31ecddf7c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/samples_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18bad7-f883-4e1d-899d-36883d443356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/samples_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9738471-1461-4b2c-b7f7-539362debae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_samples = flatten_samples(samples, ignore=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ccfe4-0448-4aaf-8fc3-4200c0db2940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trace_plot_vars = ['kernel_length']\n",
    "for key in my_samples.keys():\n",
    "    if 'Omega' in key:\n",
    "        trace_plot_vars.append(key)\n",
    "    if 'sigmas' in key:\n",
    "        trace_plot_vars.append(key)\n",
    "        \n",
    "my_samples[trace_plot_vars].plot(subplots=True, figsize=(10,40), sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea49c9-5a77-4148-8756-395fb30cfab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for var in trace_plot_vars:\n",
    "    sm.graphics.tsa.plot_acf(my_samples[var], lags=100)\n",
    "    plt.title(f\"acf for {var}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c2909-2085-4985-89b2-5613d08499d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-5\n",
    "\n",
    "samples_Ws_train = vmap(lambda Deltas: convert_to_projs(Deltas, anchor_point, reorthonormalize=False))(samples['Deltas'])\n",
    "\n",
    "for ws in samples_Ws_train:\n",
    "    assert vmap(lambda w: valid_grass_point(w, tol=tol))(ws).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5ef7f-3891-40da-9e17-d3b24c38d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_barycenters = []\n",
    "for i in tqdm(range(s_train.shape[0])):\n",
    "    points = samples_Ws_train[:,i,:,:]\n",
    "    mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    mcmc_theta_argmin = thetas[mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(mcmc_theta_argmin)\n",
    "    mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, mcmc_losses)\n",
    "    # plt.scatter(mcmc_theta_argmin, mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f812c4-6fd1-4648-b975-db611eb6fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_barycenters = np.array(mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354ed47-5740-44f2-a86a-e6252a37d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/mcmc_barycenters_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(mcmc_barycenters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2fb08-d7d9-406d-99ac-a496af4c10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/mcmc_barycenters_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     mcmc_barycenters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915cba38-8dc8-4d6f-8be8-964a0409e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_errors = vmap(grass_dist)(Ws_train, mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3a8e6-d526-4fa3-bf22-69394786f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_train,in_sample_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c016741-c218-4bf0-a07b-f03d5b8efd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_train = []\n",
    "for i in tqdm(range(s_train.shape[0])):\n",
    "    fixed = mcmc_barycenters[i]\n",
    "    dists = vmap(lambda W: grass_dist(W, fixed))(samples_Ws_train[:,i,:,:])\n",
    "    dists_Sq = dists**2\n",
    "    sd_s_train.append(np.sqrt(dists_Sq.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13bf8c-8ef4-48cf-a598-be1c59406b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_train = np.array(sd_s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675bc1de-9c6c-4e07-90e5-5d484c47d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = {'s': s_train, 'errors': in_sample_errors, 'sd': sd_s_train}\n",
    "in_sample_errors_df = pd.DataFrame(data=pd_data)\n",
    "in_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc1733-b82d-4cb8-8a17-73e53f906dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_errors_df.drop([\"s\"],axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccfbe7-56d4-4bb1-987a-29394cc81e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_grass_dists(samples_Ws_train, Ws_train, s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23435c-c570-4e2f-aca7-ce1b6a447931",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_alphas_train = np.array([[subspace_angle(w)for w in Ws_sample] for Ws_sample in samples_Ws_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee82786-b9db-4237-b0e2-83c1bf44bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/samples_alphas_train_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(samples_alphas_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369b73d-e20b-48bc-9de1-9b75a5be3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/samples_alphas_train_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     samples_alphas_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaef921-ca45-469d-b2d7-fedc36084479",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "percentiles = np.percentile(samples_alphas_train, np.array(percentile_levels), axis=0)\n",
    "lower = percentiles[0,:]\n",
    "upper = percentiles[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0f9a4-7a13-458b-bf72-53192b205b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_test, alphas, c='black', alpha=0.5, label='full data')\n",
    "plt.scatter(s_train, alphas_train, label='training data', c='g')\n",
    "plt.scatter(s_train, samples_alphas_train.mean(axis=0), label='mean samples', c='r')\n",
    "plt.fill_between(s_train, lower, upper,  color='lightblue', alpha=0.75,label=f'{conf_level}% credible interval')\n",
    "plt.xlabel(r\"$s$\")\n",
    "plt.ylabel(\"subspace angle\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f4757-7e0d-4ea2-8547-466d663879ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tangents(\n",
    "    key: chex.ArrayDevice,\n",
    "    s_test: chex.ArrayDevice,\n",
    "    s_train: chex.ArrayDevice,\n",
    "    Vs_train: chex.ArrayDevice,\n",
    "    dict_cfg,\n",
    "    samples: dict,\n",
    "    jitter: float = 1e-8\n",
    ") -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "    \n",
    "    d_in = dict_cfg.model.grass_config.d_in\n",
    "    U = np.array(dict_cfg.model.grass_config.anchor_point)\n",
    "    d, n = U.shape\n",
    "    cov_jitter = dict_cfg.model.grass_config.cov_jitter\n",
    "    k_include_noise = dict_cfg.model.grass_config.k_include_noise\n",
    "    kern_jitter = dict_cfg.model.grass_config.jitter\n",
    "    n_samples = dict_cfg.train.n_samples // dict_cfg.train.n_thinning\n",
    "    assert n_samples == samples['Deltas'].shape[0]\n",
    "    \n",
    "    def predict(\n",
    "        key: chex.ArrayDevice,\n",
    "        Omega: chex.ArrayDevice,\n",
    "        var: float,\n",
    "        length: float,\n",
    "        noise: float,\n",
    "    ) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        # iniatilize GrassGP\n",
    "        kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=kern_jitter, include_noise=k_include_noise)\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "        grass_gp = GrassGP(d_in=d_in, d_out=(d, n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=cov_jitter)\n",
    "\n",
    "        # predict\n",
    "        Deltas_mean, Deltas_pred = grass_gp.predict_tangents(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "        return Deltas_mean, Deltas_pred\n",
    "\n",
    "    # initialize vmap args\n",
    "    vmap_args = (random.split(key, n_samples),)\n",
    "    \n",
    "    cfg_Omega = dict_cfg.model.grass_config.Omega\n",
    "    cfg_var = dict_cfg.model.grass_config.var\n",
    "    cfg_length = dict_cfg.model.grass_config.length\n",
    "    cfg_noise = dict_cfg.model.grass_config.noise\n",
    "    cfg_require_noise = dict_cfg.model.grass_config.require_noise\n",
    "    \n",
    "    if cfg_Omega is None:\n",
    "        vmap_args += (samples['Omega'],)\n",
    "    else:\n",
    "        cfg_Omega = np.array(cfg_Omega)\n",
    "        vmap_args += (np.repeat(cfg_Omega[None,:,:], n_samples, axis=0),)\n",
    "    \n",
    "    if cfg_var is None:\n",
    "        vmap_args += (samples['kernel_var'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_var * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_length is None:\n",
    "        vmap_args += (samples['kernel_length'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_length * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_require_noise:\n",
    "        if cfg_noise is None:\n",
    "            vmap_args += (samples['kernel_noise'],)\n",
    "        else:\n",
    "            vmap_args += (cfg_noise * np.ones(n_samples),)\n",
    "    else:\n",
    "        vmap_args += (np.zeros(n_samples),)\n",
    "    \n",
    "    assert len(vmap_args) == 5\n",
    "    Deltas_means, Deltas_preds = vmap(predict)(*vmap_args)\n",
    "    return Deltas_means, Deltas_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8e1b0-03fc-4bd1-9afc-5ce153283f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = to_dictconf(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61a2fc-f1fa-414d-8275-2189e6866d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_key = random.PRNGKey(6578)\n",
    "Deltas_means, Deltas_preds = predict_tangents(pred_key, s_test, s_train, log_Ws_train, config, samples)\n",
    "assert np.isnan(Deltas_means).sum() == 0\n",
    "assert np.isnan(Deltas_preds).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be9eae-d2ff-4de9-8e40-ff3711d3fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/Deltas_means_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Deltas_means, f)\n",
    "    \n",
    "with open(\"results/Deltas_preds_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Deltas_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa44bbd-8f86-4f49-b28f-33c7a9067b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/Deltas_means_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     Deltas_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/Deltas_preds_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     Deltas_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045960d0-c8fb-481b-8a9d-71ed6a458459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "for i in range(d):\n",
    "    obs = log_Ws_train[:,i,0]\n",
    "    means = Deltas_means[:,:,i,0]\n",
    "    means_avg = np.mean(means, axis=0)\n",
    "    preds = Deltas_preds[:,:,i,0]\n",
    "    percentiles = np.percentile(preds, np.array(percentile_levels), axis=0)\n",
    "    lower = percentiles[0,:]\n",
    "    upper = percentiles[1,:]\n",
    "    plt.plot(s_test, log_Ws_test[:,i,0], label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "    plt.scatter(s_train, log_Ws_train[:,i,0], label='training data', c='g')\n",
    "    plt.plot(s_test, means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "    plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "    plt.xlabel(r\"$s$\")\n",
    "    plt.legend()\n",
    "    plt.vlines(s_train, 0.99*lower.min(), 1.01*upper.max(), colors='green', linestyles='dashed')\n",
    "    plt.title(f\"{i+1}th component of tangents\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2cf7a-8bcf-44d2-8082-d6e5063a1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_grass(\n",
    "    key: chex.ArrayDevice,\n",
    "    s_test: chex.ArrayDevice,\n",
    "    s_train: chex.ArrayDevice,\n",
    "    Vs_train: chex.ArrayDevice,\n",
    "    dict_cfg,\n",
    "    samples: dict,\n",
    "    jitter: float = 1e-8,\n",
    "    reortho: bool = False\n",
    ") -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "    \n",
    "    d_in = dict_cfg.model.grass_config.d_in\n",
    "    U = np.array(dict_cfg.model.grass_config.anchor_point)\n",
    "    d, n = U.shape\n",
    "    cov_jitter = dict_cfg.model.grass_config.cov_jitter\n",
    "    k_include_noise = dict_cfg.model.grass_config.k_include_noise\n",
    "    kern_jitter = dict_cfg.model.grass_config.jitter\n",
    "    n_samples = dict_cfg.train.n_samples // dict_cfg.train.n_thinning\n",
    "    assert n_samples == samples['Deltas'].shape[0]\n",
    "    \n",
    "    def predict(\n",
    "        key: chex.ArrayDevice,\n",
    "        Omega: chex.ArrayDevice,\n",
    "        var: float,\n",
    "        length: float,\n",
    "        noise: float,\n",
    "    ) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        # iniatilize GrassGP\n",
    "        kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=kern_jitter, include_noise=k_include_noise)\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "        grass_gp = GrassGP(d_in=d_in, d_out=(d, n), mu=mu, k=k, Omega=Omega, U=U, cov_jitter=cov_jitter)\n",
    "\n",
    "        # predict\n",
    "        Ws_mean, Ws_pred = grass_gp.predict_grass(key, s_test, s_train, Vs_train, jitter=jitter, reortho=reortho)\n",
    "        return Ws_mean, Ws_pred\n",
    "\n",
    "    # initialize vmap args\n",
    "    vmap_args = (random.split(key, n_samples),)\n",
    "    \n",
    "    cfg_Omega = dict_cfg.model.grass_config.Omega\n",
    "    cfg_var = dict_cfg.model.grass_config.var\n",
    "    cfg_length = dict_cfg.model.grass_config.length\n",
    "    cfg_noise = dict_cfg.model.grass_config.noise\n",
    "    cfg_require_noise = dict_cfg.model.grass_config.require_noise\n",
    "    \n",
    "    if cfg_Omega is None:\n",
    "        vmap_args += (samples['Omega'],)\n",
    "    else:\n",
    "        cfg_Omega = np.array(cfg_Omega)\n",
    "        vmap_args += (np.repeat(cfg_Omega[None,:,:], n_samples, axis=0),)\n",
    "    \n",
    "    if cfg_var is None:\n",
    "        vmap_args += (samples['kernel_var'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_var * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_length is None:\n",
    "        vmap_args += (samples['kernel_length'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_length * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_require_noise:\n",
    "        if cfg_noise is None:\n",
    "            vmap_args += (samples['kernel_noise'],)\n",
    "        else:\n",
    "            vmap_args += (cfg_noise * np.ones(n_samples),)\n",
    "    else:\n",
    "        vmap_args += (np.zeros(n_samples),)\n",
    "    \n",
    "    assert len(vmap_args) == 5\n",
    "    Ws_means, Ws_preds = vmap(predict)(*vmap_args)\n",
    "    return Ws_means, Ws_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe80f4-b59e-4616-905a-9a416f332257",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_key_grass = random.PRNGKey(7695)\n",
    "Ws_means, Ws_preds = predict_grass(pred_key_grass, s_test, s_train, log_Ws_train, config, samples)\n",
    "assert np.isnan(Ws_means).sum() == 0\n",
    "assert np.isnan(Ws_preds).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976bfd0-608a-4be3-8c91-a677a9169cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/Ws_means_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Ws_means, f)\n",
    "    \n",
    "with open(\"results/Ws_preds_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(Ws_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab56b52-af87-4eba-9f96-c4eefcd19025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/Ws_means_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     Ws_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/Ws_preds_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     Ws_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16967f7f-2235-4ca0-9b62-b99adf7a5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "for i in range(d):\n",
    "    obs = Ws_train[:,i,0]\n",
    "    means = Ws_means[:,:,i,0]\n",
    "    means_avg = np.mean(means, axis=0)\n",
    "    preds = Ws_preds[:,:,i,0]\n",
    "    percentiles = np.percentile(preds, np.array(percentile_levels), axis=0)\n",
    "    lower = percentiles[0,:]\n",
    "    upper = percentiles[1,:]\n",
    "    plt.plot(s_test, Ws_test[:,i,0], label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "    plt.scatter(s_train, Ws_train[:,i,0], label='training data', c='g')\n",
    "    plt.plot(s_test, means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "    plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "    plt.xlabel(r\"$s$\")\n",
    "    plt.legend()\n",
    "    plt.vlines(s_train, 0.99*lower.min(), 1.01*upper.max(), colors='green', linestyles='dashed')\n",
    "    plt.title(f\"{i+1}th component of projections\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29855b99-092e-47f2-a08c-9ac97f0b289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_means = np.array([[subspace_angle(w) for w in mean] for mean in Ws_means])\n",
    "alphas_preds = np.array([[subspace_angle(w) for w in pred] for pred in Ws_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3844c-9034-4259-8354-519e655f1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/alphas_means_more_train_pts.pickle_new_anchor\", 'wb') as f:\n",
    "    pickle.dump(alphas_means, f)\n",
    "    \n",
    "with open(\"results/alphas_preds_more_train_pts.pickle_new_anchor\", 'wb') as f:\n",
    "    pickle.dump(alphas_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb4f82-c245-4bf7-830f-73d1e2c131c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/alphas_means_more_train_pts.pickle_new_anchor\", 'rb') as f:\n",
    "#     alphas_means = pickle.load(f)\n",
    "    \n",
    "# with open(\"results/alphas_preds_more_train_pts.pickle_new_anchor\", 'rb') as f:\n",
    "#     alphas_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e3d19-a38e-4bc4-a878-09b2922db08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "percentile_levels = [2.5, 97.5]\n",
    "conf_level = percentile_levels[-1] - percentile_levels[0]\n",
    "alphas_means_avg = np.mean(alphas_means, axis=0)\n",
    "percentiles = np.percentile(alphas_preds, np.array(percentile_levels), axis=0)\n",
    "lower = percentiles[0,:]\n",
    "upper = percentiles[1,:]\n",
    "plt.plot(s_test, alphas, label='full data',c='black', alpha=0.75, linestyle='dashed')\n",
    "plt.scatter(s_train, alphas_train, label='training data', c='g')\n",
    "plt.plot(s_test, alphas_means_avg, label='averaged mean prediction', c='r', alpha=0.75)\n",
    "plt.fill_between(s_test, lower, upper, color='lightblue', alpha=0.75, label=f'{conf_level}% credible interval')\n",
    "plt.xlabel(r\"$s$\")\n",
    "plt.ylabel(\"subspace angle\")\n",
    "plt.legend()\n",
    "plt.vlines(s_train, 0, np.pi, colors='green', linestyles='dashed')\n",
    "plt.title(f\"predictions for subspace angles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08118c1-9630-4e67-87a3-9e82bc09b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b155b-952a-4221-b752-d5b6e57b401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means_mcmc_barycenters = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    points = Ws_means[:,i,:,:]\n",
    "    test_means_mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    test_means_mcmc_theta_argmin = thetas[test_means_mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(test_means_mcmc_theta_argmin)\n",
    "    test_means_mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, test_means_mcmc_losses)\n",
    "    # plt.scatter(test_means_mcmc_theta_argmin, test_means_mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25cf01-b304-447a-bcea-a0efb3a00c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_mcmc_barycenters = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    points = Ws_preds[:,i,:,:]\n",
    "    test_preds_mcmc_losses = vmap(lambda theta: loss(theta, points))(thetas)\n",
    "    test_preds_mcmc_theta_argmin = thetas[test_preds_mcmc_losses.argmin()]\n",
    "    barycenter = subspace_angle_to_grass_pt(test_preds_mcmc_theta_argmin)\n",
    "    test_preds_mcmc_barycenters.append(barycenter)\n",
    "    # plt.plot(thetas, test_preds_mcmc_losses)\n",
    "    # plt.scatter(test_preds_mcmc_theta_argmin, test_preds_mcmc_losses.min(),color=\"red\")\n",
    "    # plt.grid()\n",
    "    # plt.title(f\"{i}\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d7176-dd58-40c2-989d-e2c44140e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means_mcmc_barycenters = np.array(test_means_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68812e71-5108-46dc-a4e3-0d2928157869",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_mcmc_barycenters = np.array(test_preds_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebc7a2-b050-432a-a8d5-bb1485bc8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/test_means_mcmc_barycenters_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(test_means_mcmc_barycenters, f)\n",
    "\n",
    "with open(\"results/test_preds_mcmc_barycenters_more_train_pts_new_anchor.pickle\", 'wb') as f:\n",
    "    pickle.dump(test_preds_mcmc_barycenters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ca4c4-24d5-418c-9e13-0e1c8934f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/test_means_mcmc_barycenters_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     test_means_mcmc_barycenters = pickle.load(f)\n",
    "\n",
    "# with open(\"results/test_preds_mcmc_barycenters_more_train_pts_new_anchor.pickle\", 'rb') as f:\n",
    "#     test_preds_mcmc_barycenters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a2013-03ec-4b64-9866-53f63d104dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_mean_errors = vmap(grass_dist)(Ws_test, test_means_mcmc_barycenters)\n",
    "out_sample_pred_errors = vmap(grass_dist)(Ws_test, test_preds_mcmc_barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868ef33-7d06-433e-ac8c-696f17dbc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_test,out_sample_mean_errors, label='error using means')\n",
    "plt.plot(s_test,out_sample_pred_errors, label='error using preds')\n",
    "plt.vlines(s_train, 0, 1, colors=\"green\", linestyles=\"dashed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098eda7d-e0d6-475b-80a0-145f4693cde2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sd_s_test = []\n",
    "for i in tqdm(range(s_test.shape[0])):\n",
    "    fixed = test_preds_mcmc_barycenters[i]\n",
    "    dists = vmap(lambda W: grass_dist(W, fixed))(Ws_preds[:,i,:,:])\n",
    "    dists_Sq = dists**2\n",
    "    sd_s_test.append(np.sqrt(dists_Sq.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0d9ea-4187-42be-84ad-d1618c69829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_s_test = np.array(sd_s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ce4db-6237-43a2-8654-4a9176a6ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd_data = {'s': s_test, 'errors_mean': out_sample_mean_errors, 'errors_pred': out_sample_pred_errors, 'sd': sd_s_test}\n",
    "out_sample_errors_df = pd.DataFrame(data=test_pd_data)\n",
    "out_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58715c12-85af-465d-ac42-fc698447a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_errors_df.drop(['s'], axis=1).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
