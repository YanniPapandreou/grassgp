{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd8bcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/yanni/.cache/pypoetry/virtualenvs/grassgp-JZ6xTmeL-py3.10/lib/python3.10/site-packages/jax/_src/deprecations.py:51: DeprecationWarning: jax.interpreters.xla.xla_call_p is deprecated. Please use jax.experimental.pjit.pjit_p instead.\n",
      "  warnings.warn(message, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from hydra_zen import instantiate, make_config, builds, to_yaml, load_from_yaml, launch\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import vmap, random, grad, jit\n",
    "import jax.numpy.linalg as lin\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "from grassgp.grassmann import valid_grass_point, sample_karcher_mean, grass_dist, grass_exp, valid_grass_tangent, grass_log, convert_to_projs, compute_barycenter\n",
    "from grassgp.means import zero_mean\n",
    "from grassgp.kernels import rbf\n",
    "from grassgp.models_optimised import GrassGP\n",
    "from grassgp.plot_utils import flatten_samples, plot_grass_dists\n",
    "from grassgp.utils import to_dictconf, get_save_path, vec, unvec\n",
    "from grassgp.utils import safe_save_jax_array_dict as safe_save\n",
    "from grassgp.utils import load_and_convert_to_samples_dict as load_data\n",
    "\n",
    "import chex\n",
    "from chex import assert_shape, assert_rank\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, Union, Callable, Tuple\n",
    "\n",
    "import numpyro\n",
    "from numpyro.infer import SVI, Trace_ELBO, autoguide, init_to_value\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "from grassgp.inference import run_inference\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b14f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = np.load(\"xs.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886aeb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ws = np.load(\"Ws.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57faabd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert vmap(lambda W: valid_grass_point(W[:,None]))(Ws.T).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279862e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i=0\n",
    "# W0 = Ws.T[i][:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e69250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dists = vmap(lambda W: grass_dist(W[:,None], W0))(Ws.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e798b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# tcf = ax.tricontourf(xs[:,0],xs[:,1],dists)\n",
    "# fig.colorbar(tcf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda3baab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s_test = xs.copy()\n",
    "Ws_test = Ws.T.copy()[:,:,None]\n",
    "s_gap = 3\n",
    "s_train = s_test[::s_gap,:].copy()\n",
    "Ws_train = Ws_test[::s_gap,:,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd4de0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.scatter(s_test[:,0], s_test[:,1],label='test')\n",
    "# plt.scatter(s_train[:,0], s_train[:,1],label='train')\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d611002",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfb1dcd",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_svi_for_map(rng_key, model, maxiter, step_size, *args):\n",
    "    start = time.time()\n",
    "    guide = autoguide.AutoDelta(model)\n",
    "    optimzer = numpyro.optim.Adam(step_size)\n",
    "    svi = SVI(model, guide, optimzer, Trace_ELBO())\n",
    "    svi_results = svi.run(rng_key, maxiter, *args)\n",
    "    print('\\nSVI elapsed time:', time.time() - start)\n",
    "    return svi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8655a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anchor_point, _, _ = sample_karcher_mean(Ws_train)\n",
    "assert valid_grass_point(anchor_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd85d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute log of training data\n",
    "log_Ws_train = vmap(lambda W: grass_log(anchor_point, W))(Ws_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb14b634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def invert_kron_plus_sigmaSq_eye(C: chex.ArrayDevice, R: chex.ArrayDevice, sigma: float) -> chex.ArrayDevice:\n",
    "    Sc, Uc = lin.eigh(C)\n",
    "    Sr, Ur = lin.eigh(R)\n",
    "    \n",
    "    diag_term = np.kron(Sc, Sr) + sigma**2\n",
    "    diag_term_inv = np.diag(1/diag_term)\n",
    "    \n",
    "    inv = np.kron(Uc, Ur) @ diag_term_inv @ np.kron(Uc.T, Ur.T)\n",
    "    return inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9974066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class FullMatGP:\n",
    "    d_in: int\n",
    "    d_out: Tuple[int, int]\n",
    "    mu: Callable = field(repr=False)\n",
    "    k: Callable = field(repr=False)\n",
    "    Omega: chex.ArrayDevice = field(repr=False)\n",
    "    cov_jitter: float = field(default=1e-8, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_shape(self.Omega, (d_n, d_n),\n",
    "                    custom_message=f\"Omega has shape {self.Omega.shape}; expected shape {(d_n, d_n)}\")\n",
    "\n",
    "    def model(self, s: chex.ArrayDevice, use_kron_chol: bool = True) -> chex.ArrayDevice:\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_rank(s, self.d_in)\n",
    "        N = s.shape[0]\n",
    "\n",
    "        # compute mean matrix M = [mu(s[1]), mu(s[2]), ..., mu(s[N])]\n",
    "        M = np.hstack(vmap(self.mu)(s))\n",
    "        assert_shape(M, (d, n*N))\n",
    "\n",
    "        # compute kernel matrix\n",
    "        K = self.k(s, s)\n",
    "        # cond_num = numpyro.deterministic(\"cond_num\", lin.cond(K))\n",
    "        assert_shape(K, (N, N))\n",
    "\n",
    "        # compute covariance matrix and cholesky factor\n",
    "        if use_kron_chol:\n",
    "            Chol = kron_chol(K + self.cov_jitter * np.eye(N), self.Omega)\n",
    "        else:\n",
    "            Cov = np.kron(K + self.cov_jitter * np.eye(N), self.Omega)\n",
    "            Chol = lin.cholesky(Cov)\n",
    "\n",
    "        # sample vec_Vs\n",
    "        # Z = numpyro.sample(\"Z\", dist.MultivariateNormal(covariance_matrix=np.eye(N*d_n)))\n",
    "        Z = numpyro.sample(\"Z\", dist.Normal().expand([N*d_n]))\n",
    "        vec_Vs = numpyro.deterministic(\"vec_Vs\", vec(M) + Chol @ Z)\n",
    "\n",
    "        # form Vs\n",
    "        Vs = numpyro.deterministic(\"Vs\", vmap(lambda params: unvec(params, d, n))(np.array(vec_Vs.split(N))))\n",
    "        return Vs\n",
    "\n",
    "    def sample(self, seed: int, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        model = self.model\n",
    "        seeded_model = handlers.seed(model, rng_seed=seed)\n",
    "        return seeded_model(s)\n",
    "\n",
    "    def predict(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        d, n = self.d_out\n",
    "        d_in = self.d_in\n",
    "        d_n = d * n\n",
    "        N_train = s_train.shape[0]\n",
    "        N_test = s_test.shape[0]\n",
    "        if d_in > 1:\n",
    "            assert s_train.shape[1] == d_in\n",
    "            assert s_test.shape[1] == d_in\n",
    "\n",
    "        # compute means\n",
    "        M_train = np.hstack(vmap(self.mu)(s_train))\n",
    "        M_test = np.hstack(vmap(self.mu)(s_test))\n",
    "        assert_shape(M_train, (d, n*N_train))\n",
    "        assert_shape(M_test, (d, n*N_test))\n",
    "\n",
    "        # compute kernels between train and test locs\n",
    "        K_train_train = self.k(s_train, s_train)\n",
    "        assert_shape(K_train_train, (N_train, N_train))\n",
    "        K_train_test = self.k(s_train, s_test)\n",
    "        assert_shape(K_train_test, (N_train, N_test))\n",
    "        K_test_train = K_train_test.T\n",
    "        K_test_test = self.k(s_test, s_test)\n",
    "        assert_shape(K_test_test, (N_test, N_test))\n",
    "\n",
    "        # compute posterior mean and cov\n",
    "        K_test_train_Omega = np.kron(K_test_train, self.Omega)\n",
    "        K_train_test_Omega = np.kron(K_train_test, self.Omega)\n",
    "        K_test_test_Omega = np.kron(K_test_test, self.Omega)\n",
    "        # FIX: change for singular Omega\n",
    "        # print(f\"Rank of Omega = {lin.matrix_rank(self.Omega)}. Shape of Omega = {self.Omega.shape}\")\n",
    "        # if lin.matrix_rank(self.Omega) == d_n:\n",
    "        #     mean_sols = kron_solve(K_train_train, self.Omega, vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "        #     cov_sols = vmap(lambda v: kron_solve(K_train_train, self.Omega, v), in_axes=1, out_axes=1)(K_train_test_Omega)\n",
    "        # else:\n",
    "        #     K_train_train_inv = lin.inv(K_train_train)\n",
    "        #     Omega_pinv = lin.pinv(self.Omega)\n",
    "        #     K_train_train_Omega_pinv = np.kron(K_train_train_inv, Omega_pinv)\n",
    "        #     mean_sols = K_train_train_Omega_pinv @ (vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "        #     cov_sols = K_train_train_Omega_pinv @ K_train_test_Omega\n",
    "        K_train_train_inv = lin.inv(K_train_train)\n",
    "        Omega_pinv = lin.pinv(self.Omega)\n",
    "        K_train_train_Omega_pinv = np.kron(K_train_train_inv, Omega_pinv)\n",
    "        mean_sols = K_train_train_Omega_pinv @ (vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "        cov_sols = K_train_train_Omega_pinv @ K_train_test_Omega\n",
    "        \n",
    "        vec_post_mean = vec(M_test) + K_test_train_Omega @ mean_sols\n",
    "        assert_shape(vec_post_mean, (d*n*N_test,),\n",
    "                     custom_message=f\"vec_post_mean should have shape {(d*n*N_test,)}; obtained {vec_post_mean.shape}\")\n",
    "\n",
    "        # cov_sols = vmap(lambda v: kron_solve(K_train_train, self.Omega, v), in_axes=1, out_axes=1)(K_train_test_Omega)\n",
    "        post_cov = K_test_test_Omega - K_test_train_Omega @ cov_sols\n",
    "        assert_shape(post_cov, (d*n*N_test, d*n*N_test),\n",
    "                     custom_message=f\"post_cov should have shape {(d*n*N_test,d*n*N_test)}; obtained {post_cov.shape}\")\n",
    "\n",
    "        # sample predictions\n",
    "        post_cov += jitter * np.eye(d*n*N_test)\n",
    "        \n",
    "        # FIX: change for singular post_cov\n",
    "        # print(f\"Rank of posterior cov = {lin.matrix_rank(post_cov)}. Shape of posterior cov = {post_cov.shape}\")\n",
    "        vec_pred = dist.MultivariateNormal(loc=vec_post_mean, covariance_matrix=post_cov).sample(key)\n",
    "        assert_shape(vec_pred, (d*n*N_test,),\n",
    "                     custom_message=f\"vec_pred should have shape {(d*n*N_test,)}; obtained {vec_pred.shape}\")\n",
    "\n",
    "        # unvec mean and preds and return\n",
    "        post_mean = vmap(lambda params: unvec(params, d, n))(np.array(vec_post_mean.split(N_test)))\n",
    "        pred = vmap(lambda params: unvec(params, d, n))(np.array(vec_pred.split(N_test)))\n",
    "        return post_mean, pred\n",
    "    \n",
    "    def predict_new(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, sigma: float, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        d, n = self.d_out\n",
    "        d_in = self.d_in\n",
    "        d_n = d * n\n",
    "        N_train = s_train.shape[0]\n",
    "        N_test = s_test.shape[0]\n",
    "        if d_in > 1:\n",
    "            assert s_train.shape[1] == d_in\n",
    "            assert s_test.shape[1] == d_in\n",
    "\n",
    "        # compute means\n",
    "        M_train = np.hstack(vmap(self.mu)(s_train))\n",
    "        M_test = np.hstack(vmap(self.mu)(s_test))\n",
    "        assert_shape(M_train, (d, n*N_train))\n",
    "        assert_shape(M_test, (d, n*N_test))\n",
    "\n",
    "        # compute kernels between train and test locs\n",
    "        K_train_train = self.k(s_train, s_train)\n",
    "        assert_shape(K_train_train, (N_train, N_train))\n",
    "        K_train_test = self.k(s_train, s_test)\n",
    "        assert_shape(K_train_test, (N_train, N_test))\n",
    "        K_test_train = K_train_test.T\n",
    "        K_test_test = self.k(s_test, s_test)\n",
    "        assert_shape(K_test_test, (N_test, N_test))\n",
    "\n",
    "        if sigma != 0:\n",
    "            K_test_train_Omega = np.kron(K_test_train, self.Omega)\n",
    "            K_train_test_Omega = np.kron(K_train_test, self.Omega)\n",
    "            K_test_test_Omega = np.kron(K_test_test, self.Omega)\n",
    "            \n",
    "            inv = invert_kron_plus_sigmaSq_eye(K_train_train, self.Omega, sigma)\n",
    "            mean_sols = inv @ (vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "            cov_sols = inv @ K_train_test_Omega\n",
    "            \n",
    "            vec_post_mean = vec(M_test) + K_test_train_Omega @ mean_sols\n",
    "            assert_shape(vec_post_mean, (d*n*N_test,),\n",
    "                         custom_message=f\"vec_post_mean should have shape {(d*n*N_test,)}; obtained {vec_post_mean.shape}\")\n",
    "\n",
    "            # cov_sols = vmap(lambda v: kron_solve(K_train_train, self.Omega, v), in_axes=1, out_axes=1)(K_train_test_Omega)\n",
    "            post_cov = K_test_test_Omega - K_test_train_Omega @ cov_sols\n",
    "            assert_shape(post_cov, (d*n*N_test, d*n*N_test),\n",
    "                         custom_message=f\"post_cov should have shape {(d*n*N_test,d*n*N_test)}; obtained {post_cov.shape}\")\n",
    "        else:\n",
    "            K_train_train_inv = lin.inv(K_train_train)\n",
    "            Omega_pinv = lin.pinv(self.Omega)\n",
    "            \n",
    "            mean_sol_mat = np.kron(K_test_train @ K_train_train_inv, self.Omega @ Omega_pinv)\n",
    "            mean_sols = mean_sol_mat @ (vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "            vec_post_mean = vec(M_test) + mean_sols\n",
    "            assert_shape(vec_post_mean, (d*n*N_test,),\n",
    "                         custom_message=f\"vec_post_mean should have shape {(d*n*N_test,)}; obtained {vec_post_mean.shape}\")\n",
    "            \n",
    "            post_cov = np.kron(K_test_test - K_test_train @ K_train_train_inv @ K_train_test, self.Omega)\n",
    "            assert_shape(post_cov, (d*n*N_test, d*n*N_test),\n",
    "                         custom_message=f\"post_cov should have shape {(d*n*N_test,d*n*N_test)}; obtained {post_cov.shape}\")\n",
    "\n",
    "        # sample predictions\n",
    "        post_cov += jitter * np.eye(d*n*N_test)\n",
    "        \n",
    "        vec_pred = dist.MultivariateNormal(loc=vec_post_mean, covariance_matrix=post_cov).sample(key)\n",
    "        assert_shape(vec_pred, (d*n*N_test,),\n",
    "                     custom_message=f\"vec_pred should have shape {(d*n*N_test,)}; obtained {vec_pred.shape}\")\n",
    "\n",
    "        # unvec mean and preds and return\n",
    "        post_mean = vmap(lambda params: unvec(params, d, n))(np.array(vec_post_mean.split(N_test)))\n",
    "        pred = vmap(lambda params: unvec(params, d, n))(np.array(vec_pred.split(N_test)))\n",
    "        return post_mean, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e11bfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class MatGP:\n",
    "    d_in: int\n",
    "    d_out: Tuple[int, int]\n",
    "    mu: Callable = field(repr=False)\n",
    "    k: Callable = field(repr=False)\n",
    "    Omega_diag_chol: chex.ArrayDevice = field(repr=False)\n",
    "    cov_jitter: float = field(default=1e-8, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_shape(self.Omega_diag_chol, (d_n,),\n",
    "                    custom_message=f\"Omega_diag_chol has shape {self.Omega_diag_chol.shape}; expected shape {(d_n,)}\")\n",
    "\n",
    "    def model(self, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_rank(s, self.d_in)\n",
    "        N = s.shape[0]\n",
    "\n",
    "        # compute mean matrix M = [mu(s[1]), mu(s[2]), ..., mu(s[N])]\n",
    "        M = np.hstack(vmap(self.mu)(s))\n",
    "        assert_shape(M, (d, n*N))\n",
    "        # ! TODO: check this out\n",
    "        vec_M = vec(M)\n",
    "\n",
    "        # compute kernel matrix\n",
    "        K = self.k(s, s)\n",
    "        assert_shape(K, (N, N))\n",
    "            \n",
    "        K_chol = lin.cholesky(K + self.cov_jitter * np.eye(N))\n",
    "        # Omega_diag_chol = np.sqrt(self.Omega_diag)\n",
    "\n",
    "        # sample vec_Vs\n",
    "        # Z = numpyro.sample(\"Z\", dist.MultivariateNormal(covariance_matrix=np.eye(N*d_n)))\n",
    "        Z = numpyro.sample(\"Z\", dist.Normal().expand([N*d_n]))\n",
    "        unvec_Z = unvec(Z, d_n, N)\n",
    "        # vec_Vs = numpyro.deterministic(\"vec_Vs\", vec(M + np.einsum('i,ij->ij', self.Omega_diag_chol, unvec_Z @ K_chol.T)))\n",
    "        vec_Vs = numpyro.deterministic(\"vec_Vs\", vec_M + vec(np.einsum('i,ij->ij', self.Omega_diag_chol, unvec_Z @ K_chol.T)))\n",
    "\n",
    "        # form Vs\n",
    "        Vs = numpyro.deterministic(\"Vs\", vmap(lambda params: unvec(params, d, n))(np.array(vec_Vs.split(N))))\n",
    "        return Vs\n",
    "\n",
    "    def sample(self, seed: int, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        model = self.model\n",
    "        seeded_model = handlers.seed(model, rng_seed=seed)\n",
    "        return seeded_model(s)\n",
    "\n",
    "#     def predict(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "#         # TODO optimise this further to take advantage of diag structure of Omega\n",
    "#         d, n = self.d_out\n",
    "#         d_in = self.d_in\n",
    "#         N_train = s_train.shape[0]\n",
    "#         N_test = s_test.shape[0]\n",
    "#         if d_in > 1:\n",
    "#             assert s_train.shape[1] == d_in\n",
    "#             assert s_test.shape[1] == d_in\n",
    "\n",
    "#         # compute means\n",
    "#         M_train = np.hstack(vmap(self.mu)(s_train))\n",
    "#         M_test = np.hstack(vmap(self.mu)(s_test))\n",
    "#         assert_shape(M_train, (d, n*N_train))\n",
    "#         assert_shape(M_test, (d, n*N_test))\n",
    "\n",
    "#         # compute kernels between train and test locs\n",
    "#         K_train_train = self.k(s_train, s_train)\n",
    "#         assert_shape(K_train_train, (N_train, N_train))\n",
    "#         K_train_test = self.k(s_train, s_test)\n",
    "#         assert_shape(K_train_test, (N_train, N_test))\n",
    "#         K_test_train = K_train_test.T\n",
    "#         K_test_test = self.k(s_test, s_test)\n",
    "#         assert_shape(K_test_test, (N_test, N_test))\n",
    "\n",
    "#         # compute posterior mean and cov\n",
    "#         Omega_diag = self.Omega_diag_chol ** 2\n",
    "#         Omega = np.diag(Omega_diag)\n",
    "#         K_test_train_Omega = np.kron(K_test_train, Omega)\n",
    "#         K_train_test_Omega = np.kron(K_train_test, Omega)\n",
    "#         K_test_test_Omega = np.kron(K_test_test, Omega)\n",
    "#         mean_sols = kron_solve(K_train_train, Omega, vec(np.hstack(Vs_train)) - vec(M_train))\n",
    "#         vec_post_mean = vec(M_test) + K_test_train_Omega @ mean_sols\n",
    "#         assert_shape(vec_post_mean, (d*n*N_test,),\n",
    "#                      custom_message=f\"vec_post_mean should have shape {(d*n*N_test,)}; obtained {vec_post_mean.shape}\")\n",
    "\n",
    "#         cov_sols = vmap(lambda v: kron_solve(K_train_train, Omega, v), in_axes=1, out_axes=1)(K_train_test_Omega)\n",
    "#         post_cov = K_test_test_Omega - K_test_train_Omega @ cov_sols\n",
    "#         assert_shape(post_cov, (d*n*N_test, d*n*N_test),\n",
    "#                      custom_message=f\"post_cov should have shape {(d*n*N_test,d*n*N_test)}; obtained {post_cov.shape}\")\n",
    "\n",
    "#         # sample predictions\n",
    "#         post_cov += jitter * np.eye(d*n*N_test)\n",
    "#         vec_pred = dist.MultivariateNormal(loc=vec_post_mean, covariance_matrix=post_cov).sample(key)\n",
    "#         assert_shape(vec_pred, (d*n*N_test,),\n",
    "#                      custom_message=f\"vec_pred should have shape {(d*n*N_test,)}; obtained {vec_pred.shape}\")\n",
    "\n",
    "#         # unvec mean and preds and return\n",
    "#         post_mean = vmap(lambda params: unvec(params, d, n))(np.array(vec_post_mean.split(N_test)))\n",
    "#         pred = vmap(lambda params: unvec(params, d, n))(np.array(vec_pred.split(N_test)))\n",
    "#         return post_mean, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd9dca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class GrassGP:\n",
    "    d_in: int\n",
    "    d_out: Tuple[int, int]\n",
    "    mu: Callable = field(repr=False)\n",
    "    k: Callable = field(repr=False)\n",
    "    Omega_diag_chol: chex.ArrayDevice = field(repr=False)\n",
    "    U: chex.ArrayDevice\n",
    "    cov_jitter: float = field(default=1e-4, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        d, n = self.d_out\n",
    "        d_n = d * n\n",
    "        assert_shape(self.Omega_diag_chol, (d_n,),\n",
    "                    custom_message=f\"Omega_diag_chol has shape {self.Omega_diag_chol.shape}; expected shape {(d_n,)}\")\n",
    "        assert_shape(self.U, (d, n),\n",
    "                    custom_message=f\"U has shape {self.U.shape}; expected shape {(d, n)}\")\n",
    "        tol = 1e-06\n",
    "        # assert valid_grass_point(self.U), f\"U is not a valid point on Grassmann manifold G({d},{n}) at tolerance level {tol = }\"\n",
    "\n",
    "    @property\n",
    "    def V(self) -> MatGP:\n",
    "        mat_gp = MatGP(d_in=self.d_in, d_out=self.d_out, mu=self.mu, k=self.k, Omega_diag_chol=self.Omega_diag_chol, cov_jitter=self.cov_jitter)\n",
    "        return mat_gp\n",
    "\n",
    "    def tangent_model(self, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        d, n = self.d_out\n",
    "        N = s.shape[0]\n",
    "        Vs = self.V.model(s)\n",
    "        I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "        Deltas = numpyro.deterministic(\"Deltas\", np.einsum('ij,ljk->lik', I_UUT, Vs))\n",
    "        assert_shape(Deltas, (N, d, n),\n",
    "                    custom_message=f\"Deltas has shape {Deltas.shape}; expected shape {(N, d, n)}\")\n",
    "        return Deltas\n",
    "\n",
    "    def sample_tangents(self, seed: int, s: chex.ArrayDevice) -> chex.ArrayDevice:\n",
    "        tangent_model = self.tangent_model\n",
    "        seeded_model = handlers.seed(tangent_model, rng_seed=seed)\n",
    "        Deltas = seeded_model(s)\n",
    "        assert vmap(lambda Delta: valid_grass_tangent(self.U, Delta))(Deltas).all()\n",
    "        return Deltas\n",
    "\n",
    "    def sample_grass(self, seed: int, s: chex.ArrayDevice, reortho: bool = False) -> chex.ArrayDevice:\n",
    "        Deltas = self.sample_tangents(seed, s)\n",
    "        Ws = convert_to_projs(Deltas, self.U, reorthonormalize=reortho)\n",
    "        return Ws\n",
    "    \n",
    "    def predict_tangents(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        d, _ = self.d_out\n",
    "        I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "        V_mu = lambda s: I_UUT @ self.mu(s)\n",
    "        Omega = np.diag(self.Omega_diag_chol ** 2)\n",
    "        V_Omega = I_UUT @ Omega @ I_UUT.T\n",
    "        V = FullMatGP(d_in=self.d_in, d_out=self.d_out, mu=V_mu, k=self.k, Omega=V_Omega, cov_jitter=self.cov_jitter)\n",
    "        Deltas_mean, Deltas_pred = V.predict(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "        return Deltas_mean, Deltas_pred\n",
    "    \n",
    "    def predict_tangents_new(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, sigma: float, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        d, _ = self.d_out\n",
    "        I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "        V_mu = lambda s: I_UUT @ self.mu(s)\n",
    "        Omega = np.diag(self.Omega_diag_chol ** 2)\n",
    "        V_Omega = I_UUT @ Omega @ I_UUT.T\n",
    "        V = FullMatGP(d_in=self.d_in, d_out=self.d_out, mu=V_mu, k=self.k, Omega=V_Omega, cov_jitter=self.cov_jitter)\n",
    "        Deltas_mean, Deltas_pred = V.predict_new(key, s_test, s_train, Vs_train, sigma, jitter=jitter)\n",
    "        return Deltas_mean, Deltas_pred\n",
    "\n",
    "    def predict_grass(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8, reortho: bool = False) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        Deltas_mean, Deltas_pred = self.predict_tangents(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "        Ws_mean = convert_to_projs(Deltas_mean, self.U, reorthonormalize=reortho)\n",
    "        Ws_pred = convert_to_projs(Deltas_pred, self.U, reorthonormalize=reortho)\n",
    "        return Ws_mean, Ws_pred\n",
    "\n",
    "#     def predict_tangents(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "#         d, _ = self.d_out\n",
    "#         Vs_mean, Vs_pred = self.V.predict(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "#         I_UUT = np.eye(d) - self.U @ self.U.T\n",
    "#         Deltas_mean = np.einsum('ij,ljk->lik', I_UUT, Vs_mean)\n",
    "#         Deltas_pred = np.einsum('ij,ljk->lik', I_UUT, Vs_pred)\n",
    "#         return Deltas_mean, Deltas_pred\n",
    "\n",
    "#     def predict_grass(self, key: chex.ArrayDevice, s_test: chex.ArrayDevice, s_train: chex.ArrayDevice, Vs_train: chex.ArrayDevice, jitter: float = 1e-8, reortho: bool = False) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "#         Deltas_mean, Deltas_pred = self.predict_tangents(key, s_test, s_train, Vs_train, jitter=jitter)\n",
    "#         Ws_mean = convert_to_projs(Deltas_mean, self.U, reorthonormalize=reortho)\n",
    "#         Ws_pred = convert_to_projs(Deltas_pred, self.U, reorthonormalize=reortho)\n",
    "#         return Ws_mean, Ws_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1adfebe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_Ws_test = vmap(lambda W: grass_log(anchor_point, W))(Ws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "492679d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Model:\n",
    "    name: str\n",
    "    anchor_point: list\n",
    "    d_in: int\n",
    "    Omega_diag_chol: Union[list, None]\n",
    "    k_include_noise: bool\n",
    "    var: Union[float, None]\n",
    "    length: Union[float, None]\n",
    "    noise: Union[float, None]\n",
    "    require_noise: bool\n",
    "    jitter: float\n",
    "    cov_jitter: float\n",
    "    L_jitter: float\n",
    "    reorthonormalize: bool\n",
    "    b: float\n",
    "    ell: float\n",
    "    savvas_param: bool\n",
    "    s_train: list\n",
    "    s_test: list\n",
    "    Ws_train: list\n",
    "    Ws_test: list\n",
    "    \n",
    "    def model(self, s, log_Ws):\n",
    "        U = np.array(self.anchor_point)\n",
    "        d, n = U.shape\n",
    "        N = s.shape[0]\n",
    "        d_n = d * n\n",
    "        # N_params = N * d_n\n",
    "        if log_Ws is not None:\n",
    "            assert log_Ws.shape == (N, d, n)\n",
    "\n",
    "        # get/sample Omega\n",
    "        if self.Omega_diag_chol is None:\n",
    "            Omega_diag_chol = numpyro.sample('Omega_diag_chol', dist.LogNormal(0.0, 1.0).expand([d_n]))\n",
    "        else:\n",
    "            Omega_diag_chol = np.array(self.Omega_diag_chol)\n",
    "\n",
    "        # get/sample kernel params\n",
    "        if self.var is None:\n",
    "            # sample var\n",
    "            var = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, self.b))\n",
    "        else:\n",
    "            var = self.var\n",
    "\n",
    "        if self.length is None:\n",
    "            # sample length\n",
    "            if self.savvas_param:\n",
    "                length = numpyro.sample(\"kernel_length\", dist.LogNormal(-0.7, self.b))\n",
    "            else:\n",
    "                length = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, self.b))\n",
    "        else:\n",
    "            length = self.length\n",
    "\n",
    "        if self.require_noise:\n",
    "            if self.noise is None:\n",
    "                # sample noise\n",
    "                noise = numpyro.sample(\"kernel_noise\", dist.LogNormal(0.0, self.b))\n",
    "            else:\n",
    "                noise = self.noise\n",
    "        else:\n",
    "            noise = 0.0\n",
    "\n",
    "        if self.savvas_param:\n",
    "            kernel_params = {'var': var, 'length': np.sqrt(1 / length), 'noise': noise}\n",
    "        else:\n",
    "            kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        \n",
    "        # create kernel function\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=self.jitter, include_noise=self.k_include_noise)\n",
    "        # create mean function\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "\n",
    "        # initialize GrassGP\n",
    "        grass_gp = GrassGP(d_in=self.d_in, d_out=(d,n), mu=mu, k=k, Omega_diag_chol=Omega_diag_chol, U=U, cov_jitter=self.cov_jitter)\n",
    "\n",
    "        # sample Deltas\n",
    "        Deltas = grass_gp.tangent_model(s)\n",
    "\n",
    "        # # # # ! check what power this should be\n",
    "        # likelihood\n",
    "        ell = self.ell\n",
    "        with numpyro.plate(\"N\", N):\n",
    "            numpyro.sample(\"log_W\", dist.continuous.MatrixNormal(loc=Deltas, scale_tril_row=ell * np.eye(d),scale_tril_column=np.eye(n)), obs=log_Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0745f4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TangentSpaceModelConf = builds(Model, populate_full_signature=True)\n",
    "\n",
    "my_model_conf = TangentSpaceModelConf(\n",
    "    name = \"My Model\",\n",
    "    anchor_point = anchor_point.tolist(),\n",
    "    d_in = 2,\n",
    "    Omega_diag_chol = None,\n",
    "    k_include_noise= True,\n",
    "    var = 1.0,\n",
    "    length = None,\n",
    "    noise = None,\n",
    "    require_noise = False,\n",
    "    jitter = 1e-06,\n",
    "    cov_jitter = 1e-4,\n",
    "    L_jitter = 1e-8,\n",
    "    reorthonormalize = False,\n",
    "    b = 0.5,\n",
    "    ell = 0.01,\n",
    "    savvas_param = False,\n",
    "    s_train = s_train.tolist(),\n",
    "    s_test = s_test.tolist(),\n",
    "    Ws_train = Ws_train.tolist(),\n",
    "    Ws_test = Ws_test.tolist()\n",
    ")\n",
    "\n",
    "SVIConfig = make_config(\n",
    "    seed = 123514354575,\n",
    "    maxiter = 15000,\n",
    "    step_size = 0.001\n",
    ")\n",
    "\n",
    "TrainConfig = make_config(\n",
    "    seed = 9870687,\n",
    "    n_warmup = 5000,\n",
    "    n_samples = 7000,\n",
    "    n_chains = 1,\n",
    "    n_thinning = 2\n",
    ")\n",
    "\n",
    "PredictConfig = make_config(\n",
    "    seed = 6578,\n",
    "    splits = 25\n",
    ")\n",
    "\n",
    "PlotsConfig = make_config(\n",
    "    acf_lags = 100,\n",
    "    plot = False,\n",
    ")\n",
    "\n",
    "Config = make_config(\n",
    "    model = my_model_conf,\n",
    "    svi = SVIConfig,\n",
    "    train = TrainConfig,\n",
    "    predict = PredictConfig,\n",
    "    plots = PlotsConfig,\n",
    "    save_results = True,\n",
    "    save_stdout = True,\n",
    "    load_saved = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ac39294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numpyro.render_model(instantiate(Config.model).model, model_args=(s_train, log_Ws_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c760912b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pickle_save(obj, name: str):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ae9c816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pickle_load(name: str):\n",
    "    with open(name, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6980f515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    # instantiate grass model\n",
    "    model = instantiate(cfg.model).model\n",
    "    \n",
    "    save_results = cfg.save_results\n",
    "    plot_figs = cfg.plots.plot\n",
    "    save_stdout = cfg.save_stdout\n",
    "    \n",
    "    anchor_point = np.array(cfg.model.anchor_point)\n",
    "    s_train = np.array(cfg.model.s_train)\n",
    "    Ws_train = np.array(cfg.model.Ws_train)\n",
    "    s_test = np.array(cfg.model.s_test)\n",
    "    Ws_test = np.array(cfg.model.Ws_test)\n",
    "    \n",
    "    log_Ws_train = vmap(lambda W: grass_log(anchor_point, W))(Ws_train)\n",
    "    log_Ws_test = vmap(lambda W: grass_log(anchor_point, W))(Ws_test)\n",
    "        \n",
    "    if save_results:\n",
    "        training_test_data = {'s_train': s_train, 's_test': s_test, 'Ws_train': Ws_train, 'Ws_test': Ws_test, 'log_Ws_train': log_Ws_train, 'log_Ws_test': log_Ws_test, 'anchor_point': anchor_point}\n",
    "        pickle_save(training_test_data, 'training_test_data.pickle')\n",
    "    \n",
    "    # run SVI to get MAP esimtate to initialise MCMC\n",
    "    svi_key = random.PRNGKey(cfg.svi.seed)\n",
    "    maxiter = cfg.svi.maxiter\n",
    "    step_size = cfg.svi.step_size\n",
    "    print(\"Running SVI for MAP estimate to initialise MCMC\")\n",
    "    svi_results = run_svi_for_map(svi_key, model, maxiter, step_size, s_train, log_Ws_train)\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(svi_results, 'svi_results.pickle')\n",
    "    \n",
    "    if plot_figs:\n",
    "        # plot svi losses\n",
    "        plt.plot(svi_results.losses)\n",
    "        plt.show()\n",
    "        \n",
    "    # get initialisation from SVI results\n",
    "    map_est = svi_results.params\n",
    "    strip_val = len('_auto_loc')\n",
    "    init_values = {key[:-strip_val]:value for (key, value) in map_est.items()}\n",
    "    \n",
    "    # run HMC\n",
    "    train_key = random.PRNGKey(cfg.train.seed)\n",
    "    mcmc_config = {'num_warmup' : cfg.train.n_warmup, 'num_samples' : cfg.train.n_samples, 'num_chains' : cfg.train.n_chains, 'thinning' : cfg.train.n_thinning, 'init_strategy' : init_to_value(values=init_values)}\n",
    "    print(\"HMC starting.\")\n",
    "    mcmc = run_inference(train_key, mcmc_config, model, s_train, log_Ws_train)\n",
    "    \n",
    "    if save_stdout:\n",
    "        original_stdout = sys.stdout\n",
    "        with open('hmc_log.txt', 'w') as f:\n",
    "            sys.stdout = f\n",
    "            mcmc.print_summary()\n",
    "            sys.stdout = original_stdout\n",
    "    \n",
    "    samples = mcmc.get_samples()\n",
    "    inference_data = samples.copy()\n",
    "    for param, initial_val in init_values.items():\n",
    "        inference_data[f\"{param}-initial_value\"] = initial_val\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(inference_data, \"inference_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f58b3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9ae0b42-a0d7-4707-8a53-910086d15862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_tangents(\n",
    "    key: chex.ArrayDevice,\n",
    "    s_test: chex.ArrayDevice,\n",
    "    s_train: chex.ArrayDevice,\n",
    "    Vs_train: chex.ArrayDevice,\n",
    "    cfg,\n",
    "    samples: dict,\n",
    "    n_samples: Union[int,None] = None,\n",
    "    jitter: float = 1e-8\n",
    ") -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "    \n",
    "    d_in = cfg.model.d_in\n",
    "    U = np.array(cfg.model.anchor_point)\n",
    "    d, n = U.shape\n",
    "    cov_jitter = cfg.model.cov_jitter\n",
    "    k_include_noise = cfg.model.k_include_noise\n",
    "    kern_jitter = cfg.model.jitter\n",
    "    if n_samples is None:\n",
    "        n_samples = cfg.train.n_samples // cfg.train.n_thinning\n",
    "        assert n_samples == samples['Deltas'].shape[0]\n",
    "    \n",
    "    def predict(\n",
    "        key: chex.ArrayDevice,\n",
    "        Omega_diag_chol: chex.ArrayDevice,\n",
    "        var: float,\n",
    "        length: float,\n",
    "        noise: float,\n",
    "    ) -> Tuple[chex.ArrayDevice, chex.ArrayDevice]:\n",
    "        # iniatilize GrassGP\n",
    "        kernel_params = {'var': var, 'length': length, 'noise': noise}\n",
    "        k = lambda t, s: rbf(t, s, kernel_params, jitter=kern_jitter, include_noise=k_include_noise)\n",
    "        mu = lambda s: zero_mean(s, d, n)\n",
    "        grass_gp = GrassGP(d_in=d_in, d_out=(d, n), mu=mu, k=k, Omega_diag_chol=Omega_diag_chol, U=U, cov_jitter=cov_jitter)\n",
    "\n",
    "        # predict\n",
    "        Deltas_mean, Deltas_pred = grass_gp.predict_tangents_new(key, s_test, s_train, Vs_train, cfg.model.ell, jitter=jitter)\n",
    "        return Deltas_mean, Deltas_pred\n",
    "\n",
    "    # initialize vmap args\n",
    "    vmap_args = (random.split(key, n_samples),)\n",
    "    \n",
    "    cfg_Omega_diag_chol = cfg.model.Omega_diag_chol\n",
    "    cfg_var = cfg.model.var\n",
    "    cfg_length = cfg.model.length\n",
    "    cfg_noise = cfg.model.noise\n",
    "    cfg_require_noise = cfg.model.require_noise\n",
    "    \n",
    "    if cfg_Omega_diag_chol is None:\n",
    "        vmap_args += (samples['Omega_diag_chol'],)\n",
    "    else:\n",
    "        cfg_Omega_diag_chol = np.array(cfg_Omega_diag_chol)\n",
    "        vmap_args += (np.repeat(cfg_Omega_diag_chol[None,:,:], n_samples, axis=0),)\n",
    "    \n",
    "    if cfg_var is None:\n",
    "        vmap_args += (samples['kernel_var'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_var * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_length is None:\n",
    "        vmap_args += (samples['kernel_length'],)\n",
    "    else:\n",
    "        vmap_args += (cfg_length * np.ones(n_samples),)\n",
    "        \n",
    "    if cfg_require_noise:\n",
    "        if cfg_noise is None:\n",
    "            vmap_args += (samples['kernel_noise'],)\n",
    "        else:\n",
    "            vmap_args += (cfg_noise * np.ones(n_samples),)\n",
    "    else:\n",
    "        vmap_args += (np.zeros(n_samples),)\n",
    "    \n",
    "    assert len(vmap_args) == 5\n",
    "    # Deltas_means, Deltas_preds = vmap(predict)(*vmap_args)\n",
    "    Deltas_means = []\n",
    "    Deltas_preds = []\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        rand_key = vmap_args[0][i]\n",
    "        Omega_diag_chol = vmap_args[1][i]\n",
    "        var = vmap_args[2][i]\n",
    "        length = vmap_args[3][i]\n",
    "        noise = vmap_args[4][i]\n",
    "        mean, pred = predict(rand_key, Omega_diag_chol, var, length, noise)\n",
    "        Deltas_means.append(mean)\n",
    "        Deltas_preds.append(pred)\n",
    "    \n",
    "    Deltas_means = np.array(Deltas_means)\n",
    "    Deltas_preds = np.array(Deltas_preds)\n",
    "    return Deltas_means, Deltas_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4547a984-75cf-48ad-aae1-17706e773923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyse(cfg):\n",
    "    # instantiate grass model\n",
    "    model = instantiate(cfg.model).model\n",
    "    \n",
    "    save_results = cfg.save_results\n",
    "    plot_figs = cfg.plots.plot\n",
    "    save_stdout = cfg.save_stdout\n",
    "    \n",
    "    anchor_point = np.array(cfg.model.anchor_point)\n",
    "    s_train = np.array(cfg.model.s_train)\n",
    "    Ws_train = np.array(cfg.model.Ws_train)\n",
    "    s_test = np.array(cfg.model.s_test)\n",
    "    Ws_test = np.array(cfg.model.Ws_test)\n",
    "    \n",
    "    log_Ws_train = vmap(lambda W: grass_log(anchor_point, W))(Ws_train)\n",
    "    log_Ws_test = vmap(lambda W: grass_log(anchor_point, W))(Ws_test)\n",
    "    \n",
    "    inference_data = pickle_load(\"inference_data.pickle\")\n",
    "    samples = dict(filter(lambda elem: 'initial_value' not in elem[0], inference_data.items()))\n",
    "    initial_values = dict(filter(lambda elem: 'initial_value' in elem[0], inference_data.items()))\n",
    "    assert set(samples.keys()).union(initial_values.keys()) == set(inference_data.keys())\n",
    "    \n",
    "    if plot_figs:\n",
    "        my_samples = flatten_samples(samples, ignore=[])\n",
    "        trace_plot_vars = ['kernel_length']\n",
    "        # for key in my_samples.keys():\n",
    "        #     if 'Omega' in key:\n",
    "        #         trace_plot_vars.append(key)\n",
    "        #     if 'sigmas' in key:\n",
    "        #         trace_plot_vars.append(key)\n",
    "\n",
    "        my_samples[trace_plot_vars].plot(subplots=True, figsize=(10,6), sharey=False)\n",
    "        plt.show()\n",
    "\n",
    "        for var in trace_plot_vars:\n",
    "            sm.graphics.tsa.plot_acf(my_samples[var], lags=Config.plots.acf_lags)\n",
    "            plt.title(f\"acf for {var}\")\n",
    "            plt.show()\n",
    "            \n",
    "        trace_plot_vars = []\n",
    "        for name in my_samples.columns:\n",
    "            if \"Omega\" in name:\n",
    "                trace_plot_vars.append(name)\n",
    "                \n",
    "        my_samples.plot(y=trace_plot_vars,legend=False,alpha=0.75)\n",
    "        plt.show()\n",
    "\n",
    "    # compute Ws's from mcmc samples\n",
    "    tol=1e-5\n",
    "    samples_Ws_train = vmap(lambda Deltas: convert_to_projs(Deltas, anchor_point, reorthonormalize=False))(samples['Deltas'])\n",
    "    for ws in samples_Ws_train:\n",
    "        assert vmap(lambda w: valid_grass_point(w, tol=tol))(ws).all()\n",
    "        \n",
    "    if save_results:\n",
    "        pickle_save(samples_Ws_train, \"samples_Ws_train.pickle\")\n",
    "        \n",
    "    mcmc_barycenters = []\n",
    "    for i in tqdm(range(s_train.shape[0])):\n",
    "        points = samples_Ws_train[:,i,:,:]\n",
    "        barycenter, _, _ = sample_karcher_mean(points)\n",
    "        mcmc_barycenters.append(barycenter)\n",
    "        \n",
    "    mcmc_barycenters = np.array(mcmc_barycenters)\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(mcmc_barycenters, \"mcmc_barycenters.pickle\")\n",
    "    \n",
    "    in_sample_errors = vmap(grass_dist)(Ws_train, mcmc_barycenters)\n",
    "    \n",
    "    if plot_figs:\n",
    "        fig, ax = plt.subplots()\n",
    "        tcf = ax.tricontourf(s_train[:,0],s_train[:,1],in_sample_errors)\n",
    "        fig.colorbar(tcf)\n",
    "        plt.show()\n",
    "    \n",
    "    sd_s_train = []\n",
    "    for i in tqdm(range(s_train.shape[0])):\n",
    "        fixed = mcmc_barycenters[i]\n",
    "        dists = vmap(lambda W: grass_dist(W, fixed))(samples_Ws_train[:,i,:,:])\n",
    "        dists_Sq = dists**2\n",
    "        sd_s_train.append(np.sqrt(dists_Sq.mean()))\n",
    "    sd_s_train = np.array(sd_s_train)\n",
    "    \n",
    "    pd_data = {'x': s_train[:,0], 'y': s_train[:,1], 'errors': in_sample_errors, 'sd': sd_s_train}\n",
    "    in_sample_errors_df = pd.DataFrame(data=pd_data)\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(in_sample_errors_df, \"in_sample_errors_df.pickle\")\n",
    "    \n",
    "    print(\"Prediction starting\")\n",
    "    pred_key = random.PRNGKey(cfg.predict.seed)\n",
    "    Deltas_means, Deltas_preds = predict_tangents(pred_key, s_test, s_train, log_Ws_train, cfg, samples)\n",
    "    assert np.isnan(Deltas_means).sum() == 0\n",
    "    assert np.isnan(Deltas_preds).sum() == 0\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(Deltas_means, \"Deltas_means.pickle\")\n",
    "        pickle_save(Deltas_preds, \"Deltas_preds.pickle\")\n",
    "    \n",
    "    Ws_means = vmap(lambda i: convert_to_projs(Deltas_means[i], anchor_point, reorthonormalize=cfg.model.reorthonormalize))(np.arange(Deltas_means.shape[0]))\n",
    "    Ws_preds = vmap(lambda i: convert_to_projs(Deltas_preds[i], anchor_point, reorthonormalize=cfg.model.reorthonormalize))(np.arange(Deltas_preds.shape[0]))\n",
    "    assert np.isnan(Ws_means).sum() == 0\n",
    "    assert np.isnan(Ws_preds).sum() == 0\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(Ws_means, \"Ws_means.pickle\")\n",
    "        pickle_save(Ws_preds, \"Ws_preds.pickle\")\n",
    "    \n",
    "    test_means_mcmc_barycenters = []\n",
    "    for i in tqdm(range(s_test.shape[0])):\n",
    "        points = Ws_means[:,i,:,:]\n",
    "        barycenter, _, _ = sample_karcher_mean(points)\n",
    "        test_means_mcmc_barycenters.append(barycenter)\n",
    "        \n",
    "    test_preds_mcmc_barycenters = []\n",
    "    for i in tqdm(range(s_test.shape[0])):\n",
    "        points = Ws_preds[:,i,:,:]\n",
    "        barycenter, _, _ = sample_karcher_mean(points)\n",
    "        test_preds_mcmc_barycenters.append(barycenter)\n",
    "        \n",
    "    test_means_mcmc_barycenters = np.array(test_means_mcmc_barycenters)\n",
    "    test_preds_mcmc_barycenters = np.array(test_preds_mcmc_barycenters)\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(test_means_mcmc_barycenters, \"test_means_mcmc_barycenters.pickle\")\n",
    "        pickle_save(test_preds_mcmc_barycenters, \"test_preds_mcmc_barycenters.pickle\")\n",
    "    \n",
    "    out_sample_mean_errors = vmap(grass_dist)(Ws_test, test_means_mcmc_barycenters)\n",
    "    out_sample_pred_errors = vmap(grass_dist)(Ws_test, test_preds_mcmc_barycenters)\n",
    "    \n",
    "    # if plot_figs:\n",
    "#         plt.plot(s_test,out_sample_mean_errors, label='error using means')\n",
    "#         plt.plot(s_test,out_sample_pred_errors, label='error using preds')\n",
    "#         plt.vlines(s_train, 0, 1, colors=\"green\", linestyles=\"dashed\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "    sd_s_test_means = []\n",
    "    for i in tqdm(range(s_test.shape[0])):\n",
    "        fixed = test_preds_mcmc_barycenters[i]\n",
    "        dists = vmap(lambda W: grass_dist(W, fixed))(Ws_means[:,i,:,:])\n",
    "        dists_Sq = dists**2\n",
    "        sd_s_test_means.append(np.sqrt(dists_Sq.mean()))\n",
    "        \n",
    "    sd_s_test_preds = []\n",
    "    for i in tqdm(range(s_test.shape[0])):\n",
    "        fixed = test_preds_mcmc_barycenters[i]\n",
    "        dists = vmap(lambda W: grass_dist(W, fixed))(Ws_preds[:,i,:,:])\n",
    "        dists_Sq = dists**2\n",
    "        sd_s_test_preds.append(np.sqrt(dists_Sq.mean()))\n",
    "    \n",
    "    sd_s_test_means = np.array(sd_s_test_means)\n",
    "    sd_s_test_preds = np.array(sd_s_test_preds)\n",
    "    \n",
    "    test_pd_data = {'x': s_test[:,0], 'y': s_test[:,1], 'errors_mean': out_sample_mean_errors, 'errors_pred': out_sample_pred_errors, 'sd_mean': sd_s_test_means, 'sd_pred': sd_s_test_preds}\n",
    "    out_sample_errors_df = pd.DataFrame(data=test_pd_data)\n",
    "    \n",
    "    if save_results:\n",
    "        pickle_save(out_sample_errors_df, \"out_sample_errors_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a37fe38-32d5-418d-83a8-c71182c5be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0be0bec1-5f84-40b4-ac07-247a89df09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_function(cfg):\n",
    "    train_start_time = time.time()\n",
    "    train(cfg)\n",
    "    train_end_time = time.time()\n",
    "    train_elapsed_time = train_end_time - train_start_time\n",
    "    print(f\"Training took {train_elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    analyse_start_time = time.time()\n",
    "    analyse(cfg)\n",
    "    analyse_end_time = time.time()\n",
    "    analyse_elapsed_time = analyse_end_time - analyse_start_time\n",
    "    print(f\"Analyse phase took {analyse_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56647273-42cf-46ed-9ba5-b82dd74ae38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(jobs,) = launch(\n",
    "    Config,\n",
    "    task_function,\n",
    "    overrides=[\n",
    "        \"model.ell=0.01,0.001,0.0001\",\n",
    "        \"model.cov_jitter=0.0001\"\n",
    "    ],\n",
    "    multirun=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c2f616c-0e84-4b8b-aa77-60cbc4ebd6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = Config\n",
    "\n",
    "# anchor_point = np.array(cfg.model.anchor_point)\n",
    "# s_train = np.array(cfg.model.s_train)\n",
    "# Ws_train = np.array(cfg.model.Ws_train)\n",
    "# s_test = np.array(cfg.model.s_test)\n",
    "# Ws_test = np.array(cfg.model.Ws_test)\n",
    "\n",
    "# log_Ws_train = vmap(lambda W: grass_log(anchor_point, W))(Ws_train)\n",
    "# log_Ws_test = vmap(lambda W: grass_log(anchor_point, W))(Ws_test)\n",
    "\n",
    "# inference_data = pickle_load(\"inference_data.pickle\")\n",
    "# samples = dict(filter(lambda elem: 'initial_value' not in elem[0], inference_data.items()))\n",
    "# initial_values = dict(filter(lambda elem: 'initial_value' in elem[0], inference_data.items()))\n",
    "# assert set(samples.keys()).union(initial_values.keys()) == set(inference_data.keys())\n",
    "\n",
    "\n",
    "# my_samples = flatten_samples(samples, ignore=[])\n",
    "# trace_plot_vars = ['kernel_length']\n",
    "# # for key in my_samples.keys():\n",
    "# #     if 'Omega' in key:\n",
    "# #         trace_plot_vars.append(key)\n",
    "# #     if 'sigmas' in key:\n",
    "# #         trace_plot_vars.append(key)\n",
    "\n",
    "# my_samples[trace_plot_vars].plot(subplots=True, figsize=(10,6), sharey=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59c41ffc-d9ab-4223-867a-0223be93693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in trace_plot_vars:\n",
    "#     sm.graphics.tsa.plot_acf(my_samples[var], lags=Config.plots.acf_lags)\n",
    "#     plt.title(f\"acf for {var}\")\n",
    "#     plt.show()\n",
    "\n",
    "# trace_plot_vars = []\n",
    "# for name in my_samples.columns:\n",
    "#     if \"Omega\" in name:\n",
    "#         trace_plot_vars.append(name)\n",
    "\n",
    "# my_samples.plot(y=trace_plot_vars,legend=False,alpha=0.75)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce14bf4d-0985-44b7-a033-fefc69a810e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trace_plot_vars = []\n",
    "# for name in my_samples.columns:\n",
    "#     if \"Omega\" in name:\n",
    "#         sm.graphics.tsa.plot_acf(my_samples[name], lags=Config.plots.acf_lags)\n",
    "#         plt.title(name)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94439d21-7466-467c-85f9-73e541e8bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_sample_errors_df = pickle_load(\"in_sample_errors_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a28096ca-5488-451f-af77-b93c806ac2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4485cb6b-3863-4810-9290-d9a9ef570535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# tcf = ax.tricontourf(in_sample_errors_df['x'],in_sample_errors_df['y'],in_sample_errors_df['errors'])\n",
    "# ax.scatter(in_sample_errors_df['x'],in_sample_errors_df['y'],c='r')\n",
    "# fig.colorbar(tcf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60bae2cd-dde1-487c-b765-078386b2ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# tcf = ax.tricontourf(in_sample_errors_df['x'],in_sample_errors_df['y'],in_sample_errors_df['sd'])\n",
    "# ax.scatter(in_sample_errors_df['x'],in_sample_errors_df['y'],c='r')\n",
    "# fig.colorbar(tcf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "037e76ec-ca76-443e-b72f-7a1adbcc3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_sample_errors_df = pickle_load(\"out_sample_errors_df.pickle\")\n",
    "# out_sample_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5bab2ec6-f769-4487-afa3-312579f9c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# tcf = ax.tricontourf(out_sample_errors_df['x'],out_sample_errors_df['y'],out_sample_errors_df['errors_mean'])\n",
    "# ax.scatter(in_sample_errors_df['x'],in_sample_errors_df['y'],c='r',marker='x')\n",
    "# ax.scatter(out_sample_errors_df['x'],out_sample_errors_df['y'],c='black',alpha=0.25)\n",
    "# fig.colorbar(tcf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27c5e933-b88c-4d7b-88fb-fc379b2d27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# tcf = ax.tricontourf(out_sample_errors_df['x'],out_sample_errors_df['y'],out_sample_errors_df['errors_pred'])\n",
    "# ax.scatter(in_sample_errors_df['x'],in_sample_errors_df['y'],c='r',marker='x')\n",
    "# ax.scatter(out_sample_errors_df['x'],out_sample_errors_df['y'],c='black',alpha=0.25)\n",
    "# fig.colorbar(tcf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dac6588a-c387-4343-94fd-a693e1f1ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# tcf = ax.tricontourf(out_sample_errors_df['x'],out_sample_errors_df['y'],out_sample_errors_df['sd_mean'])\n",
    "# ax.scatter(in_sample_errors_df['x'],in_sample_errors_df['y'],c='r',marker='x')\n",
    "# ax.scatter(out_sample_errors_df['x'],out_sample_errors_df['y'],c='black',alpha=0.25)\n",
    "# fig.colorbar(tcf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b484ebcf-7d49-4f2a-8ad2-806386bed6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# tcf = ax.tricontourf(out_sample_errors_df['x'],out_sample_errors_df['y'],out_sample_errors_df['sd_pred'])\n",
    "# ax.scatter(in_sample_errors_df['x'],in_sample_errors_df['y'],c='r',marker='x')\n",
    "# ax.scatter(out_sample_errors_df['x'],out_sample_errors_df['y'],c='black',alpha=0.25)\n",
    "# fig.colorbar(tcf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3592f525-a1ac-4591-8547-49cbe34abaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_sample_errors_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3826605-bc41-4ef8-a7e6-e92cb0b2e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_sample_errors_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eafcec-b623-4f4d-a566-f1ed40bd4731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
