@techreport{abdulleRandomTimeStep2020,
  title = {Random Time Step Probabilistic Methods for Uncertainty Quantification in Chaotic and Geometric Numerical Integration},
  author = {Abdulle, Assyr and Garegnani, Giacomo},
  year = {2020},
  abstract = {A novel probabilistic numerical method for quantifying the uncertainty induced by the time integration of ordinary differential equations (ODEs) is introduced. Departing from the classical strategy to randomize ODE solvers by adding a random forcing term, we show that a probability measure over the numerical solution of ODEs can be obtained by introducing suitable random time-steps in a classical time integrator. This intrinsic randomization allows for the conservation of geometric properties of the underlying deterministic integrator such as mass conservation, symplecticity or conservation of first integrals. Weak and mean-square convergence analysis are derived. We also analyse the convergence of the Monte Carlo esti-mator for the proposed random time step method and show that the measure obtained with repeated sampling converges in the mean-square sense independently of the number of samples. Numerical examples including chaotic Hamiltonian systems, chemical reactions and Bayesian inferential problems illustrate the accuracy, robustness and versatility of our probabilistic numerical method.},
  keywords = {65F15,65L09 Keywords Probabilistic methods for ODEs,AMS subject classifications 65C30,chaotic systems,geometric integration,inverse problems,random time steps,uncertainty quantification},
  file = {/home/yanni/Zotero/storage/R42QEEFK/Abdulle, Garegnani - 2020 - Random time step probabilistic methods for uncertainty quantification in chaotic and geometric numerical int.pdf}
}

@article{absilRiemannianGeometryGrassmann2004,
  title = {Riemannian {{Geometry}} of {{Grassmann Manifolds}} with a {{View}} on {{Algorithmic Computation}}},
  author = {Absil, P.-A. and Mahony, R. and Sepulchre, R.},
  year = {2004},
  month = jan,
  journal = {Acta Applicandae Mathematicae},
  volume = {80},
  number = {2},
  pages = {199--220},
  issn = {0167-8019},
  doi = {10.1023/B:ACAP.0000013855.14971.91},
  abstract = {We give simple formulas for the canonical metric, gradient, Lie derivative, Riemannian connection, parallel translation, geodesics and distance on the Grassmann manifold of p-planes in Rn. In these formulas, p-planes are represented as the column space of n \textsterling{} p matrices. The Newton method on abstract Riemannian manifolds proposed by S. T. Smith is made explicit on the Grassmann manifold. Two applications \textendash computing an invariant subspace of a matrix and the mean of subspaces\textendash{} are worked out.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/E4ZYKQWB/Absil et al. - 2004 - Riemannian Geometry of Grassmann Manifolds with a .pdf}
}

@article{absilRiemannianGeometryGrassmann2004a,
  title = {Riemannian {{Geometry}} of {{Grassmann Manifolds}} with a {{View}} on {{Algorithmic Computation}}},
  author = {Absil, P.-A. and Mahony, R. and Sepulchre, R.},
  year = {2004},
  month = jan,
  journal = {Acta Applicandae Mathematicae},
  volume = {80},
  number = {2},
  pages = {199--220},
  issn = {0167-8019},
  doi = {10.1023/B:ACAP.0000013855.14971.91},
  abstract = {We give simple formulas for the canonical metric, gradient, Lie derivative, Riemannian connection, parallel translation, geodesics and distance on the Grassmann manifold of p-planes in Rn. In these formulas, p-planes are represented as the column space of n \textsterling{} p matrices. The Newton method on abstract Riemannian manifolds proposed by S. T. Smith is made explicit on the Grassmann manifold. Two applications \textendash computing an invariant subspace of a matrix and the mean of subspaces\textendash{} are worked out.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/D9JUPU5W/Absil et al. - 2004 - Riemannian Geometry of Grassmann Manifolds with a .pdf}
}

@article{absilRiemannianGeometryGrassmann2004b,
  title = {Riemannian {{Geometry}} of {{Grassmann Manifolds}} with a {{View}} on {{Algorithmic Computation}}},
  author = {Absil, P.-A. and Mahony, R. and Sepulchre, R.},
  year = {2004},
  month = jan,
  journal = {Acta Applicandae Mathematicae},
  volume = {80},
  number = {2},
  pages = {199--220},
  issn = {0167-8019},
  doi = {10.1023/B:ACAP.0000013855.14971.91},
  abstract = {We give simple formulas for the canonical metric, gradient, Lie derivative, Riemannian connection, parallel translation, geodesics and distance on the Grassmann manifold of p-planes in Rn. In these formulas, p-planes are represented as the column space of n \texttimes{} p matrices. The Newton method on abstract Riemannian manifolds proposed by Smith is made explicit on the Grassmann manifold. Two applications \textendash{} computing an invariant subspace of a matrix and the mean of subspaces \textendash{} are worked out.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/HJ5P6FL2/Absil et al. - 2004 - Riemannian Geometry of Grassmann Manifolds with a .pdf}
}

@article{adamsBayesianOnlineChangepoint2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  year = {2007},
  month = oct,
  journal = {arXiv:0710.3742 [stat]},
  eprint = {0710.3742},
  primaryclass = {stat},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/5LU6EMAD/Adams and MacKay - 2007 - Bayesian Online Changepoint Detection.pdf}
}

@article{adamsBayesianOnlineChangepoint2007a,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  year = {2007},
  month = oct,
  journal = {arXiv:0710.3742 [stat]},
  eprint = {0710.3742},
  primaryclass = {stat},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/552RP7WZ/Adams and MacKay - 2007 - Bayesian Online Changepoint Detection.pdf}
}

@article{agudelo-espanaBayesianOnlinePrediction2020,
  title = {Bayesian {{Online Prediction}} of {{Change Points}}},
  author = {{Agudelo-Espa{\~n}a}, Diego and {Gomez-Gonzalez}, Sebastian and Bauer, Stefan and Sch{\"o}lkopf, Bernhard and Peters, Jan},
  year = {2020},
  month = jun,
  journal = {arXiv:1902.04524 [cs, stat]},
  eprint = {1902.04524},
  primaryclass = {cs, stat},
  abstract = {Online detection of instantaneous changes in the generative process of a data sequence generally focuses on retrospective inference of such change points without considering their future occurrences. We extend the Bayesian Online Change Point Detection algorithm to also infer the number of time steps until the next change point (i.e., the residual time). This enables to handle observation models which depend on the total segment duration, which is useful to model data sequences with temporal scaling. The resulting inference algorithm for segment detection can be deployed in an online fashion, and we illustrate applications to synthetic and to two medical real-world data sets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/ZMFQCUC8/Agudelo-Espa√±a et al. - 2020 - Bayesian Online Prediction of Change Points.pdf}
}

@article{akyildizStatisticalFiniteElements2021,
  title = {Statistical {{Finite Elements}} via {{Langevin Dynamics}}},
  author = {Akyildiz, {\"O}mer Deniz and Duffin, Connor and Sabanis, Sotirios and Girolami, Mark},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.11131 [cs, math, stat]},
  eprint = {2110.11131},
  primaryclass = {cs, math, stat},
  abstract = {The recent statistical finite element method (statFEM) provides a coherent statistical framework to synthesise finite element models with observed data. Through embedding uncertainty inside of the governing equations, finite element solutions are updated to give a posterior distribution which quantifies all sources of uncertainty associated with the model. However to incorporate all sources of uncertainty, one must integrate over the uncertainty associated with the model parameters, the known forward problem of uncertainty quantification. In this paper, we make use of Langevin dynamics to solve the statFEM forward problem, studying the utility of the unadjusted Langevin algorithm (ULA), a Metropolis-free Markov chain Monte Carlo sampler, to build a sample-based characterisation of this otherwise intractable measure. Due to the structure of the statFEM problem, these methods are able to solve the forward problem without explicit full PDE solves, requiring only sparse matrix-vector products. ULA is also gradient-based, and hence provides a scalable approach up to high degrees-of-freedom. Leveraging the theory behind Langevin-based samplers, we provide theoretical guarantees on sampler performance, demonstrating convergence, for both the prior and posterior, in the Kullback-Leibler divergence, and, in Wasserstein-2, with further results on the effect of preconditioning. Numerical experiments are also provided, for both the prior and posterior, to demonstrate the efficacy of the sampler, with a Python package also included.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/AP3XJ7J4/Akyildiz et al. - 2021 - Statistical Finite Elements via Langevin Dynamics.pdf;/home/yanni/Zotero/storage/UKWNQW9K/2110.html}
}

@article{aldosaryStructuralReliabilityStochastic2018,
  title = {Structural Reliability and Stochastic Finite Element Methods: {{State-of-the-art}} Review and Evidence-Based Comparison},
  shorttitle = {Structural Reliability and Stochastic Finite Element Methods},
  author = {Aldosary, Muhannad and Wang, Jinsheng and Li, Chenfeng},
  year = {2018},
  month = oct,
  journal = {EC},
  volume = {35},
  number = {6},
  pages = {2165--2214},
  issn = {0264-4401},
  doi = {10.1108/EC-04-2018-0157},
  abstract = {Purpose \textendash{} This paper aims to provide a comprehensive review of uncertainty quantification methods supported by evidence-based comparison studies. Uncertainties are widely encountered in engineering practice, arising from such diverse sources as heterogeneity of materials, variability in measurement, lack of data and ambiguity in knowledge. Academia and industries have long been researching for uncertainty quantification (UQ) methods to quantitatively account for the effects of various input uncertainties on the system response. Despite the rich literature of relevant research, UQ is not an easy subject for novice researchers/practitioners, where many different methods and techniques coexist with inconsistent input/ output requirements and analysis schemes.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/9VNQLZJT/Aldosary et al. - 2018 - Structural reliability and stochastic finite eleme.pdf}
}

@misc{alvarezKernelsVectorValuedFunctions2012,
  title = {Kernels for {{Vector-Valued Functions}}: A {{Review}}},
  shorttitle = {Kernels for {{Vector-Valued Functions}}},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  year = {2012},
  month = apr,
  number = {arXiv:1106.6251},
  eprint = {arXiv:1106.6251},
  publisher = {{arXiv}},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a regularization perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a probabilistic perspective they are the key in the context of Gaussian processes, where the kernel function is known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/SANSBUWF/Alvarez et al. - 2012 - Kernels for Vector-Valued Functions a Review.pdf}
}

@article{alvarezKernelsVectorValuedFunctions2012a,
  title = {Kernels for {{Vector-Valued Functions}}: A {{Review}}},
  shorttitle = {Kernels for {{Vector-Valued Functions}}},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  year = {2012},
  month = apr,
  journal = {arXiv:1106.6251 [cs, math, stat]},
  eprint = {1106.6251},
  primaryclass = {cs, math, stat},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/3DZMQRRN/Alvarez et al. - 2012 - Kernels for Vector-Valued Functions a Review.pdf;/home/yanni/Zotero/storage/VQECLT3E/1106.html}
}

@techreport{amariWHYNATURALGRADIENT,
  title = {{{WHY NATURAL GRADIENT}}?},
  author = {Amari, S and Douglas, S C},
  abstract = {Gradient adaptation is a useful technique for adjusting a set of parameters to minimize a cost function. While often easy to implement, the convergence speed of gradient adaptation can be slow when the slope of the cost function varies widely for small changes in the parameters. In this paper, we outline an alternative technique, termed natural gradient adaptation, that overcomes the poor convergence properties of gradient adaptation in many cases. The natural gradient is based on diierential geometry and employs knowledge of the Rieman-nian structure of the parameter space to adjust the gradient search direction. Unlike Newton's method, natural gradient adaptation does not assume a locally -quadratic cost function. Moreover, for maximum likelihood estimation tasks, natural gradient adaptation is asymptotically Fisher-eecient. A simple example illustrates the desirable properties of natural gradient adaptation.},
  file = {/home/yanni/Zotero/storage/MR5UFUEC/Amari, Douglas - Unknown - WHY NATURAL GRADIENT.pdf}
}

@inproceedings{amariWhyNaturalGradient1998,
  title = {Why Natural Gradient?},
  booktitle = {Proceedings of the 1998 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}, {{ICASSP}}'98 ({{Cat}}. {{No}}. {{98CH36181}})},
  author = {Amari, Shun-Ichi and Douglas, Scott C},
  year = {1998},
  volume = {2},
  pages = {1213--1216},
  publisher = {{IEEE}}
}

@article{aminikhanghahiSurveyMethodsTime2017,
  title = {A Survey of Methods for Time Series Change Point Detection},
  author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
  year = {2017},
  month = may,
  journal = {Knowl Inf Syst},
  volume = {51},
  number = {2},
  pages = {339--367},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-016-0987-z},
  abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/9L5B58LW/Aminikhanghahi and Cook - 2017 - A survey of methods for time series change point d.pdf}
}

@techreport{andersonSHORTEDOPERATORSII1975,
  title = {{{SHORTED OPERATORS}}. {{II}}*},
  author = {Anderson, W N and Trappt, G E},
  year = {1975},
  journal = {SIAM J. APPL. MATH},
  volume = {28},
  number = {1},
  abstract = {For a positive operator A acting on a Hilbert space, the shorted operator .9(A) is defined to be the supremum of all positive operators which are smaller than A and which have range lying in a fixed subspace S. This maximization problem arises naturally in electrical network theory. In this paper we prove that the shorted operator exists, and develop various properties, including a relation to parallel addition [Anderson and Duffin, J. Math. Anal. Appl., 11 (1969), pp. 576-594]. The basic properties of the shorted operator were developed for finite-dimensional spaces by Anderson [this Journal, 20 (1971), pp. 520-525]; some of these theorems remain true in all Hilbert spaces, but the proofs are different.},
  file = {/home/yanni/Zotero/storage/PN2AUI76/Anderson, Trappt - 1975 - SHORTED OPERATORS. II.pdf}
}

@misc{andreellaProcrustesbasedDistancesExploring2023,
  title = {Procrustes-Based Distances for Exploring between-Matrices Similarity},
  author = {Andreella, Angela and De Santis, Riccardo and Vesely, Anna and Finos, Livio},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06164},
  eprint = {arXiv:2301.06164},
  publisher = {{arXiv}},
  abstract = {The statistical shape analysis called Procrustes analysis minimizes the distance between matrices by similarity transformations. The method returns a set of optimal orthogonal matrices, which project each matrix into a common space. This manuscript presents two types of distances derived from Procrustes analysis for exploring between-matrices similarity. The first one focuses on the residuals from the Procrustes analysis, i.e., the residual-based distance metric. In contrast, the second one exploits the fitted orthogonal matrices, i.e., the rotational-based distance metric. Thanks to these distances, similarity-based techniques such as the multidimensional scaling method can be applied to visualize and explore patterns and similarities among observations. The proposed distances result in being helpful in functional magnetic resonance imaging (fMRI) data analysis. The brain activation measured over space and time can be represented by a matrix. The proposed distances applied to a sample of subjects -- i.e., matrices -- revealed groups of individuals sharing patterns of neural brain activation.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {/home/yanni/Zotero/storage/XG2IH8TP/Andreella et al. - 2023 - Procrustes-based distances for exploring between-m.pdf;/home/yanni/Zotero/storage/YSYJH57U/2301.html}
}

@article{arlotKernelMultipleChangepoint2019,
  title = {A {{Kernel Multiple Change-point Algorithm}} via {{Model Selection}}},
  author = {Arlot, Sylvain and Celisse, Alain and Harchaoui, Zaid},
  year = {2019},
  month = mar,
  journal = {arXiv:1202.3878 [math, stat]},
  eprint = {1202.3878},
  primaryclass = {math, stat},
  abstract = {We tackle the change-point problem with data belonging to a general set. We build a penalty for choosing the number of change-points in the kernel-based method of Harchaoui and Capp\textasciiacute e (2007). This penalty generalizes the one proposed by Lebarbier (2005) for a one-dimensional signal changing only through its mean. We prove a nonasymptotic oracle inequality for the proposed method, thanks to a new concentration result for some function of Hilbert-space valued random variables. Experiments on synthetic and real data illustrate the accuracy of our method, showing that it can detect changes in the whole distribution of data, even when the mean and variance are constant.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/yanni/Zotero/storage/NERENXEB/Arlot et al. - 2019 - A Kernel Multiple Change-point Algorithm via Model.pdf}
}

@article{babuskaStochasticCollocationMethod2010,
  title = {A {{Stochastic Collocation Method}} for {{Elliptic Partial Differential Equations}} with {{Random Input Data}}},
  author = {Babu{\v s}ka, Ivo and Nobile, Fabio and Tempone, Ra{\'u}l},
  year = {2010},
  month = jan,
  journal = {SIAM Rev.},
  volume = {52},
  number = {2},
  pages = {317--355},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/100786356},
  abstract = {This work proposes and analyzes a stochastic collocation method for solving elliptic partial differential equations with random coefficients and forcing terms. These input data are assumed to depend on a finite number of random variables. The method consists of a Galerkin approximation in space and a collocation in the zeros of suitable tensor product orthogonal polynomials (Gauss points) in the probability space, and naturally leads to the solution of uncoupled deterministic problems as in the Monte Carlo approach. It treats easily a wide range of situations, such as input data that depend nonlinearly on the random variables, diffusivity coefficients with unbounded second moments, and random variables that are correlated or even unbounded. We provide a rigorous convergence analysis and demonstrate exponential convergence of the ``probability error'' with respect to the number of Gauss points in each direction of the probability space, under some regularity assumptions on the random input data. Numerical examples show the effectiveness of the method. Finally, we include a section with developments posterior to the original publication of this work. There we review sparse grid stochastic collocation methods, which are effective collocation strategies for problems that depend on a moderately large number of random variables.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GTTD8IGZ/Babu≈°ka et al. - 2010 - A Stochastic Collocation Method for Elliptic Parti.pdf}
}

@techreport{barataMoorePenrosePseudoinverseTutorial2011,
  title = {The {{Moore-Penrose Pseudoinverse}}. {{A Tutorial Review}} of the {{Theory}}},
  author = {Barata, J C A and Hussein, M S},
  year = {2011},
  abstract = {In the last decades the Moore-Penrose pseudoinverse has found a wide range of applications in many areas of Science and became a useful tool for physicists dealing, for instance, with optimization problems, with data analysis, with the solution of linear integral equations, etc. The existence of such applications alone should attract the interest of students and researchers in the Moore-Penrose pseudoinverse and in related subjects, like the singular values decomposition theorem for matrices. In this note we present a tutorial review of the theory of the Moore-Penrose pseudoinverse. We present the first definitions and some motivations and, after obtaining some basic results, we center our discussion on the Spectral Theorem and present an algorithmically simple expression for the computation of the Moore-Penrose pseudoinverse of a given matrix. We do not claim originality of the results. We rather intend to present a complete and self-contained tutorial review, useful for those more devoted to applications, for those more theoretically oriented and for those who already have some working knowledge of the subject.},
  keywords = {()},
  file = {/home/yanni/Zotero/storage/ZKFKQPFX/Barata, Hussein - 2011 - The Moore-Penrose Pseudoinverse. A Tutorial Review of the Theory.pdf}
}

@techreport{barberSolvingOrdinaryDifferential2014,
  title = {On Solving {{Ordinary Differential Equations}} Using {{Gaussian Processes}}},
  author = {Barber, David},
  year = {2014},
  abstract = {We describe a set of Gaussian Process based approaches that can be used to solve non-linear Ordinary Differential Equations. We suggest an explicit probabilistic solver and two implicit methods, one analogous to Picard iteration and the other to gradient matching. All methods have greater accuracy than previously suggested Gaussian Process approaches. We also suggest a general approach that can yield error estimates from any standard ODE solver.},
  file = {/home/yanni/Zotero/storage/KPBM7YCH/Barber - 2014 - On solving Ordinary Differential Equations using Gaussian Processes.pdf}
}

@article{barrioSENSITIVITYANALYSISODEs2006,
  title = {{{SENSITIVITY ANALYSIS OF ODEs}}/{{DAEs USING THE TAYLOR SERIES METHOD}} *},
  author = {Barrio, Roberto},
  year = {2006},
  journal = {SIAM J. SCI. COMPUT},
  volume = {27},
  number = {6},
  pages = {1929--1947},
  doi = {10.1137/030601892},
  abstract = {This paper studies the applicability of the Taylor method for the sensibility analysis of ODEs and DAEs. Extended automatic differentiation rules are introduced for the calculus of partial derivatives of Taylor series. The numerical method is implemented using an efficient variable-step variable-order scheme. Finally, some numerical tests are presented showing the benefits of the formulation.},
  keywords = {41A58,65L80,automatic differentiation,partial derivatives AMS subject classifications 65,sensitivity analysis,Taylor method},
  file = {/home/yanni/Zotero/storage/N86ZH5BD/Barrio - 2006 - SENSITIVITY ANALYSIS OF ODEsDAEs USING THE TAYLOR SERIES METHOD.pdf}
}

@article{barrioSensitivityAnalysisODEs2006,
  title = {Sensitivity Analysis of {{ODEs}}/{{DAEs}} Using the {{Taylor}} Series Method},
  author = {Barrio, Roberto},
  year = {2006},
  journal = {SIAM Journal on Scientific Computing},
  volume = {27},
  number = {6},
  pages = {1929--1947},
  publisher = {{SIAM}}
}

@inproceedings{begelforAffineInvarianceRevisited2006,
  title = {Affine {{Invariance Revisited}}},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Volume}} 2 ({{CVPR}}'06)},
  author = {Begelfor, E. and Werman, M.},
  year = {2006},
  volume = {2},
  pages = {2087--2094},
  publisher = {{IEEE}},
  address = {{New York, NY, USA}},
  doi = {10.1109/CVPR.2006.50},
  abstract = {This paper proposes a Riemannian geometric framework to compute averages and distributions of point configurations so that different configurations up to affine transformations are considered to be the same. The algorithms are fast and proven to be robust both theoretically and empirically. The utility of this framework is shown in a number of affine invariant clustering algorithms on image point data.},
  isbn = {978-0-7695-2597-6},
  langid = {english},
  file = {/home/yanni/Zotero/storage/4Y9V4HZE/Begelfor and Werman - 2006 - Affine Invariance Revisited.pdf}
}

@article{bellotKernelHypothesisTesting2021,
  title = {Kernel {{Hypothesis Testing}} with {{Set-valued Data}}},
  author = {Bellot, Alexis and {van der Schaar}, Mihaela},
  year = {2021},
  month = feb,
  journal = {arXiv:1907.04081 [stat]},
  eprint = {1907.04081},
  primaryclass = {stat},
  abstract = {We present a general framework for hypothesis testing on distributions of sets of individual examples. Sets may represent many common data sources such as groups of observations in time series, collections of words in text or a batch of images of a given phenomenon. This observation pattern, however, differs from the common assumptions required for hypothesis testing: each set differs in size, may have differing levels of noise, and also may incorporate nuisance variability, irrelevant for the analysis of the phenomenon of interest; all features that bias test decisions if not accounted for. In this paper, we propose to interpret sets as independent samples from a collection of latent probability distributions, and introduce kernel two-sample and independence tests in this latent space of distributions. We prove the consistency of these tests and observe them to outperform in a wide range of synthetic experiments. Finally, we showcase their use in practice with experiments on healthcare and climate data, where previously heuristics were needed for feature extraction and testing.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/J3BH6TD7/Bellot and van der Schaar - 2021 - Kernel Hypothesis Testing with Set-valued Data.pdf}
}

@article{bendokatGrassmannManifoldHandbook2020,
  title = {A {{Grassmann Manifold Handbook}}: {{Basic Geometry}} and {{Computational Aspects}}},
  shorttitle = {A {{Grassmann Manifold Handbook}}},
  author = {Bendokat, Thomas and Zimmermann, Ralf and Absil, P.-A.},
  year = {2020},
  month = dec,
  journal = {arXiv:2011.13699 [cs, math]},
  eprint = {2011.13699},
  primaryclass = {cs, math},
  abstract = {The Grassmann manifold of linear subspaces is important for the mathematical modelling of a multitude of applications, ranging from problems in machine learning, computer vision and image processing to low-rank matrix optimization problems, dynamic low-rank decompositions and model reduction. With this work, we aim to provide a collection of the essential facts and formulae on the geometry of the Grassmann manifold in a fashion that is fit for tackling the aforementioned problems with matrix-based algorithms. Moreover, we expose the Grassmann geometry both from the approach of representing subspaces with orthogonal projectors and when viewed as a quotient space of the orthogonal group, where subspaces are identified as equivalence classes of (orthogonal) bases. This bridges the associated research tracks and allows for an easy transition between these two approaches. Original contributions include a modified algorithm for computing the Riemannian logarithm map on the Grassmannian that is advantageous numerically but also allows for a more elementary, yet more complete description of the cut locus and the conjugate points. We also derive a formula for parallel transport along geodesics in the orthogonal projector perspective, formulae for the derivative of the exponential map, as well as a formula for Jacobi fields vanishing at one point.},
  archiveprefix = {arxiv},
  keywords = {15-02; 15A16; 15A18; 15B10; 22E70; 51F25; 53C80; 53Z99,Mathematics - Differential Geometry,Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/972D86XU/Bendokat et al. - 2020 - A Grassmann Manifold Handbook Basic Geometry and .pdf;/home/yanni/Zotero/storage/WFDDEBD4/2011.html}
}

@article{bensonParameterFittingDynamic1979,
  title = {Parameter Fitting in Dynamic Models},
  author = {Benson, M},
  year = {1979},
  journal = {Ecological Modelling},
  volume = {6},
  pages = {97--115}
}

@article{bensonParameterFittingDynamic1979a,
  title = {Parameter Fitting in Dynamic Models},
  author = {Benson, M.},
  year = {1979},
  journal = {Ecological Modelling},
  issn = {03043800},
  doi = {10.1016/0304-3800(79)90029-2},
  abstract = {We consider the numerical approaches for the least squares estimation of the parameter vector p in the initial value problem y{${'}$} = g(t, y, p), y(t0) = y0(p) when observations are available on some or all components of the vector y(t). Special attention is paid to the development of techniques which, although not global, are less sensitive to initial parameter estimates than the standard approach employing the sensitivity equations. Experience indicates that interactive approaches can be very valuable when good starting parameter approximations are unavailable. We describe the main features of our interactive parameter fitting package PARFIT. This package contains standard techniques employing the sensitivity equations as well as special algorithms designed to improve poor parameter estimates. These special algorithms have been selected and developed with user interaction in mind. We describe in detail one special approach designed for the case when observations are not available on all state variables. An example (using computer generated observations) is presented to illustrate this approach. Finally, the power of an interactive approach is demonstrated with two examples involving attempts to model physically observed phenomena. \textcopyright{} 1979.},
  file = {/home/yanni/Zotero/storage/99BKBR9U/Benson - 1979 - Parameter fitting in dynamic models.pdf}
}

@article{bergerStatisticalFormalismUncertainty2019,
  title = {On the {{Statistical Formalism}} of {{Uncertainty Quantification}}},
  author = {Berger, James O. and Smith, Leonard A.},
  year = {2019},
  month = mar,
  journal = {Annu. Rev. Stat. Appl.},
  volume = {6},
  number = {1},
  pages = {433--460},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-030718-105232},
  abstract = {The use of models to try to better understand reality is ubiquitous. Models have proven useful in testing our current understanding of reality; for instance, climate models of the 1980s were built for science discovery, to achieve a better understanding of the general dynamics of climate systems. Scientific insights often take the form of general qualitative predictions (i.e.,``under these conditions, the Earth's poles will warm more than the rest of the planet''); such use of models differs from making quantitative forecasts of specific events (i.e. ``high winds at noon tomorrow at London's Heathrow Airport''). It is sometimes hoped that, after sufficient model development, any model can be used to make quantitative forecasts for any target system. Even if that were the case, there would always be some uncertainty in the prediction. Uncertainty quantification aims to provide a framework within which that uncertainty can be discussed and, ideally, quantified, in a manner relevant to practitioners using the forecast system. A statistical formalism has developed that claims to be able to accurately assess the uncertainty in prediction. This article is a discussion of if and when this formalism can do so. The article arose from an ongoing discussion between the authors concerning this issue, the second author generally being considerably more skeptical concerning the utility of the formalism in providing quantitative decision-relevant information.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/ZYLCM7UC/Berger and Smith - 2019 - On the Statistical Formalism of Uncertainty Quanti.pdf}
}

@article{bergmannManoptJlOptimization2022,
  title = {Manopt.Jl: {{Optimization}} on {{Manifolds}} in {{Julia}}},
  shorttitle = {Manopt.Jl},
  author = {Bergmann, Ronny},
  year = {2022},
  month = feb,
  journal = {JOSS},
  volume = {7},
  number = {70},
  pages = {3866},
  issn = {2475-9066},
  doi = {10.21105/joss.03866},
  abstract = {Manopt.jl provides a set of optimization algorithms for optimization problems given on a Riemannian manifold M. Based on a generic optimization framework, together with the interface ManifoldsBase.jl for Riemannian manifolds, classical and recently developed methods are provided in an efficient implementation. Algorithms include the derivative-free Particle Swarm and Nelder\textendash Mead algorithms, as well as classical gradient, conjugate gradient and stochastic gradient descent. Furthermore, quasi-Newton methods like a Riemannian LBFGS (Huang et al., 2015) and nonsmooth optimization algorithms like a Cyclic Proximal Point Algorithm (Ba\v{c}\'ak, 2014), a (parallel) Douglas-Rachford algorithm (Bergmann, Persch, et al., 2016) and a Chambolle-Pock algorithm (Bergmann et al., 2021) are provided, together with several basic cost functions, gradients and proximal maps as well as debug and record capabilities.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/WCRL6ZHT/Bergmann - 2022 - Manopt.jl Optimization on Manifolds in Julia.pdf}
}

@article{bergmannManoptJlOptimization2022a,
  title = {Manopt.Jl: {{Optimization}} on {{Manifolds}} in {{Julia}}},
  shorttitle = {Manopt.Jl},
  author = {Bergmann, Ronny},
  year = {2022},
  month = feb,
  journal = {JOSS},
  volume = {7},
  number = {70},
  pages = {3866},
  issn = {2475-9066},
  doi = {10.21105/joss.03866},
  abstract = {Manopt.jl provides a set of optimization algorithms for optimization problems given on a Riemannian manifold M. Based on a generic optimization framework, together with the interface ManifoldsBase.jl for Riemannian manifolds, classical and recently developed methods are provided in an efficient implementation. Algorithms include the derivative-free Particle Swarm and Nelder\textendash Mead algorithms, as well as classical gradient, conjugate gradient and stochastic gradient descent. Furthermore, quasi-Newton methods like a Riemannian LBFGS (Huang et al., 2015) and nonsmooth optimization algorithms like a Cyclic Proximal Point Algorithm (Ba\v{c}\'ak, 2014), a (parallel) Douglas-Rachford algorithm (Bergmann, Persch, et al., 2016) and a Chambolle-Pock algorithm (Bergmann et al., 2021) are provided, together with several basic cost functions, gradients and proximal maps as well as debug and record capabilities.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/NFETIIZN/Bergmann - 2022 - Manopt.jl Optimization on Manifolds in Julia.pdf}
}

@article{berrenderoMahalanobisDistanceFunctional2018,
  title = {On {{Mahalanobis}} Distance in Functional Settings},
  author = {Berrendero, Jos{\'e} R. and {Bueno-Larraz}, Beatriz and Cuevas, Antonio},
  year = {2018},
  journal = {arXiv},
  pages = {1--27},
  issn = {23318422},
  abstract = {Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The obtained results are quite competitive when compared to other recent proposals of the literature.},
  keywords = {Functional data,Kernel methods in statistics,Mahalanobis distance,Reproducing kernel Hilbert spaces},
  file = {/home/yanni/Zotero/storage/IV5F337Y/Berrendero, Bueno-Larraz, Cuevas - 2018 - On Mahalanobis distance in functional settings.pdf}
}

@techreport{besagSpatialInteractionStatistical1974,
  title = {Spatial {{Interaction}} and the {{Statistical Analysis}} of {{Lattice Systems}}},
  author = {Besag, Julian},
  year = {1974},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {36},
  number = {2},
  pages = {192--236},
  file = {/home/yanni/Zotero/storage/7XQBWWII/Besag - 1974 - Spatial Interaction and the Statistical Analysis of Lattice Systems.pdf}
}

@article{besagSpatialInteractionStatistical1974a,
  title = {Spatial Interaction and the Statistical Analysis of Lattice Systems},
  author = {Besag, Julian},
  year = {1974},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {36},
  number = {2},
  pages = {192--225},
  publisher = {{Wiley Online Library}}
}

@techreport{beskosGeometricMCMCInfiniteDimensional2017,
  title = {Geometric {{MCMC}} for {{Infinite-Dimensional Inverse Problems}}},
  author = {Beskos, Alexandros and Girolami, Mark and Lan, Shiwei and Farrell, Patrick E and Stuart, Andrew M},
  year = {2017},
  abstract = {Bayesian inverse problems often involve sampling posterior distributions on infinite-dimensional function spaces. Traditional Markov chain Monte Carlo (MCMC) algorithms are characterized by deteriorating mixing times upon mesh-refinement, when the finite-dimensional approximations become more accurate. Such methods are typically forced to reduce step-sizes as the discretization gets finer, and thus are expensive as a function of dimension. Recently, a new class of MCMC methods with mesh-independent convergence times has emerged. However , few of them take into account the geometry of the posterior informed by the data. At the same time, recently developed geometric MCMC algorithms have been found to be powerful in exploring complicated distributions that deviate significantly from elliptic Gaussian laws, but are in general computationally intractable for models defined in infinite dimensions. In this work, we combine geometric methods on a finite-dimensional subspace with mesh-independent infinite-dimensional approaches. Our objective is to speed up MCMC mixing times, without significantly increasing the computational cost per step (for instance, in comparison with the vanilla preconditioned Crank-Nicolson (pCN) method). This is achieved by using ideas from geometric MCMC to probe the complex structure of an intrinsic finite-dimensional subspace where most data information concentrates, while retaining robust mixing times as the dimension grows by using pCN-like methods in the complementary subspace. The resulting algorithms are demonstrated in the context of three challenging inverse problems arising in subsurface flow, heat conduction and incompressible flow control. The algorithms exhibit up to two orders of magnitude improvement in sampling efficiency when compared with the pCN method.},
  keywords = {Bayesian Inverse Problems,Infinite Dimensions,Local Preconditioning,Markov Chain Monte Carlo,Uncertainty Quantification},
  file = {/home/yanni/Zotero/storage/5Q6S9C6G/Beskos et al. - 2017 - Geometric MCMC for Infinite-Dimensional Inverse Problems.pdf}
}

@article{bhattacharyaBayesianFractionalPosteriors2019,
  title = {Bayesian Fractional Posteriors},
  author = {Bhattacharya, Anirban and Pati, Debdeep and Yang and {Y.U.N.}},
  year = {2019},
  journal = {Annals of Statistics},
  volume = {47},
  number = {1},
  pages = {39--66},
  issn = {00905364},
  doi = {10.1214/18-AOS1712},
  abstract = {We consider the fractional posterior distribution that is obtained by updating a prior distribution via Bayes theorem with a fractional likelihood function, a usual likelihood function raised to a fractional power. First, we analyze the contraction property of the fractional posterior in a general misspecified framework. Our contraction results only require a prior mass condition on certain Kullback\textendash Leibler (KL) neighborhood of the true parameter (or the KL divergence minimizer in the misspecified case), and obviate constructions of test functions and sieves commonly used in the literature for analyzing the contraction property of a regular posterior. We show through a counterexample that some condition controlling the complexity of the parameter space is necessary for the regular posterior to contract, rendering additional flexibility on the choice of the prior for the fractional posterior. Second, we derive a novel Bayesian oracle inequality based on a PAC-Bayes inequality in misspecified models. Our derivation reveals several advantages of averaging based Bayesian procedures over optimization based frequentist procedures. As an application of the Bayesian oracle inequality, we derive a sharp oracle inequality in multivariate convex regression problems. We also illustrate the theory in Gaussian process regression and density estimation problems.},
  keywords = {Convex regression,Misspecified models,Oracle inequality,PAC-Bayes,Posterior contraction,R√©nyi divergence},
  file = {/home/yanni/Zotero/storage/6FD94332/1611.01125.pdf}
}

@article{biasiIMPLICITFUNCTIONTHEOREM,
  title = {{{THE IMPLICIT FUNCTION THEOREM FOR CONTINUOUS FUNCTIONS}}},
  author = {Biasi, Carlos and Gutierrez, Carlos},
  journal = {The Implicit Function Theorem},
  pages = {9},
  abstract = {In the present paper we obtain a new homological version of the implicit function theorem and some versions of the Darboux theorem. Such results are proved for continuous maps on topological manifolds. As a consequence, some versions of these classic theorems are proved when we consider differenciable (not necessarily C1) maps.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/SBA6CZ94/Biasi and Gutierrez - THE IMPLICIT FUNCTION THEOREM FOR CONTINUOUS FUNCT.pdf}
}

@article{biasiIMPLICITFUNCTIONTHEOREMa,
  title = {{{THE IMPLICIT FUNCTION THEOREM FOR CONTINUOUS FUNCTIONS}}},
  author = {Biasi, Carlos and Gutierrez, Carlos},
  journal = {The Implicit Function Theorem},
  pages = {9},
  abstract = {In the present paper we obtain a new homological version of the implicit function theorem and some versions of the Darboux theorem. Such results are proved for continuous maps on topological manifolds. As a consequence, some versions of these classic theorems are proved when we consider differenciable (not necessarily C1) maps.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/KNYW3LJL/Biasi and Gutierrez - THE IMPLICIT FUNCTION THEOREM FOR CONTINUOUS FUNCT.pdf}
}

@article{bieglerNonlinearParameterEstimation1986,
  title = {Nonlinear Parameter Estimation: {{A}} Case Study Comparison},
  shorttitle = {Nonlinear Parameter Estimation},
  author = {Biegler, L. T. and Damiano, J. J. and Blau, G. E.},
  year = {1986},
  journal = {AIChE Journal},
  volume = {32},
  number = {1},
  pages = {29--45},
  issn = {1547-5905},
  doi = {10.1002/aic.690320105},
  abstract = {The literature abounds with the application of optimization methods for estimating model parameters in equation systems. The utility of these methods is frequently demonstrated on pathological examples using simulated data generated from a known model with a random error component and a known statistical distribution. Unfortunately, parameter estimation problems encountered in practice do not have this advantage. The true model is frequently not known. In fact, one is faced with choosing among various candidate models, all of which may be wrong. Moreover, the error structure is generally unknown and must be estimated from the data. Finally, a great deal of mathematical expertise is required to transform the model and select meaningful starting guesses before parameter estimation can be successful. In order to demonstrate the difficulties of parameter estimation in the industrial environment and the limitations of existing methods, a parameter estimation problem formulated by the Dow Chemical Company is presented and solved. This test problem consists of a stiff differential/algebraic (DAE) model that describes complex kinetics and requires the estimation of nine parameters from batch reactor data. Here the model was inadequate to describe the data, the error structure was not specified and the starting guesses led to a nontrivial optimization problem. The Dow parameter estimation problem was distributed in 1981 to 165 researchers as a followup to the 1980 FOCAPD conference. Of those researchers, eleven agreed to apply their methodologies and expertise to this problem. However, only five acceptable solutions were finally submitted. Here we present and compare these results. Each solution was obtained using different strategies. In most cases the form of the model was also changed to accommodate the algorithms used and to ease the solution procedure. Therefore, while this case study does not present a direct numerical comparison of algorithms, it does offer guidelines and insight towards the solution of difficult parameter estimation problems.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/ITKPS5K8/Biegler et al. - 1986 - Nonlinear parameter estimation A case study compa.pdf;/home/yanni/Zotero/storage/XL6NPLVB/aic.html}
}

@article{blalockMultiplyingMatricesMultiplying2021,
  title = {Multiplying {{Matrices Without Multiplying}}},
  author = {Blalock, Davis and Guttag, John},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.10860 [cs, stat]},
  eprint = {2106.10860},
  primaryclass = {cs, stat},
  abstract = {Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning. Consequently, there has been significant work on efficiently approximating matrix multiplies. We introduce a learning-based algorithm for this task that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs 100\texttimes{} faster than exact matrix products and 10\texttimes{} faster than current approximate methods. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling\textemdash\textendash the core operations of our method\textemdash\textendash could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have recently been the focus of substantial research and hardware investment.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Hardware Architecture,Computer Science - Machine Learning,Computer Science - Performance,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/IURRILJ8/Blalock and Guttag - 2021 - Multiplying Matrices Without Multiplying.pdf}
}

@article{bleiVariationalInferenceReview2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  primaryclass = {cs, stat},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/HMMZ6NKH/Blei et al_2017_Variational Inference.pdf}
}

@book{bobrowskiFunctionalAnalysisProbability2005,
  title = {Functional {{Analysis}} for {{Probability}} and {{Stochastic Processes}}: {{An Introduction}}},
  shorttitle = {Functional {{Analysis}} for {{Probability}} and {{Stochastic Processes}}},
  author = {Bobrowski, Adam},
  year = {2005},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511614583},
  abstract = {This text is designed both for students of probability and stochastic processes, and for students of functional analysis. For the reader not familiar with functional analysis a detailed introduction to necessary notions and facts is provided. However, this is not a straight textbook in functional analysis; rather, it presents some chosen parts of functional analysis that can help understand ideas from probability and stochastic processes. The subjects range from basic Hilbert and Banach spaces, through weak topologies and Banach algebras, to the theory of semigroups of bounded linear operators. Numerous standard and non-standard examples and exercises make the book suitable as a course textbook or for self-study.},
  isbn = {978-0-521-83166-6},
  file = {/home/yanni/Zotero/storage/WSRFXMLH/4ACA91681A2CDE2E22F83F3257CD5106.html}
}

@book{bogachevGaussianMeasures1998,
  title = {Gaussian {{Measures}}},
  author = {Bogachev, Vladimir Igorevich},
  year = {1998},
  publisher = {{American Mathematical Soc.}},
  abstract = {This text provides a systematic exposition of the modern theory of Gaussian measures. It presents, with complete and detailed proofs, fundamental facts about finite and infinite dimensional Gaussian distributions. Covered topics include linear properties, convexity, linear and nonlinear transformations, and applications to Gaussian and diffusion processes. Suitable for use as a graduate text and/or a reference work, this volume contains many examples, exercises, and an extensive bibliography. It brings together many results that have not appeared previously in book form.},
  googlebooks = {STryBwAAQBAJ},
  isbn = {978-0-8218-1054-5},
  langid = {english},
  keywords = {Mathematics / Reference,Mathematics / Research}
}

@article{bolinNumericalSolutionFractional2020,
  title = {Numerical Solution of Fractional Elliptic Stochastic {{PDEs}} with Spatial White Noise},
  author = {Bolin, David and Kirchner, Kristin and Kov{\'a}cs, Mih{\'a}ly},
  year = {2020},
  month = apr,
  journal = {IMA Journal of Numerical Analysis},
  volume = {40},
  number = {2},
  eprint = {1705.06565},
  primaryclass = {math, stat},
  pages = {1051--1073},
  issn = {0272-4979, 1464-3642},
  doi = {10.1093/imanum/dry091},
  abstract = {The numerical approximation of solutions to stochastic partial differential equations with additive spatial white noise on bounded domains in Rd is considered. The differential operator is given by the fractional power L{$\beta$} , {$\beta$} {$\in$} (0, 1), of an integer order elliptic differential operator L and is therefore non-local. Its inverse L-{$\beta$} is represented by a Bochner integral from the Dunford\textendash Taylor functional calculus. By applying a quadrature formula to this integral representation, the inverse fractional power operator L-{$\beta$} is approximated by a weighted sum of non-fractional resolvents (I + t2j L)-1 at certain quadrature nodes tj {$>$} 0. The resolvents are then discretized in space by a standard finite element method.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {35S15; 65C30; 65C60; 65N12; 65N30,Mathematics - Numerical Analysis,Statistics - Computation},
  file = {/home/yanni/Zotero/storage/NFGM2SJH/Bolin et al. - 2020 - Numerical solution of fractional elliptic stochast.pdf}
}

@article{bolinWeakConvergenceGalerkin2018,
  title = {Weak Convergence of {{Galerkin}} Approximations for Fractional Elliptic Stochastic {{PDEs}} with Spatial White Noise},
  author = {Bolin, David and Kirchner, Kristin and Kov{\'a}cs, Mih{\'a}ly},
  year = {2018},
  month = dec,
  journal = {Bit Numer Math},
  volume = {58},
  number = {4},
  eprint = {1711.05188},
  primaryclass = {math, stat},
  pages = {881--906},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/s10543-018-0719-8},
  abstract = {The numerical approximation of the solution to a stochastic partial differential equation with additive spatial white noise on a bounded domain is considered. The differential operator is assumed to be a fractional power of an integer order elliptic differential operator. The solution is approximated by means of a finite element discretization in space and a quadrature approximation of an integral representation of the fractional inverse from the Dunford\textendash Taylor calculus.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {35S15; 65C30; 65C60; 65N12; 65N30,Mathematics - Numerical Analysis,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/F73LR79E/Bolin et al. - 2018 - Weak convergence of Galerkin approximations for fr.pdf}
}

@article{bouchikhiOnlineChangepointDetection2020,
  title = {Online Change-Point Detection with Kernels},
  author = {Bouchikhi, Ikram and Ferrari, Andr{\'e} and Richard, C{\'e}dric and Bourrier, Anthony},
  year = {2020},
  month = sep,
  journal = {arXiv:2002.02704 [eess]},
  eprint = {2002.02704},
  primaryclass = {eess},
  abstract = {Change-points in time series data are usually defined as the time instants at which changes in their properties occur. Detecting change-points is critical in a number of applications as diverse as detecting credit card and insurance frauds, or intrusions into networks. Recently the authors introduced an online kernel-based change-point detection method built upon direct estimation of the density ratio on consecutive time intervals. This paper further investigates this algorithm, making improvements and analyzing its behavior in the mean and mean square sense, in the absence and presence of a change point. These theoretical analyses are validated with Monte Carlo simulations. The detection performance of the algorithm is illustrated through experiments on real-world data and compared to state of the art methodologies.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/yanni/Zotero/storage/WDLIW27D/Bouchikhi et al. - 2020 - Online change-point detection with kernels.pdf}
}

@article{boumalIntroductionOptimizationSmooth,
  title = {An Introduction to Optimization on Smooth Manifolds},
  author = {Boumal, Nicolas},
  pages = {368},
  langid = {english},
  file = {/home/yanni/Zotero/storage/EBQ7G8KE/Boumal - An introduction to optimization on smooth manifold.pdf}
}

@article{boumalIntroductionOptimizationSmootha,
  title = {An Introduction to Optimization on Smooth Manifolds},
  author = {Boumal, Nicolas},
  pages = {368},
  langid = {english},
  file = {/home/yanni/Zotero/storage/8SKGHEJJ/Boumal - An introduction to optimization on smooth manifold.pdf}
}

@article{boumalIntroductionOptimizationSmoothb,
  title = {An Introduction to  Optimization on  Smooth Manifolds},
  author = {Boumal, Nicolas},
  pages = {362},
  langid = {english},
  file = {/home/yanni/Zotero/storage/CZMBYYNW/Boumal - An introduction to  optimization on  smooth manifo.pdf}
}

@article{boumalIntroductionOptimizationSmoothc,
  title = {An Introduction to  Optimization on  Smooth Manifolds},
  author = {Boumal, Nicolas},
  pages = {359},
  langid = {english},
  file = {/home/yanni/Zotero/storage/L9TH4VAL/Boumal - An introduction to  optimization on  smooth manifo.pdf}
}

@article{boumalIntroductionOptimizationSmoothd,
  title = {An Introduction to  Optimization on  Smooth Manifolds},
  author = {Boumal, Nicolas},
  pages = {310},
  langid = {english},
  file = {/home/yanni/Zotero/storage/KXYSYUNR/Boumal - An introduction to  optimization on  smooth manifo.pdf}
}

@article{boumalIntroductionOptimizationSmoothe,
  title = {An Introduction to Optimization on Smooth Manifolds},
  author = {Boumal, Nicolas},
  langid = {english},
  file = {/home/yanni/Zotero/storage/NV2N6ZHS/Boumal - An introduction to optimization on smooth manifold.pdf}
}

@article{bradleyPdeconstrainedOptimizationAdjoint2010,
  title = {Pde-Constrained Optimization and the Adjoint Method},
  author = {Bradley, Andrew M},
  year = {2010},
  publisher = {{Tutorial}}
}

@techreport{bradleyPDEconstrainedOptimizationAdjoint2013,
  title = {{{PDE-constrained}} Optimization and the Adjoint Method},
  author = {Bradley, Andrew M},
  year = {2013},
  abstract = {PDE-constrained optimization and the adjoint method for solving these and related problems appear in a wide range of application domains. Often the adjoint method is used in an application without explanation. The purpose of this tuto-rial is to explain the method in detail in a general setting that is kept as simple as possible. We use the following notation: the total derivative (gradient) is denoted d x (usually denoted d({$\cdot$})/dx or x); the partial derivative, {$\partial$} x (usually, {$\partial$}({$\cdot$})/{$\partial$} x); the differential, d. We also use the notation f x for both partial and total derivatives when we think the meaning is clear from context. Recall that a gradient is a row vector, and this convention induces sizing conventions for the other operators. We use only real numbers in this presentation.},
  file = {/home/yanni/Zotero/storage/8DXSICD3/Bradley - 2013 - PDE-constrained optimization and the adjoint method.pdf}
}

@article{briolProbabilisticIntegrationRole2019,
  title = {Probabilistic Integration: {{A}} Role in Statistical Computation?},
  author = {Briol, Fran{\c c}ois Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
  year = {2019},
  month = feb,
  journal = {Statistical Science},
  volume = {34},
  number = {1},
  pages = {1--22},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  doi = {10.1214/18-STS660},
  abstract = {A research frontier has emerged in scientific computation, wherein discretisation error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow, in order to assess the impact of discretisation error on the computer output. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the fact that the integrand has been discretised. Our main technical contribution is to establish, for the first time, rates of posterior contraction for one such method. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
  keywords = {Computational statistics,Nonparametric statistics,Probabilistic numerics,Uncertainty quantification}
}

@techreport{briolStatisticalInferenceGenerative2019,
  title = {Statistical {{Inference}} for {{Generative Models}} with {{Maximum Mean Discrepancy}}},
  author = {Briol, Fran{\c c}ois-Xavier and Barp, Alessandro and Duncan, Andrew B and Girolami, Mark},
  year = {2019},
  abstract = {While likelihood-based inference and its variants provide a statistically efficient and widely applicable approach to parametric inference, their application to models involving intractable likelihoods poses challenges. In this work, we study a class of minimum distance estima-tors for intractable generative models, that is, statistical models for which the likelihood is intractable, but simulation is cheap. The distance considered, maximum mean discrepancy (MMD), is defined through the embedding of probability measures into a reproducing kernel Hilbert space. We study the theoretical properties of these estimators, showing that they are consistent, asymptotically normal and robust to model misspecification. A main advantage of these estimators is the flexibility offered by the choice of kernel, which can be used to trade-off statistical efficiency and robustness. On the algorithmic side, we study the geometry induced by MMD on the parameter space and use this to introduce a novel natural gradient descent-like algorithm for efficient implementation of these estimators. We illustrate the relevance of our theoretical results on several classes of models including a discrete-time latent Markov process and two multivariate stochastic differential equation models.},
  file = {/home/yanni/Zotero/storage/QVGEWEQM/Briol et al. - 2019 - Statistical Inference for Generative Models with Maximum Mean Discrepancy.pdf}
}

@article{bui-thanhComputationalFrameworkInfinitedimensional2013,
  title = {A Computational Framework for Infinite-Dimensional Bayesian Inverse Problems Part {{I}}: {{The}} Linearized Case, with Application to Global Seismic Inversion},
  author = {{Bui-Thanh}, Tan and Ghattas, Omar and Martin, James and Stadler, Georg},
  year = {2013},
  journal = {SIAM Journal on Scientific Computing},
  volume = {35},
  number = {6},
  pages = {1--30},
  issn = {10648275},
  doi = {10.1137/12089586X},
  abstract = {We present a computational framework for estimating the uncertainty in the numerical solution of linearized infinite-dimensional statistical inverse problems. We adopt the Bayesian inference formulation: given observational data and their uncertainty, the governing forward problem and its uncertainty, and a prior probability distribution describing uncertainty in the parameter field, find the posterior probability distribution over the parameter field. The prior must be chosen appropriately in order to guarantee well-posedness of the infinite-dimensional inverse problem and facilitate computation of the posterior. Furthermore, straightforward discretizations may not lead to convergent approximations of the infinite-dimensional problem. And finally, solution of the discretized inverse problem via explicit construction of the covariance matrix is prohibitive due to the need to solve the forward problem as many times as there are parameters. Our computational framework builds on the infinite-dimensional formulation proposed by Stuart [Acta Numer., 19 (2010), pp. 451-559] and incorporates a number of components aimed at ensuring a convergent discretization of the underlying infinite-dimensional inverse problem. The framework additionally incorporates algorithms for manipulating the prior, constructing a low rank approximation of the data-informed component of the posterior covariance operator, and exploring the posterior that together ensure scalability of the entire framework to very high parameter dimensions. We demonstrate this computational framework on the Bayesian solution of an inverse problem in three-dimensional global seismic wave propagation with hundreds of thousands of parameters.Copyright \textcopyright{} by SIAM.},
  keywords = {Bayesian inference,Infinite-dimensional inverse problems,Low rank approximation,Scalable algorithms,Seismic wave propagation,Uncertainty quantification}
}

@article{buiStreamingSparseGaussian2017,
  title = {Streaming {{Sparse Gaussian Process Approximations}}},
  author = {Bui, Thang D. and Nguyen, Cuong V. and Turner, Richard E.},
  year = {2017},
  month = nov,
  journal = {arXiv:1705.07131 [stat]},
  eprint = {1705.07131},
  primaryclass = {stat},
  abstract = {Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseudo-input locations. The proposed framework is assessed using synthetic and real-world datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/W9IP67ZA/Bui et al. - 2017 - Streaming Sparse Gaussian Process Approximations.pdf}
}

@article{bulungaCHANGEPOINTDETECTIONDYNAMICAL,
  title = {{{CHANGE-POINT DETECTION IN DYNAMICAL SYSTEMS USING AUTO-ASSOCIATIVE NEURAL NETWORKS}}},
  author = {Bulunga, Meshack Linda},
  pages = {127},
  abstract = {In this research work, auto-associative neural networks have been used for changepoint detection. This is a nonlinear technique that employs the use of artificial neural networks as inspired among other by Frank Rosenblatt's linear perceptron algorithm for classification. An auto-associative neural network was used successfully to detect change-points for various types of time series data. Its performance was compared to that of singular spectrum analysis developed by Moskvina and Zhigljavsky. Fraction of Explained Variance (FEV) was also used to compare the performance of the two methods. FEV indicators are similar to the eigenvalues of the covariance matrix in principal component analysis.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/QEM78Y7G/Bulunga - CHANGE-POINT DETECTION IN DYNAMICAL SYSTEMS USING .pdf}
}

@article{burgEvaluationChangePoint2020,
  title = {An {{Evaluation}} of {{Change Point Detection Algorithms}}},
  author = {van den Burg, Gerrit J. J. and Williams, Christopher K. I.},
  year = {2020},
  month = may,
  journal = {arXiv:2003.06222 [cs, stat]},
  eprint = {2003.06222},
  primaryclass = {cs, stat},
  abstract = {Change point detection is an important part of time series analysis, as the presence of a change point indicates an abrupt and significant change in the data generating process. While many algorithms for change point detection exist, little attention has been paid to evaluating their performance on real-world time series. Algorithms are typically evaluated on simulated data and a small number of commonly-used series with unreliable ground truth. Clearly this does not provide sufficient insight into the comparative performance of these algorithms. Therefore, instead of developing yet another change point detection method, we consider it vastly more important to properly evaluate existing algorithms on real-world data. To achieve this, we present the first data set specifically designed for the evaluation of change point detection algorithms, consisting of 37 time series from various domains. Each time series was annotated by five expert human annotators to provide ground truth on the presence and location of change points. We analyze the consistency of the human annotators, and describe evaluation metrics that can be used to measure algorithm performance in the presence of multiple ground truth annotations. Subsequently, we present a benchmark study where 14 existing algorithms are evaluated on each of the time series in the data set. This study shows that binary segmentation (Scott and Knott, 1974) and Bayesian online change point detection (Adams and MacKay, 2007) are among the best performing methods. Our aim is that this data set will serve as a proving ground in the development of novel change point detection algorithms.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62M10,Computer Science - Machine Learning,G.3,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/KP5IMTIH/Burg and Williams - 2020 - An Evaluation of Change Point Detection Algorithms.pdf}
}

@misc{burtUnderstandingVariationalInference2020,
  title = {Understanding {{Variational Inference}} in {{Function-Space}}},
  author = {Burt, David R. and Ober, Sebastian W. and {Garriga-Alonso}, Adri{\`a} and {van der Wilk}, Mark},
  year = {2020},
  month = nov,
  number = {arXiv:2011.09421},
  eprint = {arXiv:2011.09421},
  publisher = {{arXiv}},
  abstract = {Recent work has attempted to directly approximate the `function-space' or predictive posterior distribution of Bayesian models, without approximating the posterior distribution over the parameters. This is appealing in e.g. Bayesian neural networks, where we only need the former, and the latter is hard to represent. In this work, we highlight some advantages and limitations of employing the Kullback-Leibler divergence in this setting. For example, we show that minimizing the KL divergence between a wide class of parametric distributions and the posterior induced by a (non-degenerate) Gaussian process prior leads to an ill-defined objective function. Then, we propose (featurized) Bayesian linear regression as a benchmark for `function-space' inference methods that directly measures approximation quality. We apply this methodology to assess aspects of the objective function and inference scheme considered in Sun, Zhang, Shi, and Grosse (2018), emphasizing the quality of approximation to Bayesian inference as opposed to predictive performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/IK9V495T/Burt et al_2020_Understanding Variational Inference in Function-Space.pdf}
}

@techreport{calderheadAcceleratingBayesianInference,
  title = {Accelerating {{Bayesian Inference}} over {{Nonlinear Differential Equations}} with {{Gaussian Processes}}},
  author = {Calderhead, Ben and Girolami, Mark and Lawrence, Neil D},
  abstract = {Identification and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many fields, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods.},
  file = {/home/yanni/Zotero/storage/D4T2VRSZ/Calderhead, Girolami, Lawrence - Unknown - Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes.pdf}
}

@techreport{calverNumericalMethodsComputing,
  title = {Numerical {{Methods}} for {{Computing Sensitivities}} for {{ODEs}} and {{DDEs}}},
  author = {Calver, Jonathan and Enright, Wayne},
  abstract = {We investigate the performance of the adjoint approach and the variational approach for computing the sensitivities of the least squares objective function commonly used when fitting models to observations. We note that the discrete nature of the objective function makes the cost of the adjoint approach for computing the sensitivities dependent on the number of observations. In the case of ODEs, this dependence is due to having to interrupt the computation at each observation point during numerical solution of the adjoint equations. Each observation introduces a jump discontinuity in the solution of the adjoint differential equations. These discontinuities are propagated in the case of DDEs, making the performance of the adjoint approach even more sensitive to the number of observations for DDEs. We quantify this cost and suggest ways to make the adjoint approach scale better with the number of observations. In numerical experiments, we compare the adjoint approach with the variational approach for computing the sensitivities.},
  keywords = {Adjoint method ¬∑,Delay differential equations ¬∑,Introduction,Ordinary differential equations ¬∑,Sensitivities,Variational equations ¬∑},
  file = {/home/yanni/Zotero/storage/PZNSTFGZ/Calver, Enright - Unknown - Numerical Methods for Computing Sensitivities for ODEs and DDEs.pdf}
}

@article{caronOnlineChangepointDetection2012,
  title = {On-Line Changepoint Detection and Parameter Estimation with Application to Genomic Data},
  author = {Caron, Fran{\c c}ois and Doucet, Arnaud and Gottardo, Raphael},
  year = {2012},
  month = mar,
  journal = {Stat Comput},
  volume = {22},
  number = {2},
  pages = {579--595},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-011-9248-x},
  abstract = {An efficient on-line changepoint detection algorithm for an important class of Bayesian product partition models has been recently proposed by Fearnhead and Liu (in J. R. Stat. Soc. B 69, 589\textendash 605, 2007). However a severe limitation of this algorithm is that it requires the knowledge of the static parameters of the model to infer the number of changepoints and their locations. We propose here an extension of this algorithm which allows us to estimate jointly on-line these static parameters using a recursive maximum likelihood estimation strategy. This particle filter type algorithm has a computational complexity which scales linearly both in the number of data and the number of particles. We demonstrate our methodology on a synthetic and two realworld datasets from RNA transcript analysis. On simulated data, it is shown that our approach outperforms standard techniques used in this context and hence has the potential to detect novel RNA transcripts.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/V3AS3R9W/Caron et al. - 2012 - On-line changepoint detection and parameter estima.pdf}
}

@article{celisseNewEfficientAlgorithms2017,
  title = {New Efficient Algorithms for Multiple Change-Point Detection with Kernels},
  author = {Celisse, Alain and Marot, Guillemette and {Pierre-Jean}, Morgane and Rigaill, Guillem},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.04556 [math, stat]},
  eprint = {1710.04556},
  primaryclass = {math, stat},
  abstract = {Several statistical approaches based on reproducing kernels have been proposed to detect abrupt changes arising in the full distribution of the observations and not only in the mean or variance. Some of these approaches enjoy good statistical properties (oracle inequality, . . . ). Nonetheless, they have a high computational cost both in terms of time and memory. This makes their application difficult even for small and medium sample sizes (n {$<$} 104). This computational issue is addressed by first describing a new efficient and exact algorithm for kernel multiple change-point detection with an improved worst-case complexity that is quadratic in time and linear in space. It allows dealing with medium size signals (up to n {$\approx$} 105). Second, a faster but approximation algorithm is described. It is based on a low-rank approximation to the Gram matrix. It is linear in time and space. This approximation algorithm can be applied to large-scale signals (n {$\geq$} 106). These exact and approximation algorithms have been implemented in R and C for various kernels. The computational and statistical performances of these new algorithms have been assessed through empirical experiments. The runtime of the new algorithms is observed to be faster than that of other considered procedures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/69APYXFH/Celisse et al. - 2017 - New efficient algorithms for multiple change-point.pdf}
}

@article{celisseNewEfficientAlgorithms2017a,
  title = {New Efficient Algorithms for Multiple Change-Point Detection with Kernels},
  author = {Celisse, Alain and Marot, Guillemette and {Pierre-Jean}, Morgane and Rigaill, Guillem},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.04556 [math, stat]},
  eprint = {1710.04556},
  primaryclass = {math, stat},
  abstract = {Several statistical approaches based on reproducing kernels have been proposed to detect abrupt changes arising in the full distribution of the observations and not only in the mean or variance. Some of these approaches enjoy good statistical properties (oracle inequality, . . . ). Nonetheless, they have a high computational cost both in terms of time and memory. This makes their application difficult even for small and medium sample sizes (n {$<$} 104). This computational issue is addressed by first describing a new efficient and exact algorithm for kernel multiple change-point detection with an improved worst-case complexity that is quadratic in time and linear in space. It allows dealing with medium size signals (up to n {$\approx$} 105). Second, a faster but approximation algorithm is described. It is based on a low-rank approximation to the Gram matrix. It is linear in time and space. This approximation algorithm can be applied to large-scale signals (n {$\geq$} 106). These exact and approximation algorithms have been implemented in R and C for various kernels. The computational and statistical performances of these new algorithms have been assessed through empirical experiments. The runtime of the new algorithms is observed to be faster than that of other considered procedures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/NTVB5FAG/Celisse et al. - 2017 - New efficient algorithms for multiple change-point.pdf}
}

@techreport{changConditioningDisintegration,
  title = {Conditioning as Disintegration},
  author = {Chang, J T and Pollard, D},
  abstract = {Conditional probability distributions seem to have a bad reputation when it comes to rigorous treatment of conditioning. Technical arguments are published as manipulations of Radon{$\pm$}Nikodym derivatives, although we all secretly perform heuristic calculations using elementary de\textregistered nitions of conditional probabilities. In print, measurability and averaging properties substitute for intuitive ideas about random variables behaving like constants given particular conditioning information. One way to engage in rigorous, guilt-free manipulation of conditional distributions is to treat them as disintegrating measures\DH families of probability measures concentrating on the level sets of a conditioning statistic. In this paper we present a little theory and a range of examples\DH from EM algorithms and the Neyman factorization, through Bayes theory and marginalization paradoxes\DH to suggest that disinte-grations have both intuitive appeal and the rigor needed for many problems in mathematical statistics.},
  keywords = {\& Phrases: Conditional probability distributions,admissibility,Basu's theorem,Bayes theory,disintegra-tions,EM algorithm,exchangeability,marginal-ization paradoxes,su¬Åciency},
  file = {/home/yanni/Zotero/storage/44IZ9PK4/Chang, Pollard - Unknown - Conditioning as disintegration.pdf}
}

@article{changKernelChangepointDetection2019,
  title = {Kernel {{Change-point Detection}} with {{Auxiliary Deep Generative Models}}},
  author = {Chang, Wei-Cheng and Li, Chun-Liang and Yang, Yiming and P{\'o}czos, Barnab{\'a}s},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.06077 [cs, stat]},
  eprint = {1901.06077},
  primaryclass = {cs, stat},
  abstract = {Detecting the emergence of abrupt property changes in time series is a challenging problem. Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches. However, selecting kernels is non-trivial in practice. Although kernel selection for two-sample test has been studied, the insufficient samples in change point detection problem hinders the success of those developed kernel selection algorithms. In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model. With deep kernel parameterization, KL-CPD endows kernel twosample test with the data-driven kernel to detect different types of change-points in real-world applications. The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/AAEXRJXA/Chang et al. - 2019 - Kernel Change-point Detection with Auxiliary Deep .pdf}
}

@article{changKERNELCHANGEPOINTDETECTION2019,
  title = {{{KERNEL CHANGE-POINT DETECTION WITH AUXIL- IARY DEEP GENERATIVE MODELS}}},
  author = {Chang, Wei-Cheng and Li, Chun-Liang and Yang, Yiming and P{\'o}czos, Barnab{\'a}s},
  year = {2019},
  pages = {14},
  abstract = {Detecting the emergence of abrupt property changes in time series is a challenging problem. Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches. However, selecting kernels is non-trivial in practice. Although kernel selection for two-sample test has been studied, the insufficient samples in change point detection problem hinders the success of those developed kernel selection algorithms. In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model. With deep kernel parameterization, KL-CPD endows kernel twosample test with the data-driven kernel to detect different types of change-points in real-world applications. The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/2B29YHN9/Chang et al. - 2019 - KERNEL CHANGE-POINT DETECTION WITH AUXIL- IARY DEE.pdf}
}

@article{chappellVariationalBayesianInference2009,
  title = {Variational {{Bayesian Inference}} for a {{Nonlinear Forward Model}}},
  author = {Chappell, M.A. and Groves, A.R. and Whitcher, B. and Woolrich, M.W.},
  year = {2009},
  month = jan,
  journal = {IEEE Trans. Signal Process.},
  volume = {57},
  number = {1},
  pages = {223--236},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2008.2005752},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GKC2F3BE/Chappell et al. - 2009 - Variational Bayesian Inference for a Nonlinear For.pdf}
}

@techreport{ChapterVariationalData,
  title = {Chapter 4 {{Variational Data Assimilation}} for {{Unsteady PDE Systems}}},
  abstract = {This chapter aims at extending the method of the previous chapter (linear tangent model, ad-joint model, local sensitivities, optimal set of parameters) to unsteady PDEs systems, parabolic or hyperbolic, non-linear or not. Calculations derived in the present section are formal. The final computational method obtained is the so-called 4D-var algorithms. Few variants of the so-called "4D-var" algorithms are presented (3D-var, incremental method). Few examples from recent research studies in fluid mechanics (shallow-water model) and in geophysics are presented. Finally, in the linear case (the forward model is linear), the equivalency between the Best Linear Unbiassed Estimate (BLUE) and the variational optimal solution is shown. 4.1 Introduction Data assimilation is the science combining "at best" the three knowledges of a system: the model (mathematical equations), the reality (observations-measurements) and some statistical errors (errors of measurements, model errors). Data assimilation refers to two different (and complimentary) approaches: 1) filtering such as the Kalman's filter (roughly, we calculate the Best Linear Unbiased Estimate-BLUE-using algebric calculations) or the Ensemble Kalman Filter (EnKF). It is stochastic methods. 2) variational data assimilation based on the present control theory. It is more deterministic methods, even if one can introduce statistical errors too. The basic principle is to minimize a cost function which measures the misfit between the output of the model and the measurements. Both approaches lead to the same estimate in the linear case. At the end of the chapter, we show the equivalency between the (basic) Best Linear Unbiased Estimate (BLUE) and the 53},
  file = {/home/yanni/Zotero/storage/CFDN3FZH/Unknown - Unknown - Chapter 4 Variational Data Assimilation for Unsteady PDE Systems.pdf}
}

@techreport{chengDifferentiationIntegralSign2010,
  title = {Differentiation {{Under}} the {{Integral Sign}} with {{Weak Derivatives}}},
  author = {Cheng, Steve},
  year = {2010},
  file = {/home/yanni/Zotero/storage/V75K7AN8/Cheng - 2010 - Differentiation Under the Integral Sign with Weak Derivatives.pdf}
}

@techreport{chenNeuralOrdinaryDifferential,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T Q and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  file = {/home/yanni/Zotero/storage/N6QTFMA7/Chen et al. - Unknown - Neural Ordinary Differential Equations.pdf}
}

@article{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  journal = {arXiv:1806.07366 [cs, stat]},
  eprint = {1806.07366},
  primaryclass = {cs, stat},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/X67M863J/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf}
}

@article{chenNeuralOrdinaryDifferential2019a,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  journal = {arXiv:1806.07366 [cs, stat]},
  eprint = {1806.07366},
  primaryclass = {cs, stat},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/SKQPLXKS/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf}
}

@article{chenRemarksMultivariateGaussian2021,
  title = {Remarks on Multivariate {{Gaussian Process}}},
  author = {Chen, Zexun and Fan, Jun and Wang, Kuo},
  year = {2021},
  month = feb,
  journal = {arXiv:2010.09830 [math, stat]},
  eprint = {2010.09830},
  primaryclass = {math, stat},
  abstract = {Gaussian processes occupy one of the leading places in modern statistics and probability theory due to their importance and a wealth of strong results. The common use of Gaussian processes is in connection with problems related to estimation, detection, and many statistical or machine learning models. With the fast development of Gaussian process applications, it is necessary to consolidate the fundamentals of vector-valued stochastic processes, in particular multivariate Gaussian processes, which is the essential theory for many applied problems with multiple correlated responses. In this paper, we propose a precise definition of multivariate Gaussian processes based on Gaussian measures on vector-valued function spaces, and provide an existence proof. In addition, several fundamental properties of multivariate Gaussian processes, such as strict stationarity and independence, are introduced. We further derive multivariate Brownian motion including It\textbackslash\^o lemma as a special case of a multivariate Gaussian process, and present a brief introduction to multivariate Gaussian process regression as a useful statistical learning method for multi-output prediction problems.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/home/yanni/Zotero/storage/PG8PSW55/Chen et al. - 2021 - Remarks on multivariate Gaussian Process.pdf;/home/yanni/Zotero/storage/4ETB5WEA/2010.html}
}

@misc{chenRemarksMultivariateGaussian2021a,
  title = {Remarks on Multivariate {{Gaussian Process}}},
  author = {Chen, Zexun and Fan, Jun and Wang, Kuo},
  year = {2021},
  month = feb,
  number = {arXiv:2010.09830},
  eprint = {arXiv:2010.09830},
  publisher = {{arXiv}},
  abstract = {Gaussian processes occupy one of the leading places in modern statistics and probability theory due to their importance and a wealth of strong results. The common use of Gaussian processes is in connection with problems related to estimation, detection, and many statistical or machine learning models. With the fast development of Gaussian process applications, it is necessary to consolidate the fundamentals of vector-valued stochastic processes, in particular multivariate Gaussian processes, which is the essential theory for many applied problems with multiple correlated responses. In this paper, we propose a precise definition of multivariate Gaussian processes based on Gaussian measures on vector-valued function spaces, and provide an existence proof. In addition, several fundamental properties of multivariate Gaussian processes, such as strict stationarity and independence, are introduced. We further derive multivariate Brownian motion including It\textbackslash\^o lemma as a special case of a multivariate Gaussian process, and present a brief introduction to multivariate Gaussian process regression as a useful statistical learning method for multi-output prediction problems.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/home/yanni/Zotero/storage/RV2ARZCS/Chen et al. - 2021 - Remarks on multivariate Gaussian Process.pdf;/home/yanni/Zotero/storage/Y9LPHZFV/2010.html}
}

@book{chikuseStatisticsSpecialManifolds2003,
  title = {Statistics on {{Special Manifolds}}},
  author = {Chikuse, Yasuko},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  year = {2003},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {174},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-21540-2},
  isbn = {978-0-387-00160-9 978-0-387-21540-2},
  langid = {english},
  file = {/home/yanni/Zotero/storage/YQ7GSYBE/Chikuse - 2003 - Statistics on Special Manifolds.pdf}
}

@book{chikuseStatisticsSpecialManifolds2003a,
  title = {Statistics on {{Special Manifolds}}},
  author = {Chikuse, Yasuko},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  year = {2003},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {174},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-21540-2},
  isbn = {978-0-387-00160-9 978-0-387-21540-2},
  langid = {english},
  file = {/home/yanni/Zotero/storage/HVTCN5KF/Chikuse - 2003 - Statistics on Special Manifolds.pdf}
}

@article{chkrebtiiBayesianSolutionUncertainty2016,
  title = {Bayesian {{Solution Uncertainty Quantification}} for {{Differential Equations}}},
  author = {Chkrebtii, Oksana A. and Campbell, David A. and Calderhead, Ben and Girolami, Mark A.},
  year = {2016},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {11},
  number = {4},
  pages = {1239--1267},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/16-BA1017},
  abstract = {We explore probability modelling of discretization uncertainty for system states defined implicitly by ordinary or partial differential equations. Accounting for this uncertainty can avoid posterior under-coverage when likelihoods are constructed from a coarsely discretized approximation to system equations. A formalism is proposed for inferring a fixed but a priori unknown model trajectory through Bayesian updating of a prior process conditional on model information. A one-step-ahead sampling scheme for interrogating the model is described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. Examples illustrate the flexibility of this framework to deal with a wide variety of complex and large-scale systems. Within the calibration problem, discretization uncertainty defines a layer in the Bayesian hierarchy, and a Markov chain Monte Carlo algorithm that targets this posterior distribution is presented. This formalism is used for inference on the JAK-STAT delay differential equation model of protein dynamics from indirectly observed measurements. The discussion outlines implications for the new field of probabilistic numerics.},
  keywords = {Bayesian numerical analysis,differential equation models,Gaussian processes,uncertainty in computer models,uncertainty quantification},
  file = {/home/yanni/Zotero/storage/XWERXJZZ/Chkrebtii et al. - 2016 - Bayesian Solution Uncertainty Quantification for D.pdf;/home/yanni/Zotero/storage/C3TU296X/16-BA1017.html}
}

@article{chopinDynamicDetectionChange2007,
  title = {Dynamic {{Detection}} of {{Change Points}} in {{Long Time Series}}},
  author = {Chopin, Nicolas},
  year = {2007},
  month = may,
  journal = {AISM},
  volume = {59},
  number = {2},
  pages = {349--366},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/s10463-006-0053-9},
  abstract = {We consider the problem of detecting change points (structural changes) in long sequences of data, whether in a sequential fashion or not, and without assuming prior knowledge of the number of these change points. We reformulate this problem as the Bayesian filtering and smoothing of a non standard state space model. Towards this goal, we build a hybrid algorithm that relies on particle filtering and Markov chain Monte Carlo ideas. The approach is illustrated by a GARCH change point model.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/26MPHGDA/Chopin - 2007 - Dynamic Detection of Change Points in Long Time Se.pdf}
}

@article{chowdhuryBayesianOnlineSpectral2012,
  title = {Bayesian On-Line Spectral Change Point Detection: A Soft Computing Approach for on-Line {{ASR}}},
  shorttitle = {Bayesian On-Line Spectral Change Point Detection},
  author = {Chowdhury, M. F. R. and Selouani, S.-A. and O'Shaughnessy, D.},
  year = {2012},
  month = mar,
  journal = {Int J Speech Technol},
  volume = {15},
  number = {1},
  pages = {5--23},
  issn = {1381-2416, 1572-8110},
  doi = {10.1007/s10772-011-9116-2},
  langid = {english},
  file = {/home/yanni/Zotero/storage/R698ZYXM/Chowdhury et al. - 2012 - Bayesian on-line spectral change point detection .pdf}
}

@article{chowellFittingDynamicModels2017,
  title = {Fitting Dynamic Models to Epidemic Outbreaks with Quantified Uncertainty: {{A}} Primer for Parameter Uncertainty, Identifiability, and Forecasts},
  author = {Chowell, Gerardo},
  year = {2017},
  journal = {Infectious Disease Modelling},
  issn = {24680427},
  doi = {10.1016/j.idm.2017.08.001},
  abstract = {Mathematical models provide a quantitative framework with which scientists can assess hypotheses on the potential underlying mechanisms that explain patterns in observed data at different spatial and temporal scales, generate estimates of key kinetic parameters, assess the impact of interventions, optimize the impact of control strategies, and generate forecasts. We review and illustrate a simple data assimilation framework for calibrating mathematical models based on ordinary differential equation models using time series data describing the temporal progression of case counts relating, for instance, to population growth or infectious disease transmission dynamics. In contrast to Bayesian estimation approaches that always raise the question of how to set priors for the parameters, this frequentist approach relies on modeling the error structure in the data. We discuss issues related to parameter identifiability, uncertainty quantification and propagation as well as model performance and forecasts along examples based on phenomenological and mechanistic models parameterized using simulated and real datasets.},
  file = {/home/yanni/Zotero/storage/A6R28F37/Chowell - 2017 - Fitting dynamic models to epidemic outbreaks with quantified uncertainty A primer for parameter uncertainty, identifiab.pdf}
}

@book{ciarletFiniteElementMethod2002,
  title = {The {{Finite Element Method}} for {{Elliptic Problems}}},
  author = {Ciarlet, Philippe G.},
  year = {2002},
  month = jan,
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898719208},
  isbn = {978-0-89871-514-9},
  keywords = {boundary value problems,differential equations,elliptics,finite element method,numerial solutions},
  file = {/home/yanni/Zotero/storage/WCHP8MP7/Ciarlet - 2002 - The Finite Element Method for Elliptic Problems.pdf}
}

@article{cockayneBayesianConjugateGradient2018,
  title = {A {{Bayesian Conjugate Gradient Method}}},
  author = {Cockayne, Jon and Oates, Chris and Ipsen, Ilse and Girolami, Mark},
  year = {2018},
  abstract = {A fundamental task in numerical computation is the solution of large linear systems. The conjugate gradient method is an iterative method which offers rapid convergence to the solution, particularly when an effective preconditioner is employed. However, for more challenging systems a substantial error can be present even after many iterations have been performed. The estimates obtained in this case are of little value unless further information can be provided about the numerical error. In this paper we propose a novel statistical model for this numerical error set in a Bayesian framework. Our approach is a strict generalisation of the conjugate gradient method, which is recovered as the posterior mean for a particular choice of prior. The estimates obtained are analysed with Krylov subspace methods and a contraction result for the posterior is presented. The method is then analysed in a simulation study as well as being applied to a challenging problem in medical imaging.},
  file = {/home/yanni/Zotero/storage/UC3RPNXD/Cockayne et al. - 2018 - A Bayesian Conjugate Gradient Method.pdf}
}

@article{cockayneBayesianConjugateGradient2019,
  title = {A {{Bayesian Conjugate Gradient Method}} (with {{Discussion}})},
  author = {Cockayne, Jon and Oates, Chris J. and Ipsen, Ilse C.F. and Girolami, Mark},
  year = {2019},
  month = sep,
  journal = {Bayesian Anal.},
  volume = {14},
  number = {3},
  issn = {1936-0975},
  doi = {10.1214/19-BA1145},
  abstract = {A fundamental task in numerical computation is the solution of large linear systems. The conjugate gradient method is an iterative method which offers rapid convergence to the solution, particularly when an effective preconditioner is employed. However, for more challenging systems a substantial error can be present even after many iterations have been performed. The estimates obtained in this case are of little value unless further information can be provided about, for example, the magnitude of the error. In this paper we propose a novel statistical model for this error, set in a Bayesian framework. Our approach is a strict generalisation of the conjugate gradient method, which is recovered as the posterior mean for a particular choice of prior. The estimates obtained are analysed with Krylov subspace methods and a contraction result for the posterior is presented. The method is then analysed in a simulation study as well as being applied to a challenging problem in medical imaging.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/F8CAGBMM/Cockayne et al. - 2019 - A Bayesian Conjugate Gradient Method (with Discuss.pdf}
}

@techreport{cockayneBayesianProbabilisticNumerical2017,
  title = {Bayesian {{Probabilistic Numerical Methods}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  year = {2017},
  abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
  file = {/home/yanni/Zotero/storage/5TLQVNIP/Cockayne et al. - 2017 - Bayesian Probabilistic Numerical Methods.pdf}
}

@article{cockayneBayesianProbabilisticNumerical2019,
  title = {Bayesian {{Probabilistic Numerical Methods}}},
  author = {Cockayne, Jon and Oates, Chris J. and Sullivan, T. J. and Girolami, Mark},
  year = {2019},
  month = jan,
  journal = {SIAM Rev.},
  volume = {61},
  number = {3},
  pages = {756--789},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/17M1139357},
  langid = {english},
  file = {/home/yanni/Zotero/storage/DCKCK3YF/Cockayne et al. - 2019 - Bayesian Probabilistic Numerical Methods.pdf}
}

@article{cockayneProbabilisticGradientsFast2020,
  title = {Probabilistic Gradients for Fast Calibration of Differential Equation Models},
  author = {Cockayne, Jon and Duncan, Andrew B.},
  year = {2020},
  journal = {arXiv},
  issn = {23318422},
  abstract = {Calibration of large-scale differential equation models to observational or experimental data is a widespread challenge throughout applied sciences and engineering. A crucial bottleneck in state-of-the art calibration methods is the calculation of local sensitivities, i.e. derivatives of the loss function with respect to the estimated parameters, which often necessitates several numerical solves of the underlying system of partial or ordinary differential equations. In this paper we present a new probabilistic approach to computing local sensitivities. The proposed method has several advantages over classical methods. Firstly, it operates within a constrained computational budget and provides a probabilistic quantification of uncertainty incurred in the sensitivities from this constraint. Secondly, information from previous sensitivity estimates can be recycled in subsequent computations, reducing the overall computational effort for iterative gradient-based calibration methods. The methodology presented is applied to two challenging test problems and compared against classical methods.},
  file = {/home/yanni/Zotero/storage/KJPGSTUR/2009.04239.pdf}
}

@article{cockayneProbabilisticIterativeMethods2021,
  title = {Probabilistic {{Iterative Methods}} for {{Linear Systems}}},
  author = {Cockayne, Jon and Ipsen, Ilse C. F. and Oates, Chris J. and Reid, Tim W.},
  year = {2021},
  month = jan,
  journal = {arXiv:2012.12615 [cs, math, stat]},
  eprint = {2012.12615},
  primaryclass = {cs, math, stat},
  abstract = {This paper presents a probabilistic perspective on iterative methods for approximating the solution \$\textbackslash mathbf\{x\}\_* \textbackslash in \textbackslash mathbb\{R\}\^d\$ of a nonsingular linear system \$\textbackslash mathbf\{A\} \textbackslash mathbf\{x\}\_* = \textbackslash mathbf\{b\}\$. In the approach a standard iterative method on \$\textbackslash mathbb\{R\}\^d\$ is lifted to act on the space of probability distributions \$\textbackslash mathcal\{P\}(\textbackslash mathbb\{R\}\^d)\$. Classically, an iterative method produces a sequence \$\textbackslash mathbf\{x\}\_m\$ of approximations that converge to \$\textbackslash mathbf\{x\}\_*\$. The output of the iterative methods proposed in this paper is, instead, a sequence of probability distributions \$\textbackslash mu\_m \textbackslash in \textbackslash mathcal\{P\}(\textbackslash mathbb\{R\}\^d)\$. The distributional output both provides a "best guess" for \$\textbackslash mathbf\{x\}\_*\$, for example as the mean of \$\textbackslash mu\_m\$, and also probabilistic uncertainty quantification for the value of \$\textbackslash mathbf\{x\}\_*\$ when it has not been exactly determined. Theoretical analysis is provided in the prototypical case of a stationary linear iterative method. In this setting we characterise both the rate of contraction of \$\textbackslash mu\_m\$ to an atomic measure on \$\textbackslash mathbf\{x\}\_*\$ and the nature of the uncertainty quantification being provided. We conclude with an empirical illustration that highlights the insight into solution uncertainty that can be provided by probabilistic iterative methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/VRYLNLL8/Cockayne et al. - 2021 - Probabilistic Iterative Methods for Linear Systems.pdf}
}

@article{cockayneProbabilisticIterativeMethods2021a,
  title = {Probabilistic {{Iterative Methods}} for {{Linear Systems}}},
  author = {Cockayne, Jon and Ipsen, Ilse C. F. and Oates, Chris J. and Reid, Tim W.},
  year = {2021},
  month = jan,
  journal = {arXiv:2012.12615 [cs, math, stat]},
  eprint = {2012.12615},
  primaryclass = {cs, math, stat},
  abstract = {This paper presents a probabilistic perspective on iterative methods for approximating the solution \$\textbackslash mathbf\{x\}\_* \textbackslash in \textbackslash mathbb\{R\}\^d\$ of a nonsingular linear system \$\textbackslash mathbf\{A\} \textbackslash mathbf\{x\}\_* = \textbackslash mathbf\{b\}\$. In the approach a standard iterative method on \$\textbackslash mathbb\{R\}\^d\$ is lifted to act on the space of probability distributions \$\textbackslash mathcal\{P\}(\textbackslash mathbb\{R\}\^d)\$. Classically, an iterative method produces a sequence \$\textbackslash mathbf\{x\}\_m\$ of approximations that converge to \$\textbackslash mathbf\{x\}\_*\$. The output of the iterative methods proposed in this paper is, instead, a sequence of probability distributions \$\textbackslash mu\_m \textbackslash in \textbackslash mathcal\{P\}(\textbackslash mathbb\{R\}\^d)\$. The distributional output both provides a "best guess" for \$\textbackslash mathbf\{x\}\_*\$, for example as the mean of \$\textbackslash mu\_m\$, and also probabilistic uncertainty quantification for the value of \$\textbackslash mathbf\{x\}\_*\$ when it has not been exactly determined. Theoretical analysis is provided in the prototypical case of a stationary linear iterative method. In this setting we characterise both the rate of contraction of \$\textbackslash mu\_m\$ to an atomic measure on \$\textbackslash mathbf\{x\}\_*\$ and the nature of the uncertainty quantification being provided. We conclude with an empirical illustration that highlights the insight into solution uncertainty that can be provided by probabilistic iterative methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/BGSE2DZB/Cockayne et al. - 2021 - Probabilistic Iterative Methods for Linear Systems.pdf}
}

@techreport{cockayneProbabilisticNumericalMethods2017,
  title = {Probabilistic {{Numerical Methods}} for {{PDE-constrained Bayesian Inverse Problems}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  year = {2017},
  abstract = {This paper develops meshless methods for probabilistically describing dis-cretisation error in the numerical solution of partial differential equations. This construction enables the solution of Bayesian inverse problems while accounting for the impact of the discretisation of the forward problem. In particular, this drives statistical inferences to be more conservative in the presence of significant solver error. Theoretical results are presented describing rates of convergence for the posteriors in both the forward and inverse problems. This method is tested on a challenging inverse problem with a nonlinear forward model.},
  file = {/home/yanni/Zotero/storage/GWSYRYJF/Cockayne et al. - 2017 - Probabilistic Numerical Methods for PDE-constrained Bayesian Inverse Problems.pdf}
}

@misc{cockayneProbabilisticNumericalMethods2017a,
  title = {Probabilistic {{Numerical Methods}} for {{Partial Differential Equations}} and {{Bayesian Inverse Problems}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  year = {2017},
  month = jul,
  number = {arXiv:1605.07811},
  eprint = {arXiv:1605.07811},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.07811},
  abstract = {This paper develops a probabilistic numerical method for solution of partial differential equations (PDEs) and studies application of that method to PDE-constrained inverse problems. This approach enables the solution of challenging inverse problems whilst accounting, in a statistically principled way, for the impact of discretisation error due to numerical solution of the PDE. In particular, the approach confers robustness to failure of the numerical PDE solver, with statistical inferences driven to be more conservative in the presence of substantial discretisation error. Going further, the problem of choosing a PDE solver is cast as a problem in the Bayesian design of experiments, where the aim is to minimise the impact of solver error on statistical inferences; here the challenge of non-linear PDEs is also considered. The method is applied to parameter inference problems in which discretisation error in non-negligible and must be accounted for in order to reach conclusions that are statistically valid.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/AKVDKE8Y/Cockayne et al. - 2017 - Probabilistic Numerical Methods for Partial Differ.pdf;/home/yanni/Zotero/storage/Q3DPHLJS/1605.html}
}

@article{cockayneTestingWhetherLearning2021,
  title = {Testing Whether a {{Learning Procedure}} Is {{Calibrated}}},
  author = {Cockayne, Jon and Graham, Matthew M. and Oates, Chris J. and Sullivan, T. J.},
  year = {2021},
  month = jan,
  journal = {arXiv:2012.12670 [math, stat]},
  eprint = {2012.12670},
  primaryclass = {math, stat},
  abstract = {A learning procedure takes as input a dataset and performs inference for the parameters \texttheta{} of a model that is assumed to have given rise to the dataset. Here we consider learning procedures whose output is a probability distribution, representing uncertainty about \texttheta{} after seeing the dataset. Bayesian inference is a prime example of such a procedure but one can also construct other learning procedures that return distributional output. This paper studies conditions for a learning procedure to be considered calibrated, in the sense that the true data-generating parameters are plausible as samples from its distributional output. A learning procedure that is calibrated need not be statistically efficient and vice versa. A hypothesis-testing framework is developed in order to assess, using simulation, whether a learning procedure is calibrated. Finally, we exploit our framework to test the calibration of some learning procedures that are motivated as being approximations to Bayesian inference but are nevertheless widely used.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/yanni/Zotero/storage/C9DZICSV/Cockayne et al. - 2021 - Testing whether a Learning Procedure is Calibrated.pdf}
}

@article{cohenGaugeEquivariantConvolutional2019,
  title = {Gauge {{Equivariant Convolutional Networks}} and the {{Icosahedral CNN}}},
  author = {Cohen, Taco S. and Weiler, Maurice and Kicanaoglu, Berkay and Welling, Max},
  year = {2019},
  month = may,
  journal = {arXiv:1902.04615 [cs, stat]},
  eprint = {1902.04615},
  primaryclass = {cs, stat},
  abstract = {The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/QR2UYMEM/Cohen et al. - 2019 - Gauge Equivariant Convolutional Networks and the I.pdf;/home/yanni/Zotero/storage/W6CPBJG5/1902.html}
}

@article{cohenMultivariateAnalysisM5MS08,
  title = {Multivariate {{Analysis}} ({{M5MS08}}) {{Lecture Notes}}},
  author = {Cohen, E A K},
  pages = {51},
  langid = {english},
  file = {/home/yanni/Zotero/storage/S5IC4B7C/Cohen - Multivariate Analysis (M5MS08) Lecture Notes.pdf}
}

@book{cohnMeasureTheorySecond2013,
  title = {Measure {{Theory}}: {{Second Edition}}},
  shorttitle = {Measure {{Theory}}},
  author = {Cohn, Donald L.},
  year = {2013},
  series = {Birkh\"auser {{Advanced Texts Basler Lehrb\"ucher}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-6956-8},
  isbn = {978-1-4614-6955-1 978-1-4614-6956-8},
  langid = {english},
  file = {/home/yanni/Zotero/storage/FJBJDF85/Cohn - 2013 - Measure Theory Second Edition.pdf}
}

@article{conradProbabilityMeasuresNumerical,
  title = {Probability {{Measures}} for {{Numerical Solutions}} of {{Differential Equations}}},
  author = {Conrad, Patrick and Girolami, Mark and {S\"arkk\"a, Simo} and {Stuart, Andrew} and Zygalakis, Konstantinos},
  issn = {00905364},
  doi = {10.1214/15-AOS1380},
  isbn = {0090-5364},
  keywords = {Folded concave penalties,Global optimization,High-dimensional statistical learning,MCP,Nonconvex quadratic programming,SCAD,Sparse recovery},
  file = {/home/yanni/Zotero/storage/XGNAR7BU/Conrad et al. - Unknown - Probability Measures for Numerical Solutions of Differential Equations.pdf}
}

@article{conradStatisticalAnalysisDifferential2017,
  title = {Statistical Analysis of Differential Equations: Introducing Probability Measures on Numerical Solutions},
  author = {Conrad, Patrick R and Girolami, {$\cdot$} Mark and S{\"a}rkk{\"a}, Simo and Stuart, Andrew and Zygalakis, Konstantinos},
  year = {2017},
  journal = {Statistics and Computing},
  volume = {27},
  pages = {1065--1082},
  doi = {10.1007/s11222-016-9671-0},
  abstract = {In this paper, we present a formal quantification of uncertainty induced by numerical solutions of ordinary and partial differential equation models. Numerical solutions of differential equations contain inherent uncertainties due to the finite-dimensional approximation of an unknown and implicitly defined function. When statistically analysing models based on differential equations describing physical, or other naturally occurring, phenomena, it can be important to explicitly account for the uncertainty introduced by the numerical method. Doing so enables objective determination of this source of uncertainty, relative to other uncertainties, such as those caused by data contaminated with noise or model error induced by missing physical or inadequate descriptors. As ever larger scale mathematical models are being used in the sciences, often sacrificing complete resolution of the differential equation on the grids used, formally accounting for the uncertainty in the numerical method is becoming increasingly more important. This paper provides the formal means to incorporate this uncer-Electronic supplementary material The online version of this article (tainty in a statistical model and its subsequent analysis. We show that a wide variety of existing solvers can be randomised, inducing a probability measure over the solutions of such differential equations. These measures exhibit contraction to a Dirac measure around the true unknown solution , where the rates of convergence are consistent with the underlying deterministic numerical method. Furthermore, we employ the method of modified equations to demonstrate enhanced rates of convergence to stochastic perturbations of the original deterministic problem. Ordinary differential equations and elliptic partial differential equations are used to illustrate the approach to quantify uncertainty in both the statistical analysis of the forward and inverse problems.},
  isbn = {1122201696710},
  keywords = {Inverse problems,Numerical analysis,Probabilistic numerics,Uncertainty quantification},
  file = {/home/yanni/Zotero/storage/RKDUCWGX/Conrad et al. - 2017 - Statistical analysis of differential equations introducing probability measures on numerical solutions.pdf}
}

@article{constantineActiveSubspaceMethods2014,
  title = {Active {{Subspace Methods}} in {{Theory}} and {{Practice}}: {{Applications}} to {{Kriging Surfaces}}},
  shorttitle = {Active {{Subspace Methods}} in {{Theory}} and {{Practice}}},
  author = {Constantine, Paul G. and Dow, Eric and Wang, Qiqi},
  year = {2014},
  month = jan,
  journal = {SIAM J. Sci. Comput.},
  volume = {36},
  number = {4},
  pages = {A1500-A1524},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/130916138},
  abstract = {High accuracy complex computer models, also called simulators, require large resources in time and memory to produce realistic results. Statistical emulators are computationally cheap approximations of such simulators. They can be built to replace simulators for various purposes, such as the propagation of uncertainties from inputs to outputs or the calibration of some internal parameters against observations. However, when the input space is of high dimension, the construction of an emulator can become prohibitively expensive. In this paper, we introduce a joint framework merging emulation with dimension reduction in order to overcome this hurdle. The gradient-based kernel dimension reduction technique is chosen due to its ability to drastically decrease dimensionality with little loss in information. The Gaussian process emulation technique is combined with this dimension reduction approach. Theoretical properties of the approximation are explored. Our proposed approach provides an answer to the dimension reduction issue in emulation for a wide range of simulation problems that cannot be tackled using existing methods. The efficiency and accuracy of the proposed framework is demonstrated theoretically and compared with other methods on an elliptic partial differential equation (PDE) problem. We finally present a realistic application to tsunami modeling. The uncertainties in the bathymetry (seafloor elevation) are modeled as high-dimensional realizations of a spatial process using a geostatistical approach. Our dimension-reduced emulation enables us to compute the impact of these uncertainties on resulting possible tsunami wave heights near-shore and on-shore. Considering an uncertain earthquake source, we observe a significant increase in the spread of uncertainties in the tsunami heights due to the contribution of the bathymetry uncertainties to the overall uncertainty budget. These results highlight the need to include the effect of uncertainties in the bathymetry in tsunami early warnings and risk assessments.},
  file = {/home/yanni/Zotero/storage/HTQUJE9E/Constantine et al. - 2014 - Active Subspace Methods in Theory and Practice Ap.pdf}
}

@book{constantineActiveSubspacesEmerging2015,
  title = {Active Subspaces: Emerging Ideas for Dimension Reduction in Parameter Studies},
  shorttitle = {Active Subspaces},
  author = {Constantine, Paul G.},
  year = {2015},
  series = {{{SIAM}} Spotlights},
  number = {2},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia}},
  isbn = {978-1-61197-385-3},
  langid = {english},
  lccn = {QA322 .C64 2015},
  keywords = {Function spaces,Functional analysis,Hilbert space,Linear topological spaces,Shift operators (Operator theory)},
  file = {/home/yanni/Zotero/storage/QXMVVLQ5/Constantine - 2015 - Active subspaces emerging ideas for dimension red.pdf}
}

@book{constantineActiveSubspacesEmerging2015a,
  title = {Active Subspaces: Emerging Ideas for Dimension Reduction in Parameter Studies},
  shorttitle = {Active Subspaces},
  author = {Constantine, Paul G.},
  year = {2015},
  series = {{{SIAM}} Spotlights},
  number = {2},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia}},
  isbn = {978-1-61197-385-3},
  langid = {english},
  lccn = {QA322 .C64 2015},
  keywords = {Function spaces,Functional analysis,Hilbert space,Linear topological spaces,Shift operators (Operator theory)},
  file = {/home/yanni/Zotero/storage/2FS2J8TL/Constantine - 2015 - Active subspaces emerging ideas for dimension red.pdf}
}

@misc{constantineComputingActiveSubspaces2015,
  title = {Computing Active Subspaces with {{Monte Carlo}}},
  author = {Constantine, Paul and Gleich, David},
  year = {2015},
  month = jul,
  number = {arXiv:1408.0545},
  eprint = {arXiv:1408.0545},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1408.0545},
  abstract = {Active subspaces can effectively reduce the dimension of high-dimensional parameter studies enabling otherwise infeasible experiments with expensive simulations. The key components of active subspace methods are the eigenvectors of a symmetric, positive semidefinite matrix whose elements are the average products of partial derivatives of the simulation's input/output map. We study a Monte Carlo method for approximating the eigenpairs of this matrix. We offer both theoretical results based on recent non-asymptotic random matrix theory and a practical approach based on the bootstrap. We extend the analysis to the case when the gradients are approximated, for example, with finite differences. Our goal is to provide guidance for two questions that arise in active subspaces: (i) How many gradient samples does one need to accurately approximate the eigenvalues and subspaces? (ii) What can be said about the accuracy of the estimated subspace, both theoretically and practically? We test the approach on both simple quadratic functions where the active subspace is known and a parameterized PDE with 100 variables characterizing the coefficients of the differential operator.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/RTAFKQML/Constantine and Gleich - 2015 - Computing active subspaces with Monte Carlo.pdf;/home/yanni/Zotero/storage/GQNKU9V7/1408.html}
}

@article{constantineNearstationarySubspaceRidge2017,
  title = {A Near-Stationary Subspace for Ridge Approximation},
  author = {Constantine, Paul G. and Eftekhari, Armin and Hokanson, Jeffrey and Ward, Rachel},
  year = {2017},
  month = nov,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {326},
  eprint = {1606.01929},
  pages = {402--421},
  issn = {00457825},
  doi = {10.1016/j.cma.2017.07.038},
  abstract = {Response surfaces are common surrogates for expensive computer simulations in engineering analysis. However, the cost of fitting an accurate response surface increases exponentially as the number of model inputs increases, which leaves response surface construction intractable for high-dimensional, nonlinear models. We describe ridge approximation for fitting response surfaces in several variables. A ridge function is constant along several directions in its domain, so fitting occurs on the coordinates of a low-dimensional subspace of the input space. We review essential theory for ridge approximation---e.g., the best mean-squared approximation and an optimal low-dimensional subspace---and we prove that the gradient-based active subspace is near-stationary for the least-squares problem that defines an optimal subspace. Motivated by the theory, we propose a computational heuristic that uses an estimated active subspace as an initial guess for a ridge approximation fitting problem. We show a simple example where the heuristic fails, which reveals a type of function for which the proposed approach is inappropriate. We then propose a simple alternating heuristic for fitting a ridge function, and we demonstrate the effectiveness of the active subspace initial guess applied to an airfoil model of drag as a function of its 18 shape parameters.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/D5TH7AGR/Constantine et al. - 2017 - A near-stationary subspace for ridge approximation.pdf;/home/yanni/Zotero/storage/RU4ANGU9/1606.html}
}

@article{ContinuousInspectionSchemes2021,
  title = {Continuous {{Inspection Schemes}}},
  year = {2021},
  pages = {17},
  langid = {english},
  file = {/home/yanni/Zotero/storage/35ZK4ERW/2021 - Continuous Inspection Schemes.pdf}
}

@article{cotterBayesianInverseProblems2009,
  title = {Bayesian Inverse Problems for Functions and Applications to Fluid Mechanics},
  author = {Cotter, S L and Dashti, M and Robinson, J C and Stuart, A M},
  year = {2009},
  month = nov,
  journal = {Inverse Problems},
  volume = {25},
  number = {11},
  pages = {115008},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/25/11/115008},
  langid = {english},
  file = {/home/yanni/Zotero/storage/6CC9QIUG/Cotter et al. - 2009 - Bayesian inverse problems for functions and applic.pdf}
}

@article{cotterBayesianInverseProblems2009a,
  title = {Bayesian Inverse Problems for Functions and Applications to Fluid Mechanics},
  author = {Cotter, S. L. and Dashti, M. and Robinson, J. C. and Stuart, A. M.},
  year = {2009},
  month = oct,
  journal = {Inverse Problems},
  volume = {25},
  number = {11},
  pages = {115008},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/25/11/115008},
  abstract = {In this paper we establish a mathematical framework for a range of inverse problems for functions, given a finite set of noisy observations. The problems are hence underdetermined and are often ill-posed. We study these problems from the viewpoint of Bayesian statistics, with the resulting posterior probability measure being defined on a space of functions. We develop an abstract framework for such problems which facilitates application of an infinite-dimensional version of Bayes theorem, leads to a well-posedness result for the posterior measure (continuity in a suitable probability metric with respect to changes in data), and also leads to a theory for the existence of maximizing the posterior probability (MAP) estimators for such Bayesian inverse problems on function space. A central idea underlying these results is that continuity properties and bounds on the forward model guide the choice of the prior measure for the inverse problem, leading to the desired results on well-posedness and MAP estimators; the PDE analysis and probability theory required are thus clearly dileneated, allowing a straightforward derivation of results. We show that the abstract theory applies to some concrete applications of interest by studying problems arising from data assimilation in fluid mechanics. The objective is to make inference about the underlying velocity field, on the basis of either Eulerian or Lagrangian observations. We study problems without model error, in which case the inference is on the initial condition, and problems with model error in which case the inference is on the initial condition and on the driving noise process or, equivalently, on the entire time-dependent velocity field. In order to undertake a relatively uncluttered mathematical analysis we consider the two-dimensional Navier\textendash Stokes equation on a torus. The case of Eulerian observations\textemdash direct observations of the velocity field itself\textemdash is then a model for weather forecasting. The case of Lagrangian observations\textemdash observations of passive tracers advected by the flow\textemdash is then a model for data arising in oceanography. The methodology which we describe herein may be applied to many other inverse problems in which it is of interest to find, given observations, an infinite-dimensional object, such as the initial condition for a PDE. A similar approach might be adopted, for example, to determine an appropriate mathematical setting for the inverse problem of determining an unknown tensor arising in a constitutive law for a PDE, given observations of the solution. The paper is structured so that the abstract theory can be read independently of the particular problems in fluid mechanics which are subsequently studied by application of the theory.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/46UB4NM4/Cotter et al. - 2009 - Bayesian inverse problems for functions and applic.pdf}
}

@article{coxRegularityConvergenceAnalysis2020,
  title = {Regularity and Convergence Analysis in {{Sobolev}} and {{H}}\textbackslash "older Spaces for Generalized {{Whittle-Mat}}\textbackslash 'ern Fields},
  author = {Cox, Sonja G. and Kirchner, Kristin},
  year = {2020},
  month = dec,
  journal = {Numer. Math.},
  volume = {146},
  number = {4},
  eprint = {1904.06569},
  primaryclass = {math, stat},
  pages = {819--873},
  issn = {0029-599X, 0945-3245},
  doi = {10.1007/s00211-020-01151-x},
  abstract = {We analyze several Galerkin approximations of a Gaussian random field Z : D \texttimes{} {$\Omega$} \textrightarrow{} R indexed by a Euclidean domain D {$\subset$} Rd whose covariance structure is determined by a negative fractional power L-2{$\beta$} of a second-order elliptic differential operator L := -{$\nabla$} {$\cdot$} (A{$\nabla$}) + {$\kappa$}2. Under minimal assumptions on the domain D, the coefficients A : D \textrightarrow{} Rd\texttimes d, {$\kappa$} : D \textrightarrow{} R, and the fractional exponent {$\beta$} {$>$} 0, we prove convergence in Lq({$\Omega$}; H{$\sigma$}(D)) and in Lq({$\Omega$}; C{$\delta$}(D)) at (essentially) optimal rates for (i) spectral Galerkin methods and (ii) finite element approximations. Specifically, our analysis is solely based on H1+{$\alpha$}(D)-regularity of the differential operator L, where 0 {$<$} {$\alpha$} {$\leq$} 1. For this setting, we furthermore provide rigorous estimates for the error in the covariance function of these approximations in L{$\infty$}(D \texttimes{} D) and in the mixed Sobolev space H{$\sigma$},{$\sigma$}(D \texttimes{} D), showing convergence which is more than twice as fast compared to the corresponding Lq({$\Omega$}; H{$\sigma$}(D))-rate.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {35S15; 65C30; 65C60; 65N12; 65N30,Mathematics - Numerical Analysis,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/HWSXTXPT/Cox and Kirchner - 2020 - Regularity and convergence analysis in Sobolev and.pdf}
}

@article{coxRegularityConvergenceAnalysis2020a,
  title = {Regularity and Convergence Analysis in {{Sobolev}} and {{H\"older}} Spaces for Generalized {{Whittle}}\textendash{{Mat\'ern}} Fields},
  author = {Cox, Sonja G. and Kirchner, Kristin},
  year = {2020},
  month = dec,
  journal = {Numer. Math.},
  volume = {146},
  number = {4},
  pages = {819--873},
  issn = {0029-599X, 0945-3245},
  doi = {10.1007/s00211-020-01151-x},
  abstract = {We analyze several types of Galerkin approximations of a Gaussian random field Z : D \texttimes{$\Omega$} \textrightarrow{} R indexed by a Euclidean domain D {$\subset$} Rd whose covariance structure is determined by a negative fractional power L-2{$\beta$} of a second-order elliptic differential operator L := -{$\nabla$} {$\cdot$} (A{$\nabla$}) + {$\kappa$}2. Under minimal assumptions on the domain D, the coefficients A : D \textrightarrow{} Rd\texttimes d , {$\kappa$} : D \textrightarrow{} R, and the fractional exponent {$\beta$} {$>$} 0, we prove convergence in Lq ({$\Omega$}; H {$\sigma$} (D)) and in Lq ({$\Omega$}; C{$\delta$}(D)) at (essentially) optimal rates for (1) spectral Galerkin methods and (2) finite element approximations. Specifically, our analysis is solely based on H 1+{$\alpha$}(D)-regularity of the differential operator L, where 0 {$<$} {$\alpha$} {$\leq$} 1. For this setting, we furthermore provide rigorous estimates for the error in the covariance function of these approximations in L{$\infty$}(D \texttimes{} D) and in the mixed Sobolev space H {$\sigma$},{$\sigma$} (D \texttimes{} D), showing convergence which is more than twice as fast compared to the corresponding Lq ({$\Omega$}; H {$\sigma$} (D))-rate. We perform several numerical experiments which validate our theoretical results for (a) the original Whittle\textendash Mat\'ern class, where A {$\equiv$} IdRd and {$\kappa$} {$\equiv$} const., and (b) an example of anisotropic, nonstationary Gaussian random fields in d = 2 dimensions, where A : D \textrightarrow{} R2\texttimes 2 and {$\kappa$} : D \textrightarrow{} R are spatially varying.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/THXSCTRW/Cox and Kirchner - 2020 - Regularity and convergence analysis in Sobolev and.pdf}
}

@article{csajiApproximationArtificialNeural,
  title = {Approximation with {{Artificial Neural Networks}}},
  author = {Cs{\'a}ji, Bal{\'a}zs Csan{\'a}d},
  pages = {45},
  langid = {english},
  file = {/home/yanni/Zotero/storage/Q36Y5MJF/Cs√°ji - Approximation with Artificial Neural Networks.pdf}
}

@article{cuesta-albertosLowerBoundsTheL2Wasserstein1996a,
  title = {On Lower Bounds for {{theL2-Wasserstein}} Metric in a {{Hilbert}} Space},
  author = {{Cuesta-Albertos}, J. A. and {Matr{\'a}n-Bea}, C. and {Tuero-Diaz}, A.},
  year = {1996},
  month = apr,
  journal = {J Theor Probab},
  volume = {9},
  number = {2},
  pages = {263--283},
  issn = {1572-9230},
  doi = {10.1007/BF02214649},
  abstract = {We provide two families of lower bounds for theL2-Wasserstein metric in separable Hilbert spaces which depend on the basis chosen for the space. Then we focus on one of these families and we provide a necessary and sufficient condition for the supremum in it to be attained. In the finite dimensional case, we identify the basis which provides the most accurate lower bound in the family.},
  langid = {english},
  keywords = {Gaussian distributions,Hilbert spaces,L 2-Wasserstein metric,Lower bound},
  file = {/home/yanni/Zotero/storage/GNENLJVZ/Cuesta-Albertos et al. - 1996 - On lower bounds for theL2-Wasserstein metric in a .pdf}
}

@article{cuesta-albertosOptimalCouplingMultivariate1993,
  title = {Optimal Coupling of Multivariate Distributions and Stochastic Processes},
  author = {{Cuesta-Albertos}, J. A. and R{\"u}schendorf, L. and Tuerodiaz, A.},
  year = {1993},
  journal = {Journal of Multivariate Analysis},
  issn = {10957243},
  doi = {10.1006/jmva.1993.1064},
  abstract = {Same explicit optimal coupling results are derived with respect to minimal metrics of lp-type. In particular, the optimality of radial transformations, positive transformations, and monotone transformations of the components is established. \textcopyright{} 1993 Academic Press, Inc.},
  keywords = {Monotone transformations,Multivariate distributions,Optimal couplings,Positive transformations,Radial transformations,Wasserstein metrics}
}

@article{dafermosPartIIIDifferential,
  title = {Part {{III Differential Geometry Lecture Notes}}},
  author = {Dafermos, Mihalis},
  pages = {56},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GIE2ZYZW/Dafermos - Part III DiÔ¨Äerential Geometry Lecture Notes.pdf}
}

@article{daiOptimalBayesClassifiers2016,
  title = {Optimal {{Bayes Classifiers}} for {{Functional Data}} and {{Density Ratios}}},
  author = {Dai, Xiongtao and M{\"u}ller, Hans-Georg and Yao, Fang},
  year = {2016},
  month = may,
  journal = {arXiv:1605.03707 [math, stat]},
  eprint = {1605.03707},
  primaryclass = {math, stat},
  abstract = {Bayes classifiers for functional data pose a challenge. This is because probability density functions do not exist for functional data. As a consequence, the classical Bayes classifier using density quotients needs to be modified. We propose to use density ratios of projections on a sequence of eigenfunctions that are common to the groups to be classified. The density ratios can then be factored into density ratios of individual functional principal components whence the classification problem is reduced to a sequence of nonparametric one-dimensional density estimates. This is an extension to functional data of some of the very earliest nonparametric Bayes classifiers that were based on simple density ratios in the one-dimensional case. By means of the factorization of the density quotients the curse of dimensionality that would otherwise severely affect Bayes classifiers for functional data can be avoided. We demonstrate that in the case of Gaussian functional data, the proposed functional Bayes classifier reduces to a functional version of the classical quadratic discriminant. A study of the asymptotic behavior of the proposed classifiers in the large sample limit shows that under certain conditions the misclassification rate converges to zero, a phenomenon that has been referred to as ``perfect classification''. The proposed classifiers also perform favorably in finite sample applications, as we demonstrate in comparisons with other functional classifiers in simulations and various data applications, including wine spectral data, functional magnetic resonance imaging (fMRI) data for attention deficit hyperactivity disorder (ADHD) patients, and yeast gene expression data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62M20; 62C10; 62H30,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/GE6RAS99/Dai et al. - 2016 - Optimal Bayes Classifiers for Functional Data and .pdf}
}

@techreport{dashtiBayesianApproachInverse2015,
  title = {The {{Bayesian Approach}} to {{Inverse Prob-lems}}},
  author = {Dashti, Masoumeh and Stuart, Andrew M},
  year = {2015},
  abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications involving the blending of mathematical models with data. The finite dimensional situation is described first, along with some motivational examples. Then the development of probability measures on separable Banach space is undertaken, using a random series over an infinite set of functions to construct draws; these probability measures are used as priors in the Bayesian approach to inverse problems. Regularity of draws from the priors is studied in the natural Sobolev or Besov spaces implied by the choice of functions in the random series construction, and the Kolmogorov continuity theorem is used to extend regularity considerations to the space of H\"older continuous functions. Bayes' theorem is derived in this prior setting, and here interpreted as finding conditions under which the posterior is absolutely continuous with respect to the prior, and determining a formula for the Radon-Nikodym derivative in terms of the likelihood of the data. Having established the form of the posterior, we then describe various properties common to it in the infinite dimensional setting. These properties include well-posedness, approximation theory, and the existence of maximum a posteriori estimators. We then describe measure-preserving dynamics, again on the infinite dimensional space, including Markov chain-Monte Carlo and sequential Monte Carlo methods, and measure-preserving reversible stochastic differential equations. By formulating the theory and algorithms on the underlying infinite dimensional space, we obtain a framework suitable for rigorous analysis of the accuracy of reconstructions, of computational complexity, as well as naturally constructing algorithms which perform well under mesh refinement, since they are inherently well-defined in infinite dimensions.},
  keywords = {()},
  file = {/home/yanni/Zotero/storage/GQQKDXUN/Dashti, Stuart - 2015 - The Bayesian Approach to Inverse Prob-lems.pdf}
}

@incollection{dashtiBayesianApproachInverse2017,
  title = {The {{Bayesian Approach}} to {{Inverse Problems}}},
  booktitle = {Handbook of {{Uncertainty Quantification}}},
  author = {Dashti, Masoumeh and Stuart, Andrew M.},
  editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
  year = {2017},
  pages = {311--428},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-12385-1_7},
  abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications involving the blending of mathematical models with data. The finite-dimensional situation is described first, along with some motivational examples. Then the development of probability measures on separable Banach space is undertaken, using a random series over an infinite set of functions to construct draws; these probability measures are used as priors in the Bayesian approach to inverse problems. Regularity of draws from the priors is studied in the natural Sobolev or Besov spaces implied by the choice of functions in the random series construction, and the Kolmogorov continuity theorem is used to extend regularity considerations to the space of H\"older continuous functions. Bayes' theorem is derived in this prior setting, and here interpreted as finding conditions under which the posterior is absolutely continuous with respect to the prior, and determining a formula for the Radon-Nikodym derivative in terms of the likelihood of the data. Having established the form of the posterior, we then describe various properties common to it in the infinite-dimensional setting. These properties include well-posedness, approximation theory, and the existence of maximum a posteriori estimators. We then describe measure-preserving dynamics, again on the infinite-dimensional space, including Markov chain Monte Carlo and sequential Monte Carlo methods, and measure-preserving reversible stochastic differential equations. By formulating the theory and algorithms on the underlying infinite-dimensional space, we obtain a framework suitable for rigorous analysis of the accuracy of reconstructions, of computational complexity, as well as naturally constructing algorithms which perform well under mesh refinement, since they are inherently well defined in infinite dimensions.},
  isbn = {978-3-319-12385-1},
  langid = {english},
  keywords = {Bayesian inversion,Inverse problems,Langevin stochastic partial differential equations,Markov chain Monte Carlo,Sequential Monte Carlo,Tikhonov regularization and MAP estimators},
  file = {/home/yanni/Zotero/storage/ISBWC98B/Dashti and Stuart - 2017 - The Bayesian Approach to Inverse Problems.pdf}
}

@techreport{demachiReflexiveSpaceRelation,
  title = {Reflexive Space and Relation between Norm and Inner Product},
  author = {Demachi, Yuki},
  abstract = {Reflexive space and relation between norm and inner product 321601218 Yuki Demachi Theorem 1 Let X be a Banach Space.Then X is reflexive if and only if X* is reflect.},
  file = {/home/yanni/Zotero/storage/QFYVJDU3/Unknown - Unknown - (No Title).pdf}
}

@book{demengelFunctionalSpacesTheory2012,
  title = {Functional {{Spaces}} for the {{Theory}} of {{Elliptic Partial Differential Equations}}},
  author = {Demengel, Fran{\c c}oise and Demengel, Gilbert},
  year = {2012},
  series = {Universitext},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-2807-6},
  isbn = {978-1-4471-2806-9 978-1-4471-2807-6},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GLVBFWUA/Demengel and Demengel - 2012 - Functional Spaces for the Theory of Elliptic Parti.pdf}
}

@article{deryckChangePointDetection2021,
  title = {Change {{Point Detection}} in {{Time Series Data}} Using {{Autoencoders}} with a {{Time-Invariant Representation}}},
  author = {De Ryck, Tim and De Vos, Maarten and Bertrand, Alexander},
  year = {2021},
  journal = {IEEE Trans. Signal Process.},
  volume = {69},
  eprint = {2008.09524},
  pages = {3513--3524},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2021.3087031},
  abstract = {Change point detection (CPD) aims to locate abrupt property changes in time series data. Recent CPD methods demonstrated the potential of using deep learning techniques, but often lack the ability to identify more subtle changes in the autocorrelation statistics of the signal and suffer from a high false alarm rate. To address these issues, we employ an autoencoderbased methodology with a novel loss function, through which the used autoencoders learn a partially time-invariant representation that is tailored for CPD. The result is a flexible method that allows the user to indicate whether change points should be sought in the time domain, frequency domain or both. Detectable change points include abrupt changes in the slope, mean, variance, autocorrelation function and frequency spectrum. We demonstrate that our proposed method is consistently highly competitive or superior to baseline methods on diverse simulated and real-life benchmark data sets. Finally, we mitigate the issue of false detection alarms through the use of a postprocessing procedure that combines a matched filter and a newly proposed change point score. We show that this combination drastically improves the performance of our method as well as all baseline methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/8BGFKP69/De Ryck et al. - 2021 - Change Point Detection in Time Series Data using A.pdf}
}

@article{desobryOnlineKernelChange2005,
  title = {An Online Kernel Change Detection Algorithm},
  author = {Desobry, F. and Davy, M. and Doncarli, C.},
  year = {2005},
  month = aug,
  journal = {IEEE Trans. Signal Process.},
  volume = {53},
  number = {8},
  pages = {2961--2974},
  issn = {1053-587X},
  doi = {10.1109/TSP.2005.851098},
  abstract = {A number of abrupt change detection methods have been proposed in the past, among which are efficient modelbased techniques such as the Generalized Likelihood Ratio (GLR) test. We consider the case where no accurate nor tractable model can be found, using a model-free approach, called Kernel change detection (KCD). KCD compares two sets of descriptors extracted online from the signal at each time instant: the immediate past set and the immediate future set. Based on the soft margin single-class Support Vector Machine (SVM), we build a dissimilarity measure in feature space between those sets, without estimating densities as an intermediary step. This dissimilarity measure is shown to be asymptotically equivalent to the Fisher ratio in the Gaussian case. Implementation issues are addressed, in particular, the dissimilarity measure can be computed online in input space. Simulation results on both synthetic signals and real music signals show the efficiency of KCD.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/4IINKQYF/Desobry et al. - 2005 - An online kernel change detection algorithm.pdf}
}

@techreport{desoerNotePseudoinverses1963,
  title = {A {{Note}} on {{Pseudoinverses}}},
  author = {Desoer, C A and Whalen, B H},
  year = {1963},
  journal = {Source: Journal of the Society for Industrial and Applied Mathematics},
  volume = {11},
  number = {2},
  pages = {442--447},
  file = {/home/yanni/Zotero/storage/85MNCTPV/Desoer, Whalen - 1963 - A Note on Pseudoinverses.pdf}
}

@article{diaconisBayesianNumericalAnalysis,
  title = {Bayesian Numerical Analysis},
  author = {{Diaconis}},
  file = {/home/yanni/Zotero/storage/K79TPJXI/Diaconis - Unknown - Bayesian numerical analysis.pdf}
}

@article{dickinsonSensitivityAnalysisOrdinary1976,
  title = {Sensitivity Analysis of Ordinary Differential Equation Systems\textemdash a Direct Method},
  author = {Dickinson, Robert P and Gelinas, Robert J},
  year = {1976},
  journal = {Journal of computational physics},
  volume = {21},
  number = {2},
  pages = {123--143},
  publisher = {{Elsevier}}
}

@article{dickinsonSensitivityAnalysisOrdinary1976a,
  title = {Sensitivity Analysis of Ordinary Differential Equation Systems-{{A}} Direct Method},
  author = {Dickinson, Robert P. and Gelinas, Robert J.},
  year = {1976},
  journal = {Journal of Computational Physics},
  issn = {10902716},
  doi = {10.1016/0021-9991(76)90007-3},
  abstract = {Given a system of time dependent ordinary differential equations, dot yi = fi(c1, c2, ..., y1, y2, ..., t), where ck are rate parameters, we simultaneously solve for both yi and a set of sensitivity functions, {$\rho$}variantyi {$\rho$}variantck, over all times t. These partial derivatives measure the sensitivity of the solution with respect to changes in the parameters ck. Often these parameters are not accurately known. An example is given from atmospheric chemical kinetics using constant as well as time varying (diurnal) rate parameters. For the purposes of this paper, our calculations considered both first- and second-order contributions to {$\Delta$}y with respect to {$\Delta$}c. It is found that second-order sensitivity terms can be highly significant, but tend to be too costly for present widespread application. \textcopyright{} 1976.},
  file = {/home/yanni/Zotero/storage/KC6GDHYZ/Dickinson, Gelinas - 1976 - Sensitivity analysis of ordinary differential equation systems-A direct method.pdf}
}

@article{dinezzaHitchhikersGuideFractional2012,
  title = {Hitchhiker's Guide to the Fractional {{Sobolev}} Spaces},
  author = {Di Nezza, Eleonora and Palatucci, Giampiero and Valdinoci, Enrico},
  year = {2012},
  month = jul,
  journal = {Bulletin des Sciences Math\'ematiques},
  volume = {136},
  number = {5},
  pages = {521--573},
  issn = {0007-4497},
  doi = {10.1016/j.bulsci.2011.12.004},
  abstract = {This paper deals with the fractional Sobolev spaces Ws,p. We analyze the relations among some of their possible definitions and their role in the trace theory. We prove continuous and compact embeddings, investigating the problem of the extension domains and other regularity results. Most of the results we present here are probably well known to the experts, but we believe that our proofs are original and we do not make use of any interpolation techniques nor pass through the theory of Besov spaces. We also present some counterexamples in non-Lipschitz domains.},
  langid = {english},
  keywords = {Fractional Laplacian,Fractional Sobolev spaces,Gagliardo norm,Nonlocal energy,Sobolev embeddings},
  file = {/home/yanni/Zotero/storage/HJYL57Y2/Di Nezza et al. - 2012 - Hitchhiker ºs guide to the fractional Sobolev space.pdf;/home/yanni/Zotero/storage/6HJ846UH/S0007449711001254.html}
}

@article{dixonMultivariateGaussianProcess,
  title = {Multivariate {{Gaussian Process Regression}} for {{Portfolio Risk Modeling}}: {{Application}} to {{CVA}}},
  author = {Dixon, Matthew F},
  pages = {22},
  abstract = {Modeling counterparty risk is computationally challenging because it requires the simultaneous evaluation of all the trades with each counterparty under both market and credit risk. We present a multi-Gaussian process regression for estimating portfolio risk, which is well suited for OTC derivative portfolios, in particular CVA computation. Our spatiotemporal modeling approach avoids nested MC simulation by learning a 'kernel pricing layer'. The pricing layer is flexible - we model the joint posterior of the derivatives as a Gaussian over function space, with the spatial covariance structure imposed only on the risk factors. Monte-Carlo (MC) simulation is then used to simulate the dynamics of the risk factors. Our approach quantifies uncertainty in portfolio risk arising from uncertainty in point estimates. Numerical experiments demonstrate the accuracy and convergence properties of our approach for CVA estimation.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/LAIC6GHV/Dixon - Multivariate Gaussian Process Regression for Portf.pdf}
}

@article{dixonMultivariateGaussianProcessa,
  title = {Multivariate {{Gaussian Process Regression}} for {{Portfolio Risk Modeling}}: {{Application}} to {{CVA}}},
  author = {Dixon, Matthew F},
  abstract = {Modeling counterparty risk is computationally challenging because it requires the simultaneous evaluation of all the trades with each counterparty under both market and credit risk. We present a multi-Gaussian process regression for estimating portfolio risk, which is well suited for OTC derivative portfolios, in particular CVA computation. Our spatiotemporal modeling approach avoids nested MC simulation by learning a 'kernel pricing layer'. The pricing layer is flexible - we model the joint posterior of the derivatives as a Gaussian over function space, with the spatial covariance structure imposed only on the risk factors. Monte-Carlo (MC) simulation is then used to simulate the dynamics of the risk factors. Our approach quantifies uncertainty in portfolio risk arising from uncertainty in point estimates. Numerical experiments demonstrate the accuracy and convergence properties of our approach for CVA estimation.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/EWVGNNU4/Dixon - Multivariate Gaussian Process Regression for Portf.pdf}
}

@article{donnetEstimationParametersIncomplete2007,
  title = {Estimation of Parameters in Incomplete Data Models Defined by Dynamical Systems},
  author = {Donnet, Sophie and Samson, Adeline},
  year = {2007},
  journal = {Journal of Statistical Planning and Inference},
  issn = {03783758},
  doi = {10.1016/j.jspi.2006.10.013},
  abstract = {Parametric incomplete data models defined by ordinary differential equations (ODEs) are widely used in biostatistics to describe biological processes accurately. Their parameters are estimated on approximate models, whose regression functions are evaluated by a numerical integration method. Accurate and efficient estimations of these parameters are critical issues. This paper proposes parameter estimation methods involving either a stochastic approximation EM algorithm (SAEM) in the maximum likelihood estimation, or a Gibbs sampler in the Bayesian approach. Both algorithms involve the simulation of non-observed data with conditional distributions using Hastings-Metropolis (H-M) algorithms. A modified H-M algorithm, including an original local linearization scheme to solve the ODEs, is proposed to reduce the computational time significantly. The convergence on the approximate model of all these algorithms is proved. The errors induced by the numerical solving method on the conditional distribution, the likelihood and the posterior distribution are bounded. The Bayesian and maximum likelihood estimation methods are illustrated on a simulated pharmacokinetic nonlinear mixed-effects model defined by an ODE. Simulation results illustrate the ability of these algorithms to provide accurate estimates. \textcopyright{} 2007 Elsevier B.V. All rights reserved.},
  keywords = {Bayesian estimation,Incomplete data model,Local linearization scheme,MCMC algorithm,Nonlinear mixed-effects model,ODE integration,SAEM algorithm},
  file = {/home/yanni/Zotero/storage/AMRPVWB4/Donnet, Samson - 2007 - Estimation of parameters in incomplete data models defined by dynamical systems.pdf}
}

@article{donnetParametricInferenceMixed2008,
  title = {Parametric Inference for Mixed Models Defined by Stochastic Differential Equations},
  author = {Donnet, Sophie and Samson, Adeline},
  year = {2008},
  journal = {ESAIM: Probability and Statistics},
  volume = {12},
  pages = {196--218},
  publisher = {{EDP Sciences}}
}

@techreport{downeyModelingSimulationPython2017,
  title = {Modeling and {{Simulation}} in {{Python}}},
  author = {Downey, Allen B},
  year = {2017},
  file = {/home/yanni/Zotero/storage/G84QDMTQ/Downey - 2017 - Modeling and Simulation in Python.pdf}
}

@article{dowsonFrechetDistanceMultivariate1982,
  title = {The {{Fr\'echet}} Distance between Multivariate Normal Distributions},
  author = {Dowson, D. C and Landau, B. V},
  year = {1982},
  month = sep,
  journal = {Journal of Multivariate Analysis},
  volume = {12},
  number = {3},
  pages = {450--455},
  issn = {0047-259X},
  doi = {10.1016/0047-259X(82)90077-X},
  abstract = {The Fr\'echet distance between two multivariate normal distributions having means {$\mu$}X, {$\mu$}Y and covariance matrices {$\Sigma$}X, {$\Sigma$}Y is shown to be given by d2 = |{$\mu$}X - {$\mu$}Y|2 + tr({$\Sigma$}X + {$\Sigma$}Y - 2({$\Sigma$}X{$\Sigma$}Y)12). The quantity d0 given by d02 = tr({$\Sigma$}X + {$\Sigma$}Y - 2({$\Sigma$}X{$\Sigma$}Y)12) is a natural metric on the space of real covariance matrices of given order.},
  langid = {english},
  keywords = {covariance matrices,Fr√©chet distance,multivariate normal distributions},
  file = {/home/yanni/Zotero/storage/KEKNWKMG/Dowson and Landau - 1982 - The Fr√©chet distance between multivariate normal d.pdf;/home/yanni/Zotero/storage/6KGN2E4T/0047259X8290077X.html}
}

@article{drydenNONEUCLIDEANSTATISTICSCOVARIANCE2009,
  title = {{{NON-EUCLIDEAN STATISTICS FOR COVARIANCE MATRICES}}, {{WITH APPLICATIONS TO DIFFUSION TENSOR IMAGING}} 1},
  author = {Dryden, Ian L and Koloydenko, Alexey and Zhou, Diwei},
  year = {2009},
  journal = {The Annals of Applied Statistics},
  volume = {3},
  number = {3},
  pages = {1102--1123},
  doi = {10.1214/09-AOAS249},
  abstract = {The statistical analysis of covariance matrix data is considered and, in particular, methodology is discussed which takes into account the non-Euclidean nature of the space of positive semi-definite symmetric matrices. The main motivation for the work is the analysis of diffusion tensors in medical image analysis. The primary focus is on estimation of a mean covariance matrix and, in particular, on the use of Procrustes size-and-shape space. Comparisons are made with other estimation techniques, including using the matrix logarithm, matrix square root and Cholesky decomposition. Applications to diffusion tensor imaging are considered and, in particular, a new measure of fractional anisotropy called Procrustes Anisotropy is discussed.},
  keywords = {Anisotropy,Cholesky,geodesic,matrix logarithm,principal components,Procrustes,Riemannian,shape,size,Wishart},
  file = {/home/yanni/Zotero/storage/VP2MTJAU/Dryden, Koloydenko, Zhou - 2009 - NON-EUCLIDEAN STATISTICS FOR COVARIANCE MATRICES, WITH APPLICATIONS TO DIFFUSION TENSOR IMAGING 1.pdf}
}

@article{duffinStatisticalFiniteElements2020,
  title = {Statistical Finite Elements for Misspecified Models},
  author = {Duffin, Connor and Cripps, Edward and Stemler, Thomas and Girolami, Mark},
  year = {2020},
  doi = {10.1073/pnas.2015006118/-/DCSupplemental.y},
  abstract = {We present a statistical finite element method for nonlinear, time-dependent phenomena, illustrated in the context of nonlinear internal waves (solitons). We take a Bayesian approach and leverage the finite element method to cast the statistical problem as a nonlinear Gaussian state-space model, updating the solution, in receipt of data, in a filtering framework. The method is applicable to problems across science and engineering for which finite element methods are appropriate. The Korteweg-de Vries equation for solitons is presented because it reflects the necessary complexity while being suitably familiar and succinct for peda-gogical purposes. We present two algorithms to implement this method, based on the extended and ensemble Kalman filters, and demonstrate effectiveness with a simulation study and a case study with experimental data. The generality of our approach is demonstrated in SI Appendix, where we present examples from additional nonlinear, time-dependent partial differential equations (Burgers equation, Kuramoto-Sivashinsky equation). Bayesian calibration | finite element methods | model discrepancy T he central role of physically derived, nonlinear, time-dependent partial differential equations (PDEs) in scientific and engineering research is undisputed, as is the need for numerical intervention in order to understand their behavior. The finite element method (FEM) has emerged as the foremost strategy to undergo this numerical intervention, yet when these discretized solutions are compared with empirical evidence, elements of model mismatch are revealed that require statistical formalisms to be dealt with appropriately (1-3). To address this problem of model misspecification, in this paper we introduce stochas-tic forcing inside the PDE and update the FEM discretized PDE solution with data in a filtering context. Stochastic forcing is introduced through a random function within the governing equations. This represents an unknown process , which may have been omitted in the formulation of the physical model. For an elliptic linear PDE with coefficients {$\Lambda$}, this can be expressed as L{$\Lambda$}u = f + {$\xi$} \$\th eta\$ , {$\xi$} \$\th eta\$ {$\sim$} GP(0, C \$\th eta\$), u := u(x), f := f (x), x {$\in$} {$\Omega$} {$\subset$} R d. The push forward of the Gaussian random field {$\xi$} \$\th eta\$ , with covari-ance parameters \$\th eta\$, induces a probability measure over the space of admissible solutions to the above. To embed this into a finite element model, we start with the weak form A{$\Lambda$}(u, v) = f , v , +{$\xi$} \$\th eta\$ , v , where A{$\Lambda$}({$\cdot$}, {$\cdot$}) is the bilinear form generated from L{$\Lambda$} and {$\cdot$}, {$\cdot\cdot$} is the appropriate Hilbert space inner product. Discretizing with finite elements u(x) = M i=1 ui {$\phi$}i (x), v (x) = M i=1 vi {$\phi$}i (x) yields the Gaussian measure over the solution FEM coefficients u = (u1, u2,. .. , uM) {$\in$} R M : p(u | {$\Lambda$}, \$\th eta\$) = N (A -1 b, A -1 G(\$\th eta\$)A -T), where Aij = A{$\Lambda$}({$\phi$}i , {$\phi$}j), bi = f , {$\phi$}i , and G(\$\th eta\$)ij = {$\phi$}i , C \$\th eta\$ {$\phi$}j. This defines a (finite-dimensional) prior distribution over the FEM model, which represents all assumed knowledge before observing data. The mean is the standard Galerkin solution, and the covariance results from the action of the discretized PDE operator on the covariance G(\$\th eta\$); further details are contained in SI Appendix, section 1. This was first developed in ref. 4, and we demonstrate the generality of such an approach by extending it to nonlinear, time-dependent PDEs. An area in which nonlinear and time-dependent problems are ubiquitous is ocean dynamic processes, where essentially all problems stem from a governing system of nonlinear, time-dependent equations (e.g., the Navier-Stokes equations). The ocean dynamics community has grown increasingly cognizant of the importance of accurate uncertainty quantification (5, 6), with many possible applications [e.g., rogue waves (7), turbulent flow (8)] for our proposed methodology. An example process is nonlinear internal waves (solitons), which are observed as waves of depression or elevation along a pycnocline in a density-stratified fluid and are of broad interest to both the scientific and engineering communities (9-13). The classical mathematical model for solitons is the Korteweg-de Vries (KdV) equation (14): ut + {$\alpha$}uux + {$\beta$}uxxx + cux = 0, u := u(x , t), x {$\in$} [0, L], t {$\in$} [0, T ], [1] where u is the pycnocline displacement. Coefficients {$\alpha$}, {$\beta$}, and c are determined by physical parameters. Eq. 1 is readily inter-pretable: waves propagate at wave speed c, nonlinear steepening results from uux , and dispersion is due to uxxx. Relative coefficient values determine the dominating regime, and waves can vary from quasilinear to highly nonlinear. Significance Science and engineering have benefited greatly from the ability of finite element methods (FEMs) to simulate nonlinear, time-dependent complex systems. The recent advent of extensive data collection from such complex systems now raises the question of how to systematically incorporate these data into finite element models, consistently updating the solution in the face of mathematical model misspecification with physical reality. This article describes general and widely applicable methodology for the coherent synthesis of data with FEM models, providing a data-driven probability distribution that captures all sources of uncertainty in the pairing of FEM with measurements.},
  file = {/home/yanni/Zotero/storage/6DADN8Y3/Duffin et al. - 2020 - Statistical finite elements for misspecified models.pdf}
}

@article{duffinSupplementaryInfoStatistical2021,
  title = {Supplementary {{Info}} for {{Statistical}} Finite Elements for Misspecified Models},
  author = {Duffin, Connor and Cripps, Edward and Stemler, Thomas and Girolami, Mark},
  year = {2021},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {118},
  number = {2},
  pages = {1--15},
  issn = {10916490},
  doi = {10.1073/pnas.2015006118},
  abstract = {We present a statistical finite element method for nonlinear, time-dependent phenomena, illustrated in the context of nonlinear internal waves (solitons). We take a Bayesian approach and leverage the finite element method to cast the statistical problem as a nonlinear Gaussian state-space model, updating the solution, in receipt of data, in a filtering framework. The method is applicable to problems across science and engineering for which finite element methods are appropriate. The Korteweg-de Vries equation for solitons is presented because it reflects the necessary complexity while being suitably familiar and succinct for pedagogical purposes. We present two algorithms to implement this method, based on the extended and ensemble Kalman filters, and demonstrate effectiveness with a simulation study and a case study with experimental data. The generality of our approach is demonstrated in SI Appendix, where we present examples from additional nonlinear, time-dependent partial differential equations (Burgers equation, Kuramoto-Sivashinsky equation).},
  pmid = {33372139},
  keywords = {Bayesian calibration,Finite element methods,Model discrepancy},
  file = {/home/yanni/Zotero/storage/XA729KN6/pnas.2015006118.sapp.pdf}
}

@article{duncanComputationalStochasticProcesses2017,
  title = {Computational {{Stochastic Processes}}},
  author = {Duncan, Andrew B and Pavliotis, Grigorios A},
  year = {2017},
  file = {/home/yanni/Zotero/storage/5696R3XK/Duncan, Pavliotis - 2017 - Computational Stochastic Processes.pdf}
}

@incollection{eckleyAnalysisChangepointModels2011,
  title = {Analysis of Changepoint Models},
  booktitle = {Bayesian {{Time Series Models}}},
  author = {Eckley, Idris A. and Fearnhead, Paul and Killick, Rebecca},
  editor = {Barber, David and Cemgil, A. Taylan and Chiappa, Silvia},
  year = {2011},
  pages = {205--224},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511984679.011},
  isbn = {978-0-511-98467-9},
  langid = {english},
  file = {/home/yanni/Zotero/storage/N5BZW755/Eckley et al. - 2011 - Analysis of changepoint models.pdf}
}

@incollection{eckleyAnalysisChangepointModels2011a,
  title = {Analysis of Changepoint Models},
  booktitle = {Bayesian {{Time Series Models}}},
  author = {Eckley, Idris A. and Fearnhead, Paul and Killick, Rebecca},
  editor = {Barber, David and Cemgil, A. Taylan and Chiappa, Silvia},
  year = {2011},
  pages = {205--224},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511984679.011},
  isbn = {978-0-511-98467-9},
  langid = {english},
  file = {/home/yanni/Zotero/storage/65C5NR7M/Eckley et al. - 2011 - Analysis of changepoint models.pdf}
}

@article{edelmanGeometryAlgorithmsOrthogonality1998,
  title = {The {{Geometry}} of {{Algorithms}} with {{Orthogonality Constraints}}},
  author = {Edelman, Alan and Arias, T. A. and Smith, Steven T.},
  year = {1998},
  month = jun,
  journal = {arXiv:physics/9806030},
  eprint = {physics/9806030},
  abstract = {In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will benefit from the theory, methods, and examples in this paper.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter,Mathematics - Numerical Analysis,Physics - Chemical Physics,Physics - Computational Physics},
  file = {/home/yanni/Zotero/storage/NQJBWIRI/Edelman et al. - 1998 - The Geometry of Algorithms with Orthogonality Cons.pdf;/home/yanni/Zotero/storage/NG9H35YC/9806030.html}
}

@article{edelmanGeometryAlgorithmsOrthogonality1998a,
  title = {The {{Geometry}} of {{Algorithms}} with {{Orthogonality Constraints}}},
  author = {Edelman, Alan and Arias, Tom{\'a}s A. and Smith, Steven T.},
  year = {1998},
  month = jan,
  journal = {SIAM J. Matrix Anal. \& Appl.},
  volume = {20},
  number = {2},
  pages = {303--353},
  issn = {0895-4798, 1095-7162},
  doi = {10.1137/S0895479895290954},
  abstract = {In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will benefit from the theory, methods, and examples in this paper.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/P8WNLJB6/Lecture Notes 2020.pdf;/home/yanni/Zotero/storage/SYV5BIYB/Edelman et al. - 1998 - The Geometry of Algorithms with Orthogonality Cons.pdf}
}

@article{eiterComputingDiscreteFr,
  title = {Computing {{Discrete Fr}}\textasciiacute echet {{Distance}}},
  author = {Eiter, Thomas and Mannila, Heikki},
  pages = {8},
  abstract = {The Fr\textasciiacute echet distance between two curves in a metric space is a measure of the similarity between the curves. We present a discrete variation of this measure. It provides good approximations of the continuous measure and can be efficiently computed using a simple algorithm. We also consider variants of discrete Fr\textasciiacute echet distance, and find an interesting connection to measuring distance between theories.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/JIIANDFX/Eiter and Mannila - Computing Discrete Fr¬¥echet Distance.pdf}
}

@techreport{eldredgeAnalysisProbabilityInfiniteDimensional2016,
  title = {Analysis and {{Probability}} on {{Infinite-Dimensional Spaces}}},
  author = {Eldredge, Nathaniel},
  year = {2016},
  keywords = {()},
  file = {/home/yanni/Zotero/storage/EX8BNQDY/Eldredge - 2016 - Analysis and Probability on Infinite-Dimensional Spaces(2).pdf}
}

@misc{ElsevierEnhancedReader,
  title = {Elsevier {{Enhanced Reader}}},
  doi = {10.1016/j.cma.2017.07.038},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0045782516305114?token=EF2A5F35883781025B89908BE232D3F8AABACD2B5888E022C8A405A02B324E7A03C9BC8933EC898E0B9360BE9DB01158\&originRegion=eu-west-1\&originCreation=20211215083046},
  langid = {english},
  file = {/home/yanni/Zotero/storage/6M3EFA3X/Elsevier Enhanced Reader.pdf;/home/yanni/Zotero/storage/ZT7PK8GB/S0045782516305114.html}
}

@article{etal.ParameterEstimationContinuoustime2006,
  title = {Parameter Estimation in Continuous-Time Dynamic Models Using Principal Differential Analysis},
  author = {{Et al.}, A A Poyton},
  year = {2006},
  journal = {Computers \& Chemical Engineering},
  volume = {30},
  pages = {698--708}
}

@book{evansPartialDifferentialEquations2010,
  title = {Partial {{Differential Equations}}},
  author = {Evans, Lawrence C.},
  year = {2010},
  publisher = {{American Mathematical Soc.}},
  abstract = {This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, a significantly expanded bibliography.  About the First Edition: I have used this book for both regular PDE and topics courses. It has a wonderful combination of insight and technical detail...Evans' book is evidence of his mastering of the field and the clarity of presentation (Luis Caffarelli, University of Texas) It is fun to teach from Evans' book. It explains many of the essential ideas and techniques of partial differential equations ...Every graduate student in analysis should read it. (David Jerison, MIT) I use Partial Differential Equations to prepare my students for their Topic exam, which is a requirement before starting working on their dissertation. The book provides an excellent account of PDE's ...I am very happy with the preparation it provides my students. (Carlos Kenig, University of Chicago) Evans' book has already attained the status of a classic. It is a clear choice for students just learning the subject, as well as for experts who wish to broaden their knowledge ...An outstanding reference for many aspects of the field. (Rafe Mazzeo, Stanford University.},
  googlebooks = {Xnu0o\_EJrCQC},
  isbn = {978-0-8218-4974-3},
  langid = {english},
  keywords = {Mathematics / Differential Equations / General}
}

@misc{ExploringRegressionStructure,
  title = {Exploring {{Regression Structure Using Nonparametric Functional Estimation}}},
  issn = {0162-1459},
  howpublished = {https://www.tandfonline.com/doi/epdf/10.1080/01621459.1993.10476348?needAccess=true},
  langid = {english},
  file = {/home/yanni/Zotero/storage/P29DRHDY/01621459.1993.html}
}

@article{fearnheadExactEfficientBayesian2006,
  title = {Exact and Efficient {{Bayesian}} Inference for Multiple Changepoint Problems},
  author = {Fearnhead, Paul},
  year = {2006},
  month = jun,
  journal = {Stat Comput},
  volume = {16},
  number = {2},
  pages = {203--213},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-006-8450-8},
  abstract = {We demonstrate how to perform direct simulation from the posterior distribution of a class of multiple changepoint models where the number of changepoints is unknown. The class of models assumes independence between the posterior distribution of the parameters associated with segments of data between successive changepoints. This approach is based on the use of recursions, and is related to work on product partition models. The computational complexity of the approach is quadratic in the number of observations, but an approximate version, which introduces negligible error, and whose computational cost is roughly linear in the number of observations, is also possible. Our approach can be useful, for example within an MCMC algorithm, even when the independence assumptions do not hold. We demonstrate our approach on coal-mining disaster data and on well-log data. Our method can cope with a range of models, and exact simulation from the posterior distribution is possible in a matter of minutes.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/QW6KWT7F/Fearnhead - 2006 - Exact and efficient Bayesian inference for multipl.pdf}
}

@article{fearnheadOnlineInferenceMultiple,
  title = {On-Line {{Inference}} for {{Multiple Change Points Problems}}},
  author = {Fearnhead, Paul and Liu, Zhen},
  pages = {26},
  abstract = {We propose an on-line algorithm for exact filtering of multiple changepoint problems. This algorithm enables simulation from the true joint posterior distribution of the number and position of the changepoints for a class of changepoint models. The computational cost of this exact algorithm is quadratic in the number of observations. We further show how resampling ideas from particle filters can be used to reduce the computational cost to linear in the number of observations, at the expense of introducing small errors; and propose two new, optimum resampling algorithms for this problem. One, a version of rejection control, allows the particle filter to automatically choose the number of particles required at each time-step. The new resampling algorithms substantially out-perform standard resampling algorithms on examples we consider; and we demonstrate how the resulting particle filter is practicable for segmentation of human GC content.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/D49SR8H3/Fearnhead and Liu - On-line Inference for Multiple Change Points Probl.pdf}
}

@article{flamaryPOTPythonOptimal,
  title = {{{POT}}: {{Python Optimal Transport}}},
  author = {Flamary, Remi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z and Boisbunon, Aurelie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and Gautheron, Leo and Gayraud, Nathalie T H and Janati, Hicham and Rakotomamonjy, Alain and Redko, Ievgen and Rolet, Antoine and Schutz, Antony and Seguy, Vivien and Sutherland, Danica J and Tavenard, Romain and Tong, Alexander and Vayer, Titouan},
  pages = {8},
  abstract = {Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/UAXANFL4/Flamary et al. - POT Python Optimal Transport.pdf}
}

@techreport{flaxmanBayesianLearningKernel,
  title = {Bayesian {{Learning}} of {{Kernel Embeddings}}},
  author = {Flaxman, Seth and Sejdinovic, Dino and Cunningham, John P and Filippi, Sarah},
  abstract = {Kernel methods are one of the mainstays of machine learning, but the problem of kernel learning remains challenging, with only a few heuris-tics and very little theory. This is of particular importance in methods based on estimation of kernel mean embeddings of probability measures. For characteristic kernels, which include most commonly used ones, the kernel mean embedding uniquely determines its probability measure , so it can be used to design a powerful statistical testing framework, which includes non-parametric two-sample and independence tests. In practice, however, the performance of these tests can be very sensitive to the choice of kernel and its lengthscale parameters. To address this central issue, we propose a new probabilistic model for kernel mean embeddings, the Bayesian Kernel Embedding model, combining a Gaus-sian process prior over the Reproducing Kernel Hilbert Space containing the mean embedding with a conjugate likelihood function, thus yielding a closed form posterior over the mean embedding. The posterior mean of our model is closely related to recently proposed shrinkage es-timators for kernel mean embeddings, while the posterior uncertainty is a new, interesting feature with various possible applications. Critically for the purposes of kernel learning, our model gives a simple, closed form marginal pseudolikelihood of the observed data given the kernel hyperpa-rameters. This marginal pseudolikelihood can either be optimized to inform the hyperparameter choice or fully Bayesian inference can be used.},
  file = {/home/yanni/Zotero/storage/IV7NHCKM/Flaxman et al. - Unknown - Bayesian Learning of Kernel Embeddings.pdf}
}

@article{flaxmanFastHierarchicalGaussian,
  title = {Fast Hierarchical {{Gaussian}} Processes},
  author = {Flaxman, Seth and Gelman, Andrew and Neill, Daniel and Smola, Alex and Vehtari, Aki and Wilson, Andrew Gordon},
  pages = {18},
  abstract = {While the framework of Gaussian process priors for functions is very flexible and has a number of advantages, its use within a fully Bayesian hierarchical modeling framework has been limited due to computational constraints. Most often, simple models are fit, with hyperparameters learned by maximum likelihood. But this approach understates the posterior uncertainty in inference. We consider priors over kernel hyperparameters, thus inducing a very flexible Bayesian hierarchical modeling framework in which we perform inference using MCMC not just for the posterior function but also for the posterior over the kernel hyperparameters. We address the central challenge of computational efficiency with MCMC by exploiting separable structure in the covariance matrix corresponding to the kernel, yielding significant gains in time and memory efficiency. Our method can be conveniently implemented in a probabilistic programming language (Stan), is widely applicable to any setting involving structured kernels, and immediately enables a number of useful features, including kernel learning through novel prior specifications, learning nonparametric priors over categorical variables, clustering through a factor analysis approach, and missing observations. We demonstrate our methods on real and synthetic spatiotemporal datasets.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/PJ23XFU6/Flaxman et al. - Fast hierarchical Gaussian processes.pdf}
}

@article{flaxmanFastHierarchicalGaussiana,
  title = {Fast Hierarchical {{Gaussian}} Processes},
  author = {Flaxman, Seth and Gelman, Andrew and Neill, Daniel and Smola, Alex and Vehtari, Aki and Wilson, Andrew Gordon},
  pages = {18},
  abstract = {While the framework of Gaussian process priors for functions is very flexible and has a number of advantages, its use within a fully Bayesian hierarchical modeling framework has been limited due to computational constraints. Most often, simple models are fit, with hyperparameters learned by maximum likelihood. But this approach understates the posterior uncertainty in inference. We consider priors over kernel hyperparameters, thus inducing a very flexible Bayesian hierarchical modeling framework in which we perform inference using MCMC not just for the posterior function but also for the posterior over the kernel hyperparameters. We address the central challenge of computational efficiency with MCMC by exploiting separable structure in the covariance matrix corresponding to the kernel, yielding significant gains in time and memory efficiency. Our method can be conveniently implemented in a probabilistic programming language (Stan), is widely applicable to any setting involving structured kernels, and immediately enables a number of useful features, including kernel learning through novel prior specifications, learning nonparametric priors over categorical variables, clustering through a factor analysis approach, and missing observations. We demonstrate our methods on real and synthetic spatiotemporal datasets.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/T8824UC6/Flaxman et al. - Fast hierarchical Gaussian processes.pdf}
}

@article{flaxmanMachineLearningSpace,
  title = {Machine Learning in Space and Time},
  author = {Flaxman, Seth R},
  pages = {148},
  langid = {english},
  file = {/home/yanni/Zotero/storage/C88BGGHE/Flaxman - Machine learning in space and time.pdf}
}

@techreport{francoisLocalityKernelsHighdimensional,
  title = {About the Locality of Kernels in High-Dimensional Spaces},
  author = {Francois, Damien and Wertz, Vincent and Verleysen, Michel},
  abstract = {Gaussian kernels are widely used in many data analysis tools such as Radial-Basis Function networks, Support Vector Machines and many others. Gaus-sian kernels are most often deemed to provide a local measure of similarity between vectors. In this paper, we show that Gaussian kernels are adequate measures of similarity when the representation dimension of the space remains small, but that they fail to reach their goal in high-dimensional spaces. We suggest the use of p-Gaussian kernels that include a supplementary degree of freedom in order to adapt to the distribution of data in high-dimensional problems. The use of such more flexible kernel may greatly improve the numerical stability of algorithms, and also the discriminative power of distance-and neighbor-based data analysis methods.},
  keywords = {Gaussian Kernels,High dimensional spaces,Local Models},
  file = {/home/yanni/Zotero/storage/7HDVNIMP/Francois, Wertz, Verleysen - Unknown - About the locality of kernels in high-dimensional spaces.pdf}
}

@misc{frazierTutorialBayesianOptimization2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02811},
  eprint = {arXiv:1807.02811},
  publisher = {{arXiv}},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/UBDL4N6K/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf;/home/yanni/Zotero/storage/7T86YCMB/1807.html}
}

@techreport{frohlichScalableInferenceOrdinary,
  title = {Scalable {{Inference}} of {{Ordinary Differential Equation Models}} of {{Biochemical Processes}}},
  author = {Fr{\"o}hlich, Fabian and Loos, Carolin and Hasenauer, Jan},
  abstract = {Ordinary differential equation models have become a standard tool for the mechanistic description of biochemical processes. If parameters are inferred from experimental data, such mechanistic models can provide accurate predictions about the behavior of latent variables or the process under new experimental conditions. Complementarily, inference of model structure can be used to identify the most plausible model structure from a set of candidates, and, thus, gain novel biological insight. Several toolboxes can infer model parameters and structure for small-to medium-scale mechanistic models out of the box. However, models for highly multiplexed datasets can require hundreds to thousands of state variables and parameters. For the analysis of such large-scale models, most algorithms require intractably high computation times. This chapter provides an overview of state-of-the-art methods for parameter and model inference, with an emphasis on scalability.},
  keywords = {Large-Scale Models,Ordinary Differential Equations,Parameter Estimation,Uncertainty Analysis}
}

@article{fukumizuKernelDimensionReduction2009,
  title = {Kernel Dimension Reduction in Regression},
  author = {Fukumizu, Kenji and Bach, Francis R and Jordan, Michael I},
  year = {2009},
  journal = {Ann. Statist.},
  volume = {37},
  number = {4},
  pages = {1871--1905},
  publisher = {{The Institute of Mathematical Statistics}},
  doi = {10.1214/08-AOS637},
  file = {/home/yanni/Downloads/08-AOS637.pdf}
}

@article{fukumizuKernelMeasuresConditional,
  title = {Kernel {{Measures}} of {{Conditional Dependence}}},
  author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Sch{\"o}lkopf, Bernhard},
  pages = {8},
  abstract = {We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of infinite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/3AYKKEXG/Fukumizu et al. - Kernel Measures of Conditional Dependence.pdf}
}

@techreport{gahalautConditionNumberEstimates,
  title = {Condition Number Estimates for Matrices Arising in the Isogeometric Discretizations},
  author = {Gahalaut, K and Tomar, S and Gahalaut, K P S and Tomar, S K},
  abstract = {We derive the bounds for the extremal eigenvalues and the spectral condition number of matrices for isogeometric discretizations of elliptic partial differential equations in {$\Omega$} {$\in$} R d , d = 2, 3. For the h-refinement, the condition number of the stiffness matrix is bounded above and below by a constant times h -2 , and the condition number of the mass matrix is uniformly bounded. For the p-refinement, the condition number is bounded above by p 2d 4 pd and p 2(d-1) 4 pd for the stiffness matrix and the mass matrix respectively. Numerical results supporting the theoretical estimates are presented. Some numerical results on the condition number for varying smoothness of the basis functions are also discussed.},
  keywords = {B-Splines,Condition number,Eigenvalues,Galerkin formulation,h-p-r-refinement,Isogeometric method,Mass matrix,NURBS,Stiffness matrix},
  file = {/home/yanni/Zotero/storage/B97CFQAJ/Gahalaut et al. - Unknown - Condition number estimates for matrices arising in the isogeometric discretizations.pdf}
}

@article{galUncertaintyDeepLearning,
  title = {Uncertainty in {{Deep Learning}}},
  author = {Gal, Yarin},
  pages = {174},
  langid = {english},
  file = {/home/yanni/Zotero/storage/KJVEGXG6/Gal - Uncertainty in Deep Learning.pdf}
}

@article{gargMomentumTurningPoints2019,
  title = {Momentum {{Turning Points}}},
  author = {Garg, Ashish and Goulding, Christian L. and Harvey, Campbell R. and Mazzoleni, Michele},
  year = {2019},
  journal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3489539},
  langid = {english},
  file = {/home/yanni/Zotero/storage/YBDQSXK2/Garg et al. - 2019 - Momentum Turning Points.pdf}
}

@article{garnettSequentialBayesianPrediction2010,
  title = {Sequential {{Bayesian Prediction}} in the {{Presence}} of {{Changepoints}} and {{Faults}}},
  author = {Garnett, R. and Osborne, M. A. and Reece, S. and Rogers, A. and Roberts, S. J.},
  year = {2010},
  month = nov,
  journal = {The Computer Journal},
  volume = {53},
  number = {9},
  pages = {1430--1446},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/bxq003},
  langid = {english},
  file = {/home/yanni/Zotero/storage/5WKGL7ID/Garnett et al. - 2010 - Sequential Bayesian Prediction in the Presence of .pdf}
}

@article{garreauChangepointDetectionKernel,
  title = {Change-Point Detection and Kernel Methods},
  author = {Garreau, Damien},
  pages = {155},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GGLIP7XY/Garreau - Change-point detection and kernel methods.pdf}
}

@techreport{garreauLargeSampleAnalysis,
  title = {Large Sample Analysis of the Median Heuristic},
  author = {Garreau, Damien and Kanagawa, Motonobu},
  abstract = {In kernel methods, the median heuristic has been widely used as a way of setting the bandwidth of RBF kernels. While its empirical performances make it a safe choice under many circumstances, there is little theoretical understanding of why this is the case. Our aim in this paper is to advance our understanding of the median heuristic by focusing on the setting of kernel two-sample test. We collect new findings that may be of interest for both theoreticians and practitioners. In theory, we provide a convergence analysis that shows the asymptotic normality of the bandwidth chosen by the median heuristic in the setting of kernel two-sample test. Systematic empirical investigations are also conducted in simple settings, comparing the performances based on the bandwidths chosen by the median heuristic and those by the maximization of test power.},
  file = {/home/yanni/Zotero/storage/X646CW92/Garreau, Kanagawa - Unknown - Large sample analysis of the median heuristic.pdf}
}

@techreport{GaussianRandomVectors2006,
  title = {Gaussian {{Random Vectors}}},
  year = {2006},
  file = {/home/yanni/Zotero/storage/EUVKME7A/Unknown - 2006 - (No Title).pdf}
}

@misc{gautierFullyBayesianGradientFree2021,
  title = {A {{Fully Bayesian Gradient-Free Supervised Dimension Reduction Method}} Using {{Gaussian Processes}}},
  author = {Gautier, Raphael and Pandita, Piyush and Ghosh, Sayan and Mavris, Dimitri},
  year = {2021},
  month = jul,
  number = {arXiv:2008.03534},
  eprint = {arXiv:2008.03534},
  publisher = {{arXiv}},
  abstract = {Modern day engineering problems are ubiquitously characterized by sophisticated computer codes that map parameters or inputs to an underlying physical process. In other situations, experimental setups are used to model the physical process in a laboratory, ensuring high precision while being costly in materials and logistics. In both scenarios, only limited amount of data can be generated by querying the expensive information source at a finite number of inputs or designs. This problem is compounded further in the presence of a high-dimensional input space. State-of-the-art parameter space dimension reduction methods, such as active subspace, aim to identify a subspace of the original input space that is sufficient to explain the output response. These methods are restricted by their reliance on gradient evaluations or copious data, making them inadequate to expensive problems without direct access to gradients. The proposed methodology is gradient-free and fully Bayesian, as it quantifies uncertainty in both the low-dimensional subspace and the surrogate model parameters. This enables a full quantification of epistemic uncertainty and robustness to limited data availability. It is validated on multiple datasets from engineering and science and compared to two other state-of-the-art methods based on four aspects: a) recovery of the active subspace, b) deterministic prediction accuracy, c) probabilistic prediction accuracy, and d) training time. The comparison shows that the proposed method improves the active subspace recovery and predictive accuracy, in both the deterministic and probabilistic sense, when only few model observations are available for training, at the cost of increased training time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/YVIX9R56/Gautier et al_2021_A Fully Bayesian Gradient-Free Supervised Dimension Reduction Method using.pdf}
}

@article{gautierFullyBayesianGradientFree2021a,
  title = {A {{Fully Bayesian Gradient-Free Supervised Dimension Reduction Method}} Using {{Gaussian Processes}}},
  author = {Gautier, Raphael and Pandita, Piyush and Ghosh, Sayan and Mavris, Dimitri},
  year = {2021},
  month = jul,
  journal = {arXiv:2008.03534 [cs, stat]},
  eprint = {2008.03534},
  primaryclass = {cs, stat},
  abstract = {Modern day engineering problems are ubiquitously characterized by sophisticated computer codes that map parameters or inputs to an underlying physical process. In other situations, experimental setups are used to model the physical process in a laboratory, ensuring high precision while being costly in materials and logistics. In both scenarios, only limited amount of data can be generated by querying the expensive information source at a finite number of inputs or designs. This problem is compounded further in the presence of a high-dimensional input space. State-of-the-art parameter space dimension reduction methods, such as active subspace, aim to identify a subspace of the original input space that is sufficient to explain the output response. These methods are restricted by their reliance on gradient evaluations or copious data, making them inadequate to expensive problems without direct access to gradients. The proposed methodology is gradient-free and fully Bayesian, as it quantifies uncertainty in both the low-dimensional subspace and the surrogate model parameters. This enables a full quantification of epistemic uncertainty and robustness to limited data availability. It is validated on multiple datasets from engineering and science and compared to two other state-of-the-art methods based on four aspects: a) recovery of the active subspace, b) deterministic prediction accuracy, c) probabilistic prediction accuracy, and d) training time. The comparison shows that the proposed method improves the active subspace recovery and predictive accuracy, in both the deterministic and probabilistic sense, when only few model observations are available for training, at the cost of increased training time.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/W88R3YE7/Gautier et al. - 2021 - A Fully Bayesian Gradient-Free Supervised Dimensio.pdf}
}

@article{gehretLectureNotesMath182,
  title = {Lecture Notes for {{Math182}}: {{Algorithms Draft}}: {{Last}} Revised {{July}} 28, 2020},
  author = {Gehret, Allen},
  pages = {150},
  langid = {english},
  file = {/home/yanni/Zotero/storage/F92J4GJ9/Gehret - Lecture notes for Math182 Algorithms Draft Last .pdf}
}

@article{gehretLectureNotesMath182a,
  title = {Lecture Notes for {{Math182}}: {{Algorithms Draft}}: {{Last}} Revised {{July}} 28, 2020},
  author = {Gehret, Allen},
  pages = {150},
  langid = {english},
  file = {/home/yanni/Zotero/storage/35PFSXLG/Gehret - Lecture notes for Math182 Algorithms Draft Last .pdf}
}

@article{gehretLectureNotesMath182b,
  title = {Lecture Notes for {{Math182}}: {{Algorithms Draft}}: {{Last}} Revised {{July}} 28, 2020},
  author = {Gehret, Allen},
  pages = {150},
  langid = {english},
  file = {/home/yanni/Zotero/storage/5RF5LYI3/Gehret - Lecture notes for Math182 Algorithms Draft Last .pdf}
}

@techreport{gelmanSimulatingNormalizingConstants1998,
  title = {Simulating {{Normalizing Constants}}: {{From Importance Sampling}} to {{Bridge Sampling}} to {{Path Sampling}}},
  author = {Gelman, Andrew and Meng, Xiao-Li},
  year = {1998},
  journal = {Statistical Science},
  volume = {13},
  number = {2},
  pages = {163--185},
  abstract = {Computing (ratios of) normalizing constants of probability models is a fundamental computational problem for many statistical and scientific studies. Monte Carlo simulation is an effective technique, especially with complex and high-dimensional models. This paper aims to bring to the attention of general statistical audiences of some effective methods originating from theoretical physics and at the same time to explore these methods from a more statistical perspective, through establishing theoretical connections and illustrating their uses with statistical problems. We show that the acceptance ratio method and thermodynamic integration are natural generalizations of importance sampling, which is most familiar to statistical audiences. The former generalizes importance sampling through the use of a single "bridge" density and is thus a case of bridge sampling in the sense of Meng and Wong. Thermodynamic integration, which is also known in the numerical analysis literature as Ogata's method for high-dimensional integration, corresponds to the use of infinitely many and continuously connected bridges (and thus a "path"). Our path sampling formulation offers more flexibility and thus potential efficiency to thermodynamic integration, and the search of optimal paths turns out to have close connections with the Jeffreys prior density and the Rao and Hellinger distances between two densities. We provide an informative theoretical example as well as two empirical examples (involving 17-to 70-dimensional integrations) to illustrate the potential and implementation of path sampling. We also discuss some open problems.},
  keywords = {and phrases: Acceptance ratio method,Hellinger distance,Jeffreys prior density,Markov chain Monte Carlo,numerical integration,Rao distance,thermodynamic integration},
  file = {/home/yanni/Zotero/storage/ZHRDSCT4/Gelman, Meng - 1998 - Simulating Normalizing Constants From Importance Sampling to Bridge Sampling to Path Sampling.pdf}
}

@techreport{gibbsCHOOSINGBOUNDINGPROBABILITY2002,
  title = {{{ON CHOOSING AND BOUNDING PROBABILITY METRICS}}},
  author = {Gibbs, Alison L and Su, Francis Edward},
  year = {2002},
  abstract = {When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability met-rics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric. Abr\'eg\'e. Le choix de m\'etrique de probabilit\'e est une d\'ecision tr\`es importante lorsqu'on\'etudieon\'etudie la convergence des mesures. Nous vous fournissons avec un sommaire de plusieurs m\'etriques/distances de prob-abilit\'e couramment utilis\'ees par des statisticiens(nes) at par des proba-bilistes, ainsi que certains nouveaux r\'esultats qui se rapportent\`arapportent\`a leurs bornes. Avoir connaissance d'autres m\'etriques peut vous fournir avec un moyen de d\'eriver des bornes pour une autre m\'etrique dans unprob\`i eme appliqu\'e. Le fait de prendre en consid\'eration plusieurs m\'etriques vous permettra d'approcher desprob\`i emes d'uneman\`i ere diff\'erente. Ainsi, nous vous d\'emontrons que les taux de convergence peuvent d\'ependre de fa\c{c}on importante sur votre choix de m\'etrique. Il est donc important de tout consid\'erer lorsqu'on doit choisir une m\'etrique.},
  file = {/home/yanni/Zotero/storage/KXP3NQ9A/Gibbs, Su - 2002 - ON CHOOSING AND BOUNDING PROBABILITY METRICS.pdf}
}

@techreport{girolamiStatisticalFiniteElement2019,
  title = {The {{Statistical Finite Element Method}}},
  author = {Girolami, Mark and Gregory, Alastair and Yin, Ge and Cirak, Fehmi},
  year = {2019},
  abstract = {The finite element method (FEM) is one of the great triumphs of modern day applied mathematics, numerical analysis and algorithm development. Engineering and the sciences benefit from the ability to simulate complex systems with FEM. At the same time the ability to obtain data by measurements from these complex systems, often through sensor networks, poses the question of how one systematically incorporates data into the FEM, consistently updating the finite element solution in the face of mathematical model misspecification with physical reality. This paper presents a statistical construction of FEM which goes beyond forward uncertainty propagation or solving inverse problems, and for the first time provides the means for the coherent synthesis of data and FEM.},
  isbn = {1905.06391v1},
  keywords = {Bayesian model updating,Finite element models,Model discrepancy},
  file = {/home/yanni/Zotero/storage/MRH4XTU2/Girolami et al. - 2019 - The Statistical Finite Element Method.pdf}
}

@article{girolamiStatisticalFiniteElement2021,
  title = {The Statistical Finite Element Method ({{statFEM}}) for Coherent Synthesis of Observation Data and Model Predictions},
  author = {Girolami, Mark and Febrianto, Eky and Yin, Ge and Cirak, Fehmi},
  year = {2021},
  month = mar,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {375},
  pages = {113533},
  publisher = {{Elsevier BV}},
  issn = {00457825},
  doi = {10.1016/j.cma.2020.113533},
  abstract = {The increased availability of observation data from engineering systems in operation poses the question of how to incorporate this data into finite element models. To this end, we propose a novel statistical construction of the finite element method that provides the means of synthesising measurement data and finite element models. The Bayesian statistical framework is adopted to treat all the uncertainties present in the data, the mathematical model and its finite element discretisation. From the outset, we postulate a data-generating model which additively decomposes data into a finite element, a model misspecification and a noise component. Each of the components may be uncertain and is considered as a random variable with a respective prior probability density. The prior of the finite element component is given by a conventional stochastic forward problem. The prior probabilities of the model misspecification and measurement noise, without loss of generality, are assumed to have zero-mean and known covariance structure. Our proposed statistical model is hierarchical in the sense that each of the three random components may depend on non-observable random hyperparameters. Because of the hierarchical structure of the statistical model, Bayes rule is applied on three different levels in turn to infer the posterior densities of the three random components and hyperparameters. On level one, we determine the posterior densities of the finite element component and the true system response using the prior finite element density given by the forward problem and the data likelihood. On the next level, we infer the hyperparameter posterior densities from their respective priors and the marginal likelihood of the first inference problem. Finally, on level three we use Bayes rule to choose the most suitable finite element model in light of the observed data by computing the model posteriors.},
  keywords = {Bayesian inference,Data-centric engineering,Finite elements,Gaussian processes,Physics-informed machine learning,Stochastic PDEs},
  file = {/home/yanni/Zotero/storage/HB7N2KE6/Girolami et al. - 2021 - The statistical finite element method (statFEM) for coherent synthesis of observation data and model predict(2).pdf}
}

@book{girolamiStatisticalInferenceGenerative2019,
  title = {Statistical {{Inference}} for {{Generative Models}} with {{Maximum Mean Discrepancy}}},
  author = {Girolami, Francois-Xavier Briol \& Alessandro Barp \& Andrew B Duncan \& Mark},
  year = {2019}
}

@book{godinhoIntroductionRiemannianGeometry2014,
  title = {An {{Introduction}} to {{Riemannian Geometry}}: {{With Applications}} to {{Mechanics}} and {{Relativity}}},
  shorttitle = {An {{Introduction}} to {{Riemannian Geometry}}},
  author = {Godinho, Leonor and Nat{\'a}rio, Jos{\'e}},
  year = {2014},
  series = {Universitext},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-08666-8},
  isbn = {978-3-319-08665-1 978-3-319-08666-8},
  langid = {english},
  file = {/home/yanni/Zotero/storage/ZXKPIYJG/Godinho and Nat√°rio - 2014 - An Introduction to Riemannian Geometry With Appli.pdf}
}

@book{gohbergBasicClassesLinear2012,
  title = {Basic {{Classes}} of {{Linear Operators}}},
  author = {Gohberg, Israel and Goldberg, Seymour and Kaashoek, Marinus},
  year = {2012},
  month = dec,
  publisher = {{Birkh\"auser}},
  abstract = {The present book is an expanded and enriched version ofthe textBasicOperator Theory, written by the first two authors more than twenty years ago. Since then the three ofus have used the basic operator theory text in various courses. This experience motivated us to update and improve the old text by including a wider variety ofbasic classes ofoperators and their applications. The present book has also been written in such a way that it can serve as an introduction to our previous booksClassesofLinearOperators, Volumes I and II. We view the three books as a unit. We gratefully acknowledge the support of the mathematical departments of Tel-Aviv University, the University of Maryland at College Park, and the Vrije Universiteit atAmsterdam. The generous support ofthe Silver Family Foundation is highly appreciated. Amsterdam, November 2002 The authors Introduction This elementary text is an introduction to functional analysis, with a strong emphasis on operator theory and its applications. It is designed for graduate and senior undergraduate students in mathematics, science, engineering, and other fields.},
  googlebooks = {sR\_yBwAAQBAJ},
  isbn = {978-3-0348-7980-4},
  langid = {english},
  keywords = {Mathematics / Functional Analysis,Mathematics / Mathematical Analysis}
}

@article{goldDoublyStochasticChange2018,
  title = {A {{Doubly Stochastic Change Point Detection Algorithm}} for {{Noisy Biological Signals}}},
  author = {Gold, Nathan and Frasch, Martin G. and Herry, Christophe L. and Richardson, Bryan S. and Wang, Xiaogang},
  year = {2018},
  month = jan,
  journal = {Front. Physiol.},
  volume = {8},
  pages = {1112},
  issn = {1664-042X},
  doi = {10.3389/fphys.2017.01112},
  abstract = {Experimentally and clinically collected time series data are often contaminated with significant confounding noise, creating short, noisy time series. This noise, due to natural variability and measurement error, poses a challenge to conventional change point detection methods. We propose a novel and robust statistical method for change point detection for noisy biological time sequences. Our method is a significant improvement over traditional change point detection methods, which only examine a potential anomaly at a single time point. In contrast, our method considers all suspected anomaly points and considers the joint probability distribution of the number of change points and the elapsed time between two consecutive anomalies. We validate our method with three simulated time series, a widely accepted benchmark data set, two geological time series, a data set of ECG recordings, and a physiological data set of heart rate variability measurements of fetal sheep model of human labor, comparing it to three existing methods. Our method demonstrates significantly improved performance over the existing point-wise detection methods.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/9IDRIQ4L/Gold et al. - 2018 - A Doubly Stochastic Change Point Detection Algorit.pdf}
}

@book{goossensLaTeXCompanion1993,
  title = {The \textbackslash{{LaTeX}}\textbackslash{} {{Companion}}},
  author = {Goossens, Michel and Mittelbach, Frank and Samarin, Alexander},
  year = {1993},
  publisher = {{Addison-Wesley}}
}

@techreport{graepelSolvingNoisyLinear,
  title = {Solving {{Noisy Linear Operator Equations}} by {{Gaussian Processes}}: {{Application}} to {{Ordinary}} and {{Partial Differential Equations}}},
  author = {Graepel, Thore},
  abstract = {We formulate the problem of solving stochas-tic linear operator equations in a Bayesian Ganssian process (GP) framework. The solution is obtained in the spirit of a collocation method based on noisy evaluations of the target function at randomly drawn or deliberately chosen points. Prior knowledge about the solution is encoded by the covariance kernel of the GP. As in GP regression, analytical expressions for the mean and variance of the estimated target function are obtained, from which the solution of the operator equation follows by a manipulation of the kernel. Linear initial and boundary value constraints can be enforced by embedding the non-parametric model in a form that automatically satisfies the boundary conditions. The method is illustrated on a noisy linear first-order ordinary differential equation with initial condition and on a noisy second-order partial differential equation with Dirichlet boundary conditions.},
  file = {/home/yanni/Zotero/storage/GMAYHDG2/Graepel - Unknown - Solving Noisy Linear Operator Equations by Gaussian Processes Application to Ordinary and Partial Differential Eq(2).pdf}
}

@techreport{graepelSolvingNoisyLineara,
  title = {Solving {{Noisy Linear Operator Equations}} by {{Gaussian Processes}}: {{Application}} to {{Ordinary}} and {{Partial Differential Equations}}},
  author = {Graepel, Thore},
  abstract = {We formulate the problem of solving stochas-tic linear operator equations in a Bayesian Ganssian process (GP) framework. The solution is obtained in the spirit of a collocation method based on noisy evaluations of the target function at randomly drawn or deliberately chosen points. Prior knowledge about the solution is encoded by the covariance kernel of the GP. As in GP regression, analytical expressions for the mean and variance of the estimated target function are obtained, from which the solution of the operator equation follows by a manipulation of the kernel. Linear initial and boundary value constraints can be enforced by embedding the non-parametric model in a form that automatically satisfies the boundary conditions. The method is illustrated on a noisy linear first-order ordinary differential equation with initial condition and on a noisy second-order partial differential equation with Dirichlet boundary conditions.},
  file = {/home/yanni/Zotero/storage/VRVA6B6J/Graepel - Unknown - Solving Noisy Linear Operator Equations by Gaussian Processes Application to Ordinary and Partial Differential Equat.pdf}
}

@techreport{gregorySynthesisDataInstrumented2019,
  title = {The Synthesis of Data from Instrumented Structures and Physics-Based Models via {{Gaussian}} Processes},
  author = {Gregory, Alastair and Din, F and Lau, -Houn and Girolami, Mark and Butler, Liam J and Elshafie, Mohammed Z E B},
  year = {2019},
  abstract = {At the heart of structural engineering research is the use of data obtained from physical structures such as bridges, viaducts and buildings. These data can represent how the structure responds to various stimuli over time when in operation. Many models have been proposed in literature to represent such data, such as linear statistical models. Based upon these models, the health of the structure is reasoned about, e.g. through damage indices, changes in likelihood and statistical parameter estimates. On the other hand, physics-based models are typically used when designing structures to predict how the structure will respond to operational stimuli. These models represent how the structure responds to stimuli under idealised conditions. What remains unclear in the literature is how to combine the observed data with information from the idealised physics-based model into a model that describes the responses of the operational structure. This paper introduces a new approach which fuses together observed data from a physical structure during operation and information from a mathematical model. The observed data are combined with data simulated from the physics-based model using a multi-output Gaussian process formulation. The novelty of this method is how the information from observed data and the physics-based model is balanced to obtain a representative model of the structures response to stimuli. We present our method using data obtained from a fibre-optic sensor network installed on experimental railway sleepers. The curvature of the sleeper at sensor and also non-sensor locations is modelled, guided by the mathematical representation. We discuss how this approach can be used to reason about changes in the structures behaviour over time using simulations and experimental data. The results show that the methodology can accurately detect such changes. They also indicate that the methodology can infer information about changes in the parameters within the physics-based model, including those governing components of the structure not measured directly by sensors such as the ballast foundation.},
  isbn = {1811.10882v2},
  keywords = {damage detection,data-centric engineering,Gaussian processes,structural health monitoring},
  file = {/home/yanni/Zotero/storage/GE8TISNS/Gregory et al. - 2019 - The synthesis of data from instrumented structures and physics-based models via Gaussian processes.pdf}
}

@article{grettonIntroductionRKHSSimplea,
  title = {Introduction to {{RKHS}}, and Some Simple Kernel Algorithms},
  author = {Gretton, Arthur},
  langid = {english},
  file = {/home/yanni/Zotero/storage/BPRT8WXX/Gretton - Introduction to RKHS, and some simple kernel algor.pdf}
}

@techreport{grettonKernelMethodTwoSample2008,
  title = {A {{Kernel Method}} for the {{Two-Sample Problem}}},
  author = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {1},
  pages = {1--10},
  abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  keywords = {asymptotic analysis,Borgwardt,hypothesis testing Gretton,Kernel methods,Rasch,schema matching,Sch√∂lkopf and Smola,two sample test,uniform convergence bounds},
  file = {/home/yanni/Zotero/storage/6J82MWBW/Gretton et al. - 2008 - A Kernel Method for the Two-Sample Problem.pdf}
}

@book{grisvardEllipticProblemsNonsmooth2011,
  title = {Elliptic Problems in Nonsmooth Domains},
  author = {Grisvard, P.},
  year = {2011},
  series = {Classics in Applied Mathematics},
  edition = {SIAM ed},
  number = {69},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia}},
  isbn = {978-1-61197-202-3},
  langid = {english},
  lccn = {QA379 .G74 2011},
  keywords = {Boundary value problems,Differential equations; Elliptic,Numerical solutions},
  annotation = {OCLC: ocn746618649},
  file = {/home/yanni/Zotero/storage/U67EXG5I/Grisvard - 2011 - Elliptic problems in nonsmooth domains.pdf}
}

@article{guedonExploringLatentSegmentation,
  title = {Exploring the Latent Segmentation Space for the Assessment of Multiple Change-Point Models},
  author = {Gu{\'e}don, Yann},
  pages = {38},
  abstract = {This paper addresses the retrospective or off-line multiple change-point detection problem. Multiple change-point models are here viewed as latent structure models and the focus is on inference concerning the latent segmentation space. Methods for exploring the space of possible segmentations of a sequence for a fixed number of change points may be divided into two categories: (i) enumeration of segmentations, (ii) summary of the possible segmentations in change-point or segment profiles. Concerning the first category, a dynamic programming algorithm for computing the top N most probable segmentations is derived. Concerning the second category, a forwardbackward dynamic programming algorithm and a smoothing-type forward-backward algorithm for computing two types of change-point and segment profiles are derived. The proposed methods are mainly useful for exploring the segmentation space for successive numbers of change points and provide a set of assessment tools for multiple change-point models that can be applied both in a non-Bayesian and a Bayesian framework. We show using examples that the proposed methods may help to compare alternative multiple change-point models (e.g. Gaussian model with piecewise constant variances or global variance), predict supplementary change points, highlight overestimation of the number of change points and summarize the uncertainty concerning the position of change points.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VEXF55X5/Gu√©don - Exploring the latent segmentation space for the as.pdf}
}

@article{guedonExploringLatentSegmentation2013,
  title = {Exploring the Latent Segmentation Space for the Assessment of Multiple Change-Point Models},
  author = {Gu{\'e}don, Yann},
  year = {2013},
  month = dec,
  journal = {Comput Stat},
  volume = {28},
  number = {6},
  pages = {2641--2678},
  issn = {0943-4062, 1613-9658},
  doi = {10.1007/s00180-013-0422-9},
  abstract = {This paper addresses the retrospective or off-line multiple change-point detection problem. Multiple change-point models are here viewed as latent structure models and the focus is on inference concerning the latent segmentation space. Methods for exploring the space of possible segmentations of a sequence for a fixed number of change points may be divided into two categories: (i) enumeration of segmentations, (ii) summary of the possible segmentations in change-point or segment profiles. Concerning the first category, a dynamic programming algorithm for computing the top N most probable segmentations is derived. Concerning the second category, a forwardbackward dynamic programming algorithm and a smoothing-type forward-backward algorithm for computing two types of change-point and segment profiles are derived. The proposed methods are mainly useful for exploring the segmentation space for successive numbers of change points and provide a set of assessment tools for multiple change-point models that can be applied both in a non-Bayesian and a Bayesian framework. We show using examples that the proposed methods may help to compare alternative multiple change-point models (e.g. Gaussian model with piecewise constant variances or global variance), predict supplementary change points, highlight overestimation of the number of change points and summarize the uncertainty concerning the position of change points.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/FL7YHUPF/Gu√©don - 2013 - Exploring the latent segmentation space for the as.pdf}
}

@inproceedings{guFastChangePoint2013,
  title = {Fast {{Change Point Detection}} for Electricity Market Analysis},
  booktitle = {2013 {{IEEE International Conference}} on {{Big Data}}},
  author = {Gu, William and Choi, Jaesik and Gu, Ming and Simon, Horst and Wu, Kesheng},
  year = {2013},
  month = oct,
  pages = {50--57},
  publisher = {{IEEE}},
  address = {{Silicon Valley, CA, USA}},
  doi = {10.1109/BigData.2013.6691733},
  abstract = {Electricity is a vital part of our daily life; therefore it is important to avoid irregularities such as the California Electricity Crisis of 2000 and 2001. In this work, we seek to predict anomalies using advanced machine learning algorithms. These algorithms are effective, but computationally expensive, especially if we plan to apply them on hourly electricity market data covering a number of years. To address this challenge, we significantly accelerate the computation of the Gaussian Process (GP) for time series data. In the context of a Change Point Detection (CPD) algorithm, we reduce its computational complexity from O(n5) to O(n2). Our efficient algorithm makes it possible to compute the Change Points using the hourly price data from the California Electricity Crisis. By comparing the detected Change Points with known events, we show that the Change Point Detection algorithm is indeed effective in detecting signals preceding major events.},
  isbn = {978-1-4799-1293-3},
  langid = {english},
  file = {/home/yanni/Zotero/storage/7RHCLDF7/Gu et al. - 2013 - Fast Change Point Detection for electricity market.pdf}
}

@article{hainesGaussianConjugatePrior,
  title = {Gaussian {{Conjugate Prior Cheat Sheet}}},
  author = {Haines, Tom SF},
  pages = {7},
  langid = {english},
  file = {/home/yanni/Zotero/storage/T7YCJVRY/Haines - Gaussian Conjugate Prior Cheat Sheet.pdf}
}

@article{hainesGaussianConjugatePriora,
  title = {Gaussian {{Conjugate Prior Cheat Sheet}}},
  author = {Haines, Tom SF},
  pages = {7},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VKH59H2E/Haines - Gaussian Conjugate Prior Cheat Sheet.pdf}
}

@techreport{hairerIntroductionStochasticPDEs2009,
  title = {An {{Introduction}} to {{Stochastic PDEs}}},
  author = {Hairer, Martin},
  year = {2009},
  file = {/home/yanni/Zotero/storage/EQMCTUQ9/Hairer - 2009 - An Introduction to Stochastic PDEs.pdf}
}

@techreport{hamiltonParameterEstimationDifferential2011,
  title = {Parameter {{Estimation}} in {{Differential Equations}}: {{A Numerical Study}} of {{Shooting Methods}}},
  author = {Hamilton, Franz and Sauer, Timothy},
  year = {2011},
  abstract = {Differential equation modeling is central to applications of mathematics to science and engineering. When a particular system of equations is used in an application, it is often important to determine unknown parameters. We compare the traditional shooting method to versions of multiple shooting methods in chaotic systems with noise added and conduct numerical experiments as to the reliability and accuracy of both methods.},
  file = {/home/yanni/Zotero/storage/PW4WIBRF/Hamilton, Sauer - 2011 - Parameter Estimation in Differential Equations A Numerical Study of Shooting Methods.pdf}
}

@article{harchaouiKernelChangepointAnalysis,
  title = {Kernel {{Change-point Analysis}}},
  author = {Harchaoui, Za{\"i}d and Moulines, Eric and Bach, Francis R},
  pages = {8},
  abstract = {We introduce a kernel-based method for change-point analysis within a sequence of temporal observations. Change-point analysis of an unlabelled sample of observations consists in, first, testing whether a change in the distribution occurs within the sample, and second, if a change occurs, estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution. We propose a test statistic based upon the maximum kernel Fisher discriminant ratio as a measure of homogeneity between segments. We derive its limiting distribution under the null hypothesis (no change occurs), and establish the consistency under the alternative hypothesis (a change occurs). This allows to build a statistical hypothesis testing procedure for testing the presence of a change-point, with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting. If a change actually occurs, the test statistic also yields an estimator of the change-point location. Promising experimental results in temporal segmentation of mental tasks from BCI data and pop song indexation are presented.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/4NB6T4E7/Harchaoui et al. - Kernel Change-point Analysis.pdf}
}

@article{haynesEfficientPenaltySearch2014,
  title = {Efficient Penalty Search for Multiple Changepoint Problems},
  author = {Haynes, Kaylea and Eckley, Idris A. and Fearnhead, Paul},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.3617 [stat]},
  eprint = {1412.3617},
  primaryclass = {stat},
  abstract = {In the multiple changepoint setting, various search methods have been proposed which involve optimising either a constrained or penalised cost function over possible numbers and locations of changepoints using dynamic programming. Such methods are typically computationally intensive. Recent work in the penalised optimisation setting has focussed on developing a pruning-based approach which gives an improved computational cost that, under certain conditions, is linear in the number of data points. Such an approach naturally requires the specification of a penalty to avoid under/over-fitting. Work has been undertaken to identify the appropriate penalty choice for data generating processes with known distributional form, but in many applications the model assumed for the data is not correct and these penalty choices are not always appropriate. Consequently it is desirable to have an approach that enables us to compare segmentations for different choices of penalty. To this end we present a method to obtain optimal changepoint segmentations of data sequences for all penalty values across a continuous range. This permits an evaluation of the various segmentations to identify a suitably parsimonious penalty choice. The computational complexity of this approach can be linear in the number of data points and linear in the difference between the number of changepoints in the optimal segmentations for the smallest and largest penalty values. This can be orders of magnitude faster than alternative approaches that find optimal segmentations for a range of the number of changepoints.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/QKNQC7TD/Haynes et al. - 2014 - Efficient penalty search for multiple changepoint .pdf}
}

@techreport{heinonenLearningUnknownODE,
  title = {Learning Unknown {{ODE}} Models with {{Gaussian}} Processes},
  author = {Heinonen, Markus and Yildiz, Cagatay and Mannerstr{\"o}m, Henrik and Intosalmi, Jukka and L{\"a}hdesm{\"a}ki, Harri},
  abstract = {In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modeling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model's capabilities to infer dynamics from sparse data and to simulate the system forward into future.},
  file = {/home/yanni/Zotero/storage/SLBLHW95/Heinonen et al. - Unknown - Learning unknown ODE models with Gaussian processes.pdf}
}

@techreport{hennigPROBABILISTICINTERPRETATIONLINEAR,
  title = {{{PROBABILISTIC INTERPRETATION OF LINEAR SOLVERS}}},
  author = {Hennig, Philipp},
  abstract = {This manuscript proposes a probabilistic framework for algorithms that iteratively solve unconstrained linear problems Bx = b with positive definite B for x. The goal is to replace the point estimates returned by existing methods with a Gaussian posterior belief over the elements of the inverse of B, which can be used to estimate errors. Recent probabilistic interpretations of the secant family of quasi-Newton optimization algorithms are extended. Combined with properties of the conjugate gradient algorithm, this leads to uncertainty-calibrated methods with very limited cost overhead over conjugate gradients, a self-contained novel interpretation of the quasi-Newton and conjugate gradient algorithms, and a foundation for new nonlinear optimization methods.},
  keywords = {60G15,65K05,conjugate gradient,Gaussian inference AMS subject classifications 49M15,linear programming,quasi-Newton methods},
  file = {/home/yanni/Zotero/storage/MK8EQK9R/Hennig - Unknown - PROBABILISTIC INTERPRETATION OF LINEAR SOLVERS.pdf}
}

@article{hennigProbabilisticNumericsUncertainty2015,
  title = {Probabilistic Numerics and Uncertainty in Computations},
  author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
  year = {2015},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {471},
  number = {2179},
  pages = {20150142},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2015.0142},
  abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data have led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimizers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.},
  keywords = {inference,numerical methods,probability,statistics},
  file = {/home/yanni/Zotero/storage/QGKGZDUE/Hennig et al. - 2015 - Probabilistic numerics and uncertainty in computat.pdf}
}

@techreport{hennigProbabilisticSolutionsDifferential,
  title = {Probabilistic {{Solutions}} to {{Differential Equations}} and Their {{Application}} to {{Riemannian Statistics}}},
  author = {Hennig, Philipp and Hauberg, S{\o}ren},
  abstract = {We study a probabilistic numerical method for the solution of both boundary and initial value problems that returns a joint Gaus-sian process posterior over the solution. Such methods have concrete value in the statistics on Riemannian manifolds, where non-analytic ordinary differential equations are involved in virtually all computations. The probabilistic formulation permits marginalis-ing the uncertainty of the numerical solution such that statistics are less sensitive to inaccuracies. This leads to new Riemannian algorithms for mean value computations and principal geodesic analysis. Marginalisation also means results can be less precise than point estimates, enabling a noticeable speed-up over the state of the art. Our approach is an argument for a wider point that uncertainty caused by numerical calculations should be tracked throughout the pipeline of machine learning algorithms.},
  file = {/home/yanni/Zotero/storage/LGUCCBWM/Hennig, Hauberg - Unknown - Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics.pdf}
}

@techreport{highamAlgorithmicIntroductionNumerical2001,
  title = {An {{Algorithmic Introduction}} to {{Numerical Simulation}} of {{Stochastic Differential Equations}} *},
  author = {Higham, Desmond J},
  year = {2001},
  journal = {Society for Industrial and Applied Mathematics},
  volume = {43},
  number = {3},
  pages = {525--546},
  abstract = {A practical and accessible introduction to numerical methods for stochastic differential equations is given. The reader is assumed to be familiar with Euler's method for de-terministic differential equations and to have at least an intuitive feel for the concept of a random variable; however, no knowledge of advanced probability theory or stochastic processes is assumed. The article is built around 10 MATLAB programs, and the topics covered include stochastic integration, the Euler-Maruyama method, Milstein's method, strong and weak convergence, linear stability, and the stochastic chain rule.},
  keywords = {65C20,Euler-Maruyama method,MATLAB,Milstein method,Monte Carlo,stochastic simula-tion,strong and weak convergence AMS subject classifications 65C30},
  file = {/home/yanni/Zotero/storage/XQKQJPBV/Higham - 2001 - An Algorithmic Introduction to Numerical Simulation of Stochastic Differential Equations.pdf}
}

@misc{HitchhikerGuideFractional,
  title = {Hitchhiker's Guide to the Fractional {{Sobolev}} Spaces | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.bulsci.2011.12.004},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0007449711001254?token=4B684A3ABA758C1F55E36A1276AA68913BE1A74FE125F3B9B60B2A9EE07EAD048F4CCA7D0293404EE91DFA3980B49858\&originRegion=eu-west-1\&originCreation=20221216195105},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VDUY2C3K/Hitchhiker's guide to the fractional Sobolev space.pdf}
}

@article{hongParametricRegressionGrassmannian2016,
  title = {Parametric {{Regression}} on the {{Grassmannian}}},
  author = {Hong, Yi and Kwitt, Roland and Singh, Nikhil and Vasconcelos, Nuno and Niethammer, Marc},
  year = {2016},
  month = nov,
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {38},
  number = {11},
  pages = {2284--2297},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2016.2516533},
  langid = {english},
  file = {/home/yanni/Zotero/storage/W4MCQTLR/Hong et al. - 2016 - Parametric Regression on the Grassmannian.pdf}
}

@article{huangIntrinsicRepresentationTangent2017,
  title = {Intrinsic Representation of Tangent Vectors and Vector Transports on Matrix Manifolds},
  author = {Huang, Wen and Absil, P.-A. and Gallivan, K. A.},
  year = {2017},
  month = jun,
  journal = {Numer. Math.},
  volume = {136},
  number = {2},
  pages = {523--543},
  issn = {0029-599X, 0945-3245},
  doi = {10.1007/s00211-016-0848-4},
  abstract = {The quasi-Newton methods on Riemannian manifolds proposed thus far do not appear to lend themselves to satisfactory convergence analyses unless they resort to an isometric vector transport. This prompts us to propose a computationally tractable isometric vector transport on the Stiefel manifold of orthonormal p-frames in Rn. Specifically, it requires O(np2) flops, which is considerably less expensive than existing alternatives in the frequently encountered case where n p. We then build on this result to also propose computationally tractable isometric vector transports on other manifolds, namely the Grassmann manifold, the fixed-rank manifold, and the positivesemidefinite fixed-rank manifold. In the process, we also propose a convenient way to represent tangent vectors to these manifolds as elements of Rd, where d is the dimension of the manifold. We call this an ``intrinsic'' representation, as opposed to ``extrinsic'' representations as elements of Rw, where w is the dimension of the embedding space. Finally, we demonstrate the performance of the proposed isometric vector transport in the context of a Riemannian quasi-Newton method applied to minimizing the Brockett cost function.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/5A49KDAI/Huang et al. - 2017 - Intrinsic representation of tangent vectors and ve.pdf}
}

@article{hubertMinimumCovarianceDeterminant2018,
  title = {Minimum {{Covariance Determinant}} and {{Extensions}}},
  author = {Hubert, Mia and Debruyne, Michiel and Rousseeuw, Peter J.},
  year = {2018},
  month = may,
  journal = {WIREs Comp Stat},
  volume = {10},
  number = {3},
  eprint = {1709.07045},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1421},
  abstract = {The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. Since estimating the covariance matrix is the cornerstone of many multivariate statistical methods, the MCD is an important building block when developing robust multivariate techniques. It also serves as a convenient and efficient tool for outlier detection. The MCD estimator is reviewed, along with its main properties such as affine equivariance, breakdown value, and influence function. We discuss its computation, and list applications and extensions of the MCD in applied and methodological multivariate statistics. Two recent extensions of the MCD are described. The first one is a fast deterministic algorithm which inherits the robustness of the MCD while being almost affine equivariant. The second is tailored to high-dimensional data, possibly with more dimensions than cases, and incorporates regularization to prevent singular matrices.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/TLAG7REN/Hubert et al. - 2018 - Minimum Covariance Determinant and Extensions.pdf}
}

@techreport{huBOUNDSEIGENVALUESCONDITION1998,
  title = {{{BOUNDS FOR EIGENVALUES AND CONDITION NUMBERS IN THE}} P-{{VERSION OF THE FINITE ELEMENT METHOD}}},
  author = {Hu, Ning and Guo, Xian-Zhong and Katz, I Norman},
  year = {1998},
  journal = {MATHEMATICS OF COMPUTATION},
  volume = {67},
  number = {98},
  pages = {983--984},
  abstract = {In this paper, we present a theory for bounding the minimum eigenvalues, maximum eigenvalues, and condition numbers of stiffness matrices arising from the p-version of finite element analysis. Bounds are derived for the eigenvalues and the condition numbers, which are valid for stiffness matrices based on a set of general basis functions that can be used in the p-version. For a set of hierarchical basis functions satisfying the usual local support condition that has been popularly used in the p-version, explicit bounds are derived for the minimum eigenvalues, maximum eigenvalues, and condition numbers of stiffness matrices. We prove that the condition numbers of the stiffness matrices grow like p 4(d-1) , where d is the number of dimensions. Our results disprove a conjecture of Olsen and Douglas in which the authors assert that "regardless of the choice of basis, the condition numbers grow like p 4d or faster". Numerical results are also presented which verify that our theoretical bounds are correct.},
  file = {/home/yanni/Zotero/storage/CU48MI9B/Hu, Guo, Katz - 1998 - BOUNDS FOR EIGENVALUES AND CONDITION NUMBERS IN THE p-VERSION OF THE FINITE ELEMENT METHOD.pdf}
}

@article{huBriefIntroductionManifold2020,
  title = {A {{Brief Introduction}} to {{Manifold Optimization}}},
  author = {Hu, Jiang and Liu, Xin and Wen, Zai-Wen and Yuan, Ya-Xiang},
  year = {2020},
  month = jun,
  journal = {J. Oper. Res. Soc. China},
  volume = {8},
  number = {2},
  pages = {199--248},
  issn = {2194-668X, 2194-6698},
  doi = {10.1007/s40305-020-00295-9},
  abstract = {Manifold optimization is ubiquitous in computational and applied mathematics, statistics, engineering, machine learning, physics, chemistry, etc. One of the main challenges usually is the non-convexity of the manifold constraints. By utilizing the geometry of manifold, a large class of constrained optimization problems can be viewed as unconstrained optimization problems on manifold. From this perspective, intrinsic structures, optimality conditions and numerical algorithms for manifold optimization are investigated. Some recent progress on the theoretical results of manifold optimization is also presented.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/664WS4V9/Hu et al. - 2020 - A Brief Introduction to Manifold Optimization.pdf}
}

@article{hunter-zinckTenSimpleRules2021,
  title = {Ten Simple Rules on Writing Clean and Reliable Open-Source Scientific Software},
  author = {{Hunter-Zinck}, Haley and {de Siqueira}, Alexandre Fioravante and V{\'a}squez, V{\'a}leri N. and Barnes, Richard and Martinez, Ciera C.},
  editor = {Markel, Scott},
  year = {2021},
  month = nov,
  journal = {PLoS Comput Biol},
  volume = {17},
  number = {11},
  pages = {e1009481},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009481},
  abstract = {Functional, usable, and maintainable open-source software is increasingly essential to scientific research, but there is a large variation in formal training for software development and maintainability. Here, we propose 10 ``rules'' centered on 2 best practice components: clean code and testing. These 2 areas are relatively straightforward and provide substantial utility relative to the learning investment. Adopting clean code practices helps to standardize and organize software code in order to enhance readability and reduce cognitive load for both the initial developer and subsequent contributors; this allows developers to concentrate on core functionality and reduce errors. Clean coding styles make software code more amenable to testing, including unit tests that work best with modular and consistent software code. Unit tests interrogate specific and isolated coding behavior to reduce coding errors and ensure intended functionality, especially as code increases in complexity; unit tests also implicitly provide example usages of code. Other forms of testing are geared to discover erroneous behavior arising from unexpected inputs or emerging from the interaction of complex codebases. Although conforming to coding styles and designing tests can add time to the software development project in the short term, these foundational tools can help to improve the correctness, quality, usability, and maintainability of open-source scientific software code. They also advance the principal point of scientific research: producing accurate results in a reproducible way. In addition to suggesting several tips for getting started with clean code and testing practices, we recommend numerous tools for the popular open-source scientific software languages Python, R, and Julia.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/ZZTVYVLA/Hunter-Zinck et al. - 2021 - Ten simple rules on writing clean and reliable ope.pdf}
}

@article{huntPrevalenceTranslationinvariantAlmost1992,
  title = {Prevalence: A Translation-Invariant ``Almost Every'' on Infinite-Dimensional Spaces},
  shorttitle = {Prevalence},
  author = {Hunt, Brian R. and Sauer, Tim and Yorke, James A.},
  year = {1992},
  journal = {Bull. Amer. Math. Soc.},
  volume = {27},
  number = {2},
  pages = {217--238},
  issn = {0273-0979, 1088-9485},
  doi = {10.1090/S0273-0979-1992-00328-2},
  abstract = {We present a measure-theoretic condition for a property to hold "almost everywhere" on an infinite-dimensional vector space, with particular emphasis on function spaces such as Ck and LP . Like the concept of "Lebesgue almost every" on finite-dimensional spaces, our notion of "prevalence" is translation invariant. Instead of using a specific measure on the entire space, we define prevalence in terms of the class of all probability measures with compact support. Prevalence is a more appropriate condition than the topological concepts of "open and dense" or "generic" when one desires a probabilistic result on the likelihood of a given property on a function space. We give several examples of properties which hold "almost everywhere" in the sense of prevalence. For instance, we prove that almost every C1 map on R" has the property that all of its periodic orbits are hyperbolic.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/EUT9HGHQ/Hunt et al. - 1992 - Prevalence a translation-invariant ‚Äúalmost every‚Äù.pdf}
}

@article{huperComputationMeansGrassmann2010,
  title = {On the {{Computation}} of {{Means}} on {{Grassmann Manifolds}}},
  author = {H{\"u}per, Knut and Helmke, Uwe and Herzberg, Sven},
  year = {2010},
  abstract = {Given a set of data points on a Grassmann manifold sufficiently close to each other, one way to define their centroid or geometric mean is via the minimizer of a certain cost function. If one chooses the cost as the sum of squared geodesic distances between a given point and all the data points we end up with the definition of the Karcher mean. In this paper we analyze the critical points for this cost function.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VT5EQR79/H√ºper et al. - 2010 - On the Computation of Means on Grassmann Manifolds.pdf}
}

@inproceedings{ideChangePointDetectionUsing2007,
  title = {Change-{{Point Detection}} Using {{Krylov Subspace Learning}}},
  booktitle = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Id{\'e}, Tsuyoshi and Tsuda, Koji},
  year = {2007},
  month = apr,
  pages = {515--520},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972771.54},
  abstract = {We propose an efficient algorithm for principal component analysis (PCA) that is applicable when only the inner product with a given vector is needed. We show that Krylov subspace learning works well both in matrix compression and implicit calculation of the inner product by taking full advantage of the arbitrariness of the seed vector. We apply our algorithm to a PCA-based change-point detection algorithm, and show that it results in about 50 times improvement in computational time.},
  isbn = {978-0-89871-630-6 978-1-61197-277-1},
  langid = {english},
  file = {/home/yanni/Zotero/storage/LZPBB5JT/Id√© and Tsuda - 2007 - Change-Point Detection using Krylov Subspace Learn.pdf}
}

@inproceedings{ideChangePointDetectionUsing2007a,
  title = {Change-{{Point Detection}} Using {{Krylov Subspace Learning}}},
  booktitle = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Id{\'e}, Tsuyoshi and Tsuda, Koji},
  year = {2007},
  month = apr,
  pages = {515--520},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972771.54},
  abstract = {We propose an efficient algorithm for principal component analysis (PCA) that is applicable when only the inner product with a given vector is needed. We show that Krylov subspace learning works well both in matrix compression and implicit calculation of the inner product by taking full advantage of the arbitrariness of the seed vector. We apply our algorithm to a PCA-based change-point detection algorithm, and show that it results in about 50 times improvement in computational time.},
  isbn = {978-0-89871-630-6 978-1-61197-277-1},
  langid = {english},
  file = {/home/yanni/Zotero/storage/S79SFMVX/Id√© and Tsuda - 2007 - Change-Point Detection using Krylov Subspace Learn.pdf}
}

@inproceedings{ideChangePointDetectionUsing2007b,
  title = {Change-{{Point Detection}} Using {{Krylov Subspace Learning}}},
  booktitle = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Id{\'e}, Tsuyoshi and Tsuda, Koji},
  year = {2007},
  month = apr,
  pages = {515--520},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972771.54},
  abstract = {We propose an efficient algorithm for principal component analysis (PCA) that is applicable when only the inner product with a given vector is needed. We show that Krylov subspace learning works well both in matrix compression and implicit calculation of the inner product by taking full advantage of the arbitrariness of the seed vector. We apply our algorithm to a PCA-based change-point detection algorithm, and show that it results in about 50 times improvement in computational time.},
  isbn = {978-0-89871-630-6 978-1-61197-277-1},
  langid = {english},
  file = {/home/yanni/Zotero/storage/UJWSPBP9/Id√© and Tsuda - 2007 - Change-Point Detection using Krylov Subspace Learn.pdf}
}

@inproceedings{ideKnowledgeDiscoveryHeterogeneous2005,
  title = {Knowledge {{Discovery}} from {{Heterogeneous Dynamic Systems}} Using {{Change-Point Correlations}}},
  booktitle = {Proceedings of the 2005 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Id{\'e}, Tsuyoshi and Inoue, Keisuke},
  year = {2005},
  month = apr,
  pages = {571--575},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972757.63},
  abstract = {Most of the stream mining techniques presented so far have primary paid attention to discovering association rules by direct comparison between time-series data sets. However, their utility is very limited for heterogeneous systems, where time series of various types (discrete, continuous, oscillatory, noisy, etc.) act dynamically in a strongly correlated manner. In this paper, we introduce a new nonlinear transformation, singular spectrum transformation (SST), to address the problem of knowledge discovery of causal relationships from a set of time series. SST is a transformation that transforms a time series into the probability density function that represents a chance to observe some particular change. For an automobile data set, we demonstrate that SST enables us to discover a hidden and useful dependency between variables.},
  isbn = {978-0-89871-593-4 978-1-61197-275-7},
  langid = {english},
  file = {/home/yanni/Zotero/storage/HGXHYKLT/Id√© and Inoue - 2005 - Knowledge Discovery from Heterogeneous Dynamic Sys.pdf}
}

@inproceedings{ideKnowledgeDiscoveryHeterogeneous2005a,
  title = {Knowledge {{Discovery}} from {{Heterogeneous Dynamic Systems}} Using {{Change-Point Correlations}}},
  booktitle = {Proceedings of the 2005 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Id{\'e}, Tsuyoshi and Inoue, Keisuke},
  year = {2005},
  month = apr,
  pages = {571--575},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972757.63},
  abstract = {Most of the stream mining techniques presented so far have primary paid attention to discovering association rules by direct comparison between time-series data sets. However, their utility is very limited for heterogeneous systems, where time series of various types (discrete, continuous, oscillatory, noisy, etc.) act dynamically in a strongly correlated manner. In this paper, we introduce a new nonlinear transformation, singular spectrum transformation (SST), to address the problem of knowledge discovery of causal relationships from a set of time series. SST is a transformation that transforms a time series into the probability density function that represents a chance to observe some particular change. For an automobile data set, we demonstrate that SST enables us to discover a hidden and useful dependency between variables.},
  isbn = {978-0-89871-593-4 978-1-61197-275-7},
  langid = {english},
  file = {/home/yanni/Zotero/storage/KYIU24PL/Id√© and Inoue - 2005 - Knowledge Discovery from Heterogeneous Dynamic Sys.pdf}
}

@article{IdentifyingBullBear2021,
  title = {Identifying {{Bull}} and {{Bear Markets}} in {{Stock Returns}}},
  year = {2021},
  pages = {14},
  langid = {english},
  file = {/home/yanni/Zotero/storage/6TZ5X34V/2021 - Identifying Bull and Bear Markets in Stock Returns.pdf}
}

@techreport{innesDifferentiableProgrammingSystem,
  title = {{$\partial$}{{P}} : {{A Differentiable Programming System}} to {{Bridge Machine Learning}} and {{Scientific Computing}}},
  author = {Innes, Mike and Edelman, Alan and Fischer Julia Computing Chris Rackauckas, Keno and Saba Julia Computing Viral Shah Julia Computing Will Tebbutt, Elliot B},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe a Differentiable Programming ({$\partial$}P) system that is able to take gradients of Julia programs making Automatic Differentiation a first class language feature. Our system supports almost all language constructs (control flow, recur-sion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning and, more importantly, it enables users to utilize the existing Julia ecosystem of scientific computing packages in deep learning models. We discuss support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present how this leads to efficient code generation. We then showcase several examples of differentiating programs and mixing deep learning with existing Julia packages, including differentiable ray tracing, machine learning on simulated quantum hardware, training neural stochastic differential equation representations of financial models and more.},
  file = {/home/yanni/Zotero/storage/QS9II5ED/Innes et al. - Unknown - ‚àÇP A Differentiable Programming System to Bridge Machine Learning and Scientific Computing.pdf}
}

@article{jacksonAlgorithmOptimalPartitioning2005,
  title = {An {{Algorithm}} for {{Optimal Partitioning}} of {{Data}} on an {{Interval}}},
  author = {Jackson, Brad and Scargle, Jeffrey D. and Barnes, David and Arabhi, Sundararajan and Alt, Alina and Gioumousis, Peter and Gwin, Elyus and Sangtrakulcharoen, Paungkaew and Tan, Linda and Tsai, Tun Tao},
  year = {2005},
  month = feb,
  journal = {IEEE Signal Process. Lett.},
  volume = {12},
  number = {2},
  eprint = {math/0309285},
  pages = {105--108},
  issn = {1070-9908},
  doi = {10.1109/LSP.2001.838216},
  abstract = {Many signal processing problems can be solved by maximizing the fitness of a segmented model over all possible partitions of the data interval. This letter describes a simple but powerful algorithm that searches the exponentially large space of partitions of N data points in time O(N 2). The algorithm is guaranteed to find the exact global optimum, automatically determines the model order (the number of segments), has a convenient real-time mode, can be extended to higher dimensional data spaces, and solves a surprising variety of problems in signal detection and characterization, density estimation, cluster analysis and classification.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {65C60,Astrophysics,Computer Science - Computational Engineering; Finance; and Science,Computer Science - Data Structures and Algorithms,Computer Science - Information Theory,Mathematics - Combinatorics,Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/S3KGK8AR/Jackson et al. - 2005 - An Algorithm for Optimal Partitioning of Data on a.pdf}
}

@techreport{kalmanNewApproachLinear1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, R E},
  year = {1960},
  journal = {Transactions of the ASME-Journal of Basic Engineering},
  volume = {82},
  pages = {35--45},
  abstract = {Introduction AN IMPORTANT class of theoretical and practical problems in communication and control is of a statistical nature. Such problems are: (i) Prediction of random signals; (ii) separation of random signals from random noise; (iii) detection of signals of known form (pulses, sinusoids) in the presence of random noise. In his pioneering work, Wiener [1] 3 showed that problems (i) and (ii) lead to the so-called Wiener-Hopf integral equation; he also gave a method (spectral factorization) for the solution of this integral equation in the practically important special case of stationary statistics and rational spectra. Many extensions and generalizations followed Wiener's basic work. Zadeh and Ragazzini solved the finite-memory case [2]. Concurrently and independently of Bode and Shannon [3], they also gave a simplified method [2] of solution. Booton discussed the nonstationary Wiener-Hopf equation [4]. These results are now in standard texts [5-6]. A somewhat different approach along these main lines has been given recently by Darlington [7]. For extensions to sampled signals, see, e.g., Franklin [8], Lees [9]. Another approach based on the eigenfunctions of the Wiener-Hopf equation (which applies also to nonstationary problems whereas the preceding methods in general don't), has been pioneered by Davis [10] and applied by many others, e.g., Shinbrot [11], Blum [12], Pugachev [13], Solodovnikov [14]. In all these works, the objective is to obtain the specification of a linear dynamic system (Wiener filter) which accomplishes the prediction, separation, or detection of a random signal. 4-Present methods for solving the Wiener problem are subject to a number of limitations which seriously curtail their practical usefulness: (1) The optimal filter is specified by its impulse response. It is not a simple task to synthesize the filter from such data. (2) Numerical determination of the optimal impulse response is often quite involved and poorly suited to machine computation. The situation gets rapidly worse with increasing complexity of the problem. (3) Important generalizations (e.g., growing-memory filters, nonstationary prediction) require new derivations, frequently of considerable difficulty to the nonspecialist. (4) The mathematics of the derivations are not transparent. Fundamental assumptions and their consequences tend to be obscured. This paper introduces a new look at this whole assemblage of problems, sidestepping the difficulties just mentioned. The following are the highlights of the paper: (5) Optimal Estimates and Orthogonal Projections. The Wiener problem is approached from the point of view of conditional distributions and expectations. In this way, basic facts of the Wiener theory are quickly obtained; the scope of the results and the fundamental assumptions appear clearly. It is seen that all statistical calculations and results are based on first and second order averages; no other statistical data are needed. Thus difficulty (4) is eliminated. This method is well known in probability theory (see pp. 75-78 and 148-155 of Doob [15] and pp. 455-464 of Lo\`eve [16]) but has not yet been used extensively in engineering. (6) Models for Random Processes. Following, in particular, Bode and Shannon [3], arbitrary random signals are represented (up to second order average statistical properties) as the output of a linear dynamic system excited by independent or uncorrelated random signals ("white noise"). This is a standard trick in the engineering applications of the Wiener theory [2-7]. The approach taken here differs from the conventional one only in the way in which linear dynamic systems are described. We shall emphasize the concepts of state and state transition; in other words, linear systems will be specified by systems of first-order difference (or differential) equations. This point of view is A New Approach to Linear Filtering and Prediction Problems 1 The classical filtering and prediction problem is reexamined using the Bode-Shannon representation of random processes and the "state transition" method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
  file = {/home/yanni/Zotero/storage/NMVTT4CQ/Kalman - 1960 - (No Title).pdf}
}

@misc{kanagawaGaussianProcessesKernel2018,
  title = {Gaussian {{Processes}} and {{Kernel Methods}}: {{A Review}} on {{Connections}} and {{Equivalences}}},
  shorttitle = {Gaussian {{Processes}} and {{Kernel Methods}}},
  author = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K.},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02582},
  eprint = {1807.02582},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {This paper is an attempt to bridge the conceptual gaps between researchers working on the two widely used approaches based on positive definite kernels: Bayesian learning or inference using Gaussian processes on the one side, and frequentist kernel methods based on reproducing kernel Hilbert spaces on the other. It is widely known in machine learning that these two formalisms are closely related; for instance, the estimator of kernel ridge regression is identical to the posterior mean of Gaussian process regression. However, they have been studied and developed almost independently by two essentially separate communities, and this makes it difficult to seamlessly transfer results between them. Our aim is to overcome this potential difficulty. To this end, we review several old and new results and concepts from either side, and juxtapose algorithmic quantities from each framework to highlight close similarities. We also provide discussions on subtle philosophical and theoretical differences between the two approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/BTCB37SN/Kanagawa et al_2018_Gaussian Processes and Kernel Methods.pdf}
}

@article{kanagawaGaussianProcessesKernel2018a,
  title = {Gaussian Processes and Kernel Methods: {{A}} Review on Connections and Equivalences},
  author = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K.},
  year = {2018},
  journal = {arXiv},
  pages = {1--64},
  issn = {23318422},
  abstract = {This paper is an attempt to bridge the conceptual gaps between researchers working on the two widely used approaches based on positive definite kernels: Bayesian learning or inference using Gaussian processes on the one side, and frequentist kernel methods based on reproducing kernel Hilbert spaces on the other. It is widely known in machine learning that these two formalisms are closely related; for instance, the estimator of kernel ridge regression is identical to the posterior mean of Gaussian process regression. However, they have been studied and developed almost independently by two essentially separate communities, and this makes it difficult to seamlessly transfer results between them. Our aim is to overcome this potential difficulty. To this end, we review several old and new results and concepts from either side, and juxtapose algorithmic quantities from each framework to highlight close similarities. We also provide discussions on subtle philosophical and theoretical differences between the two approaches.},
  file = {/home/yanni/Zotero/storage/8TFLU2T3/1807.02582.pdf}
}

@article{karvonenErrorAnalysisStatistical2022,
  title = {Error Analysis for a Statistical Finite Element Method},
  author = {Karvonen, Toni and Cirak, Fehmi and Girolami, Mark},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.07543 [cs, math, stat]},
  eprint = {2201.07543},
  primaryclass = {cs, math, stat},
  abstract = {The recently proposed statistical finite element (statFEM) approach synthesises measurement data with finite element models and allows for making predictions about the true system response. We provide a probabilistic error analysis for a prototypical statFEM setup based on a Gaussian process prior under the assumption that the noisy measurement data are generated by a deterministic true system response function that satisfies a second-order elliptic partial differential equation for an unknown true source term. In certain cases, properties such as the smoothness of the source term may be misspecified by the Gaussian process model. The error estimates we derive are for the expectation with respect to the measurement noise of the \$L\^2\$-norm of the difference between the true system response and the mean of the statFEM posterior. The estimates imply polynomial rates of convergence in the numbers of measurement points and finite element basis functions and depend on the Sobolev smoothness of the true source term and the Gaussian process model. A numerical example for Poisson's equation is used to illustrate these theoretical results.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory},
  file = {/home/yanni/Zotero/storage/BH7BKAU3/Karvonen et al. - 2022 - Error analysis for a statistical finite element me.pdf;/home/yanni/Zotero/storage/2JKEGEZS/2201.html}
}

@article{kennedyBayesianCalibrationComputer2001,
  title = {Bayesian Calibration of Computer Models},
  author = {Kennedy, Marc C. and O'Hagan, Anthony},
  year = {2001},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {63},
  number = {3},
  pages = {425--464},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00294},
  abstract = {We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.},
  langid = {english},
  keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
  file = {/home/yanni/Zotero/storage/ELF5WF4B/Kennedy and O'Hagan - 2001 - Bayesian calibration of computer models.pdf;/home/yanni/Zotero/storage/ZJFWLFH6/1467-9868.html}
}

@techreport{kerstingActiveUncertaintyCalibration2016,
  title = {Active {{Uncertainty Calibration}} in {{Bayesian ODE Solvers}}},
  author = {Kersting, Hans and Hennig, Philipp},
  year = {2016},
  abstract = {There is resurging interest, in statistics and machine learning, in solvers for ordinary differential equations (ODEs) that return probability measures instead of point estimates. Recently, Con-rad et al. introduced a sampling-based class of methods that are 'well-calibrated' in a specific sense. But the computational cost of these methods is significantly above that of classic methods. On the other hand, Schober et al. pointed out a precise connection between classic Runge-Kutta ODE solvers and Gaussian filters, which gives only a rough probabilistic calibration, but at negligible cost overhead. By formulating the solution of ODEs as approximate inference in linear Gaussian SDEs, we investigate a range of probabilistic ODE solvers, that bridge the trade-off between computational cost and probabilistic calibration, and identify the inaccurate gradient measurement as the crucial source of uncertainty. We propose the novel filtering-based method Bayesian Quadrature filtering (BQF) which uses Bayesian quadrature to actively learn the impre-cision in the gradient measurement by collecting multiple gradient evaluations.},
  isbn = {1605.03364v2},
  file = {/home/yanni/Zotero/storage/P9QD464J/Kersting, Hennig - 2016 - Active Uncertainty Calibration in Bayesian ODE Solvers.pdf}
}

@techreport{kerstingConvergenceRatesGaussian,
  title = {Convergence {{Rates}} of {{Gaussian ODE Filters}}},
  author = {Kersting, Hans and Sullivan, T J and Hennig, Philipp},
  abstract = {A recently-introduced class of probabilistic (uncertainty-aware) solvers for ordinary differential equations (ODEs) applies Gaussian (Kalman) filtering to initial value problems. These methods model the true solution x and its first q derivatives a priori as a Gauss-Markov process X, which is then iteratively conditioned on information about \textperiodcentered{} x. This article establishes worst-case local convergence rates of order q + 1 for a wide range of versions of this Gaussian ODE filter, as well as global convergence rates of order q in the case of q = 1 and an integrated Brownian motion prior, and analyses how inaccurate information on \textperiodcentered{} x coming from approximate evaluations of f affects these rates. Moreover, we show that, in the globally convergent case, the posterior credible intervals are well calibrated in the sense that they globally contract at the same rate as the truncation error. We illustrate these theoretical results by numerical experiments which suggest their generalizability to q {$\in$} 2, 3, 4,. .. .},
  keywords = {60J70,62G20,62M05,65C20,65L05,Gaussian processes,initial value problems,Markov processes AMS subject classifications 60G15,numerical analysis,ordinary differential equations,probabilistic numerics},
  file = {/home/yanni/Zotero/storage/VF2KIZAL/Kersting, Sullivan, Hennig - Unknown - Convergence Rates of Gaussian ODE Filters.pdf}
}

@article{killickOptimalDetectionChangepoints2012,
  title = {Optimal Detection of Changepoints with a Linear Computational Cost},
  author = {Killick, R. and Fearnhead, P. and Eckley, I. A.},
  year = {2012},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {107},
  number = {500},
  eprint = {1101.1438},
  pages = {1590--1598},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2012.737745},
  abstract = {We consider the problem of detecting multiple changepoints in large data sets. Our focus is on applications where the number of changepoints will increase as we collect more data: for example in genetics as we analyse larger regions of the genome, or in finance as we observe time-series over longer periods. We consider the common approach of detecting changepoints through minimising a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalised likelihood and minimum description length. We introduce a new {${_\ast}$}R. Killick is Senior Research Associate, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: r.killick@lancs.ac.uk). P. Fearnhead is Professor, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: p.fearnhead@lancs.ac.uk). I.A. Eckley is Senior Lecturer, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: i.eckley@lancs.ac.uk). The authors are grateful to Richard Davis and Alice Cleynen for providing the Auto-PARM and PDPA software respectively. Part of this research was conducted whilst R. Killick was a jointly funded Engineering and Physical Sciences Research Council (EPSRC) / Shell Research Ltd graduate student at Lancaster University. Both I.A. Eckley and R. Killick also gratefully acknowledge the financial support of the EPSRC grant number EP/I016368/1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Quantitative Biology - Genomics,Quantitative Biology - Quantitative Methods,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/VV68G8LF/Killick et al. - 2012 - Optimal detection of changepoints with a linear co.pdf}
}

@inproceedings{kimReadingDocumentsBayesian2015,
  title = {Reading {{Documents}} for {{Bayesian Online Change Point Detection}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kim, Taehoon and Choi, Jaesik},
  year = {2015},
  pages = {1610--1619},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/D15-1184},
  abstract = {Modeling non-stationary time-series data for making predictions is a challenging but important task. One of the key issues is to identify long-term changes accurately in time-varying data. Bayesian Online Change Point Detection (BO-CPD) algorithms efficiently detect long-term changes without assuming the Markov property which is vulnerable to local signal noise. We propose a Document based BO-CPD (DBO-CPD) model which automatically detects long-term temporal changes of continuous variables based on a novel dynamic Bayesian analysis which combines a non-parametric regression, the Gaussian Process (GP), with generative models of texts such as news articles and posts on social networks. Since texts often include important clues of signal changes, DBO-CPD enables the accurate prediction of long-term changes accurately. We show that our algorithm outperforms existing BO-CPDs in two real-world datasets: stock prices and movie revenues.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/V66449FI/Kim and Choi - 2015 - Reading Documents for Bayesian Online Change Point.pdf}
}

@techreport{kingmaADAMMETHODSTOCHASTIC,
  title = {{{ADAM}}: {{A METHOD FOR STOCHASTIC OPTIMIZATION}}},
  author = {Kingma, Diederik P and Lei Ba, Jimmy},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  file = {/home/yanni/Zotero/storage/SWWY4RTU/Kingma, Lei Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION(2).pdf}
}

@article{kobinAnalysisBanachSpaces,
  title = {Analysis of {{Banach Spaces}}},
  author = {Kobin, Andrew},
  pages = {67},
  langid = {english},
  file = {/home/yanni/Zotero/storage/IRTV9GIX/Kobin - Analysis of Banach Spaces.pdf}
}

@article{kondorKernelSetsVectors,
  title = {A {{Kernel Between Sets}} of {{Vectors}}},
  author = {Kondor, Risi and Jebara, Tony},
  pages = {8},
  abstract = {In various application domains, including image recognition, it is natural to represent each example as a set of vectors. With a base kernel we can implicitly map these vectors to a Hilbert space and fit a Gaussian distribution to the whole set using Kernel PCA. We define our kernel between examples as Bhattacharyya's measure of affinity between such Gaussians. The resulting kernel is computable in closed form and enjoys many favorable properties, including graceful behavior under transformations, potentially justifying the vector set representation even in cases when more conventional representations also exist.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/W25MGLM9/Kondor and Jebara - A Kernel Between Sets of Vectors.pdf}
}

@article{koolenMinimaxTimeSeries2015,
  title = {Minimax Time Series Prediction},
  author = {Koolen, Wouter M. and Bartlett, Peter L. and Malek, Alan and {Abbasi-Yadkori}, Yasin},
  year = {2015},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2015-Janua},
  number = {1},
  pages = {2557--2565},
  issn = {10495258},
  abstract = {We consider an adversarial formulation of the problem of predicting a time series with square loss. The aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect. Our approach allows natural measures of smoothness such as the squared norm of increments. More generally, we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms. We derive the minimax strategy for all problems of this type and show that it can be implemented efficiently. The optimal predictions are linear in the previous observations. We obtain an explicit expression for the regret in terms of the parameters defining the problem. For typical, simple definitions of smoothness, the computation of the optimal predictions involves only sparse matrices. In the case of norm-constrained data, where the smoothness is defined in terms of the squared norm of the comparator's increments, we show that the regret grows as T/{$\surd\Lambda$}T, where T is the length of the game and {$\Lambda$}T is an increasing limit on comparator smoothness.},
  file = {/home/yanni/Zotero/storage/FWRKL5TM/24076D.pdf}
}

@book{kreyszigIntroductoryFunctionalAnalysis1978,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig, Erwin},
  year = {1978},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-50731-4},
  langid = {english},
  lccn = {QA320 .K74},
  keywords = {Functional analysis},
  file = {/home/yanni/Zotero/storage/MRVYCKTK/Kreyszig - 1978 - Introductory functional analysis with applications.pdf}
}

@article{kuchibhotlaAllLinearRegression2019,
  title = {All of {{Linear Regression}}},
  author = {Kuchibhotla, Arun K. and Brown, Lawrence D. and Buja, Andreas and Cai, Junhui},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.06386 [math, stat]},
  eprint = {1910.06386},
  primaryclass = {math, stat},
  abstract = {Least squares linear regression is one of the oldest and widely used data analysis tools. Although the theoretical analysis of ordinary least squares (OLS) estimator is as old, several fundamental questions are yet to be answered. Suppose regression observations pX1, Y1q, . . . , pXn, Ynq P Rd \textasciicircum{} R (not necessarily independent) are available. Some of the questions we deal with are as follows: under what conditions, does the OLS estimator converge and what is the limit? What happens if the dimension is allowed to grow with n? What happens if the observations are dependent with dependence possibly strengthening with n? How to do statistical inference under these kinds of misspecification? What happens to OLS estimator under variable selection? How to do inference under misspecification and variable selection? We answer all the questions raised above with one simple deterministic inequality which holds for any set of observations and any sample size. This implies that all our results are finite sample (non-asymptotic) in nature. At the end, one only needs to bound certain random quantities under specific settings of interest to get concrete rates and we derive these bounds for the case of independent observations. In particular the problem of inference after variable selection is studied, for the first time, when d, the number of covariates increases (almost exponentially) with sample size n. We provide comments on the ``right'' statistic to consider for inference under variable selection and efficient computation of quantiles.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/L3MNTVF7/Kuchibhotla et al. - 2019 - All of Linear Regression.pdf}
}

@book{kunitaStochasticFlowsStochastic1997,
  title = {Stochastic Flows and Stochastic Differential Equations},
  author = {Kunita, Hiroshi},
  year = {1997},
  volume = {24},
  publisher = {{Cambridge university press}}
}

@article{laiSIMPLERGRASSMANNIANOPTIMIZATION,
  title = {{{SIMPLER GRASSMANNIAN OPTIMIZATION}}},
  author = {Lai, Zehua and Lim, Lek-Heng and Ye, Ke},
  pages = {36},
  abstract = {There are two widely used models for the Grassmannian Gr(k, n), as the set of equivalence classes of orthogonal matrices O(n)/ O(k) \texttimes{} O(n - k) , and as the set of trace-k projection matrices \{P {$\in$} Rn\texttimes n : P T = P = P 2, tr(P ) = k\}. The former, standard in manifold optimization, has the downside of relying on equivalence classes but working with orthogonal matrices is generally good numerical practice. The latter, widely adopted in coding theory and probability, uses actual matrices (as opposed to equivalence classes) but working with projection matrices is numerically unstable. We present an alternative that has both advantages and suffers from neither of the disadvantages; by representing k-dimensional subspaces as symmetric orthogonal matrices of trace 2k - n, we obtain Gr(k, n) {$\sim$}= \{Q {$\in$} O(n) : QT = Q, tr(Q) = 2k - n\}. As with the other two models, we show that differential geometric objects and operations \textemdash{} tangent vector, metric, normal vector, exponential map, geodesic, parallel transport, gradient, Hessian, etc \textemdash{} have closed-form analytic expressions that are computable with standard numerical linear algebra. In the proposed model, these expressions are considerably simpler, a result of representing Gr(k, n) as a linear section of a compact matrix Lie group O(n), and can be computed with at most one qr decomposition and one exponential of a special skew-symmetric matrix that takes only O nk(n - k) time. In particular, we completely avoid eigen- and singular value decompositions in our steepest descent, conjugate gradient, quasi-Newton, and Newton methods for the Grassmannian. Another important feature of these algorithms, particularly evident in steepest descent and Newton method, is that they exhibit clear signs of numerical stability; various measures of errors consistently reduce to the order of machine precision throughout extensive numerical experiments.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/FJCZRNLJ/Lai et al. - SIMPLER GRASSMANNIAN OPTIMIZATION.pdf}
}

@article{laroccaGroupInvariantQuantumMachine2022,
  title = {Group-{{Invariant Quantum Machine Learning}}},
  author = {Larocca, Martin and Sauvage, Frederic and Sbahi, Faris M. and Verdon, Guillaume and Coles, Patrick J. and Cerezo, M.},
  year = {2022},
  month = may,
  journal = {arXiv:2205.02261 [quant-ph, stat]},
  eprint = {2205.02261},
  primaryclass = {quant-ph, stat},
  abstract = {Quantum Machine Learning (QML) models are aimed at learning from data encoded in quantum states. Recently, it has been shown that models with little to no inductive biases (i.e., with no assumptions about the problem embedded in the model) are likely to have trainability and generalization issues, especially for large problem sizes. As such, it is fundamental to develop schemes that encode as much information as available about the problem at hand. In this work we present a simple, yet powerful, framework where the underlying invariances in the data are used to build QML models that, by construction, respect those symmetries. These so-called group-invariant models produce outputs that remain invariant under the action of any element of the symmetry group \$\textbackslash mathfrak\{G\}\$ associated to the dataset. We present theoretical results underpinning the design of \$\textbackslash mathfrak\{G\}\$-invariant models, and exemplify their application through several paradigmatic QML classification tasks including cases when \$\textbackslash mathfrak\{G\}\$ is a continuous Lie group and also when it is a discrete symmetry group. Notably, our framework allows us to recover, in an elegant way, several well known algorithms for the literature, as well as to discover new ones. Taken together, we expect that our results will help pave the way towards a more geometric and group-theoretic approach to QML model design.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantum Physics,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/U7GRDG5D/Larocca et al. - 2022 - Group-Invariant Quantum Machine Learning.pdf;/home/yanni/Zotero/storage/V7A8ZASS/2205.html}
}

@article{lavinSimulationIntelligenceNew2021,
  title = {Simulation {{Intelligence}}: {{Towards}} a {{New Generation}} of {{Scientific Methods}}},
  shorttitle = {Simulation {{Intelligence}}},
  author = {Lavin, Alexander and Zenil, Hector and Paige, Brooks and Krakauer, David and Gottschlich, Justin and Mattson, Tim and Anandkumar, Anima and Choudry, Sanjay and Rocki, Kamil and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Prunkl, Carina and Paige, Brooks and Isayev, Olexandr and Peterson, Erik and McMahon, Peter L. and Macke, Jakob and Cranmer, Kyle and Zhang, Jiaxin and Wainwright, Haruko and Hanuka, Adi and Veloso, Manuela and Assefa, Samuel and Zheng, Stephan and Pfeffer, Avi},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.03235 [cs]},
  eprint = {2112.03235},
  primaryclass = {cs},
  abstract = {The original "Seven Motifs" set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the "Nine Motifs of Simulation Intelligence", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning,Computer Science - Mathematical Software},
  file = {/home/yanni/Zotero/storage/HZU3AUQY/Lavin et al. - 2021 - Simulation Intelligence Towards a New Generation .pdf;/home/yanni/Zotero/storage/BSIBRDA6/2112.html}
}

@techreport{lawDataAssimilation,
  title = {Data {{Assimilation}}},
  author = {Law, Kody and Stuart, Andrew and Zygalakis, Konstantinos},
  file = {/home/yanni/Zotero/storage/5G77J5W4/Law, Stuart, Zygalakis - Unknown - Data Assimilation.pdf}
}

@article{leanderStochasticDifferentialEquations2014,
  title = {Stochastic Differential Equations as a Tool to Regularize the Parameter Estimation Problem for Continuous Time Dynamical Systems given Discrete Time Measurements},
  author = {Leander, Jacob and Lundh, Torbj{\"o}rn and Jirstrand, Mats},
  year = {2014},
  journal = {Mathematical Biosciences},
  issn = {18793134},
  doi = {10.1016/j.mbs.2014.03.001},
  abstract = {In this paper we consider the problem of estimating parameters in ordinary differential equations given discrete time experimental data. The impact of going from an ordinary to a stochastic differential equation setting is investigated as a tool to overcome the problem of local minima in the objective function. Using two different models, it is demonstrated that by allowing noise in the underlying model itself, the objective functions to be minimized in the parameter estimation procedures are regularized in the sense that the number of local minima is reduced and better convergence is achieved. The advantage of using stochastic differential equations is that the actual states in the model are predicted from data and this will allow the prediction to stay close to data even when the parameters in the model is incorrect. The extended Kalman filter is used as a state estimator and sensitivity equations are provided to give an accurate calculation of the gradient of the objective function. The method is illustrated using in silico data from the FitzHugh-Nagumo model for excitable media and the Lotka-Volterra predator-prey system. The proposed method performs well on the models considered, and is able to regularize the objective function in both models. This leads to parameter estimation problems with fewer local minima which can be solved by efficient gradient-based methods. \textcopyright{} 2014 The Authors.},
  keywords = {Extended Kalman filter,FitzHugh-Nagumo,Lotka-Volterra,Ordinary differential equations,Parameter estimation,Stochastic differential equations},
  file = {/home/yanni/Zotero/storage/W6VSNMPK/Leander, Lundh, Jirstrand - 2014 - Stochastic differential equations as a tool to regularize the parameter estimation problem for contin.pdf}
}

@book{leeIntroductionRiemannianManifolds2018,
  title = {Introduction to {{Riemannian Manifolds}}},
  author = {Lee, John M.},
  year = {2018},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {176},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-91755-9},
  isbn = {978-3-319-91754-2 978-3-319-91755-9},
  langid = {english},
  file = {/home/yanni/Zotero/storage/SY72F5VQ/Lee - 2018 - Introduction to Riemannian Manifolds.pdf}
}

@book{leeIntroductionRiemannianManifolds2018a,
  title = {Introduction to {{Riemannian Manifolds}}},
  author = {Lee, John M.},
  year = {2018},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {176},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-91755-9},
  isbn = {978-3-319-91754-2 978-3-319-91755-9},
  langid = {english},
  file = {/home/yanni/Zotero/storage/WH26PUF4/Lee - 2018 - Introduction to Riemannian Manifolds.pdf}
}

@book{leeIntroductionRiemannianManifolds2018b,
  title = {Introduction to {{Riemannian Manifolds}}},
  author = {Lee, John M.},
  year = {2018},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {176},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-91755-9},
  isbn = {978-3-319-91754-2 978-3-319-91755-9},
  langid = {english},
  file = {/home/yanni/Zotero/storage/8765DM7T/Lee - 2018 - Introduction to Riemannian Manifolds.pdf}
}

@article{leibfriedTutorialSparseGaussian2021,
  title = {A {{Tutorial}} on {{Sparse Gaussian Processes}} and {{Variational Inference}}},
  author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
  year = {2021},
  month = jul,
  journal = {arXiv:2012.13962 [cs, stat]},
  eprint = {2012.13962},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/E39RS5DS/Leibfried et al. - 2021 - A Tutorial on Sparse Gaussian Processes and Variat.pdf;/home/yanni/Zotero/storage/HE6X25FD/2012.html}
}

@article{leisSensitivityAnalysisSystems1985,
  title = {Sensitivity Analysis of Systems of Differential and Algebraic Equations},
  author = {Leis, Jorge R and Kramer, Mark A},
  year = {1985},
  journal = {Computers \& chemical engineering},
  volume = {9},
  number = {1},
  pages = {93--96},
  publisher = {{Elsevier}}
}

@article{leisSensitivityAnalysisSystems1985a,
  title = {Sensitivity Analysis of Systems of Differential and Algebraic Equations},
  author = {Leis, Jorge R. and Kramer, Mark A.},
  year = {1985},
  journal = {Computers and Chemical Engineering},
  issn = {00981354},
  doi = {10.1016/0098-1354(85)87008-3},
  abstract = {Formulae are derived for parametric sensitivity analysis of mathematical mo dels consisting of sets of differential and algebraic equations. Such equations often arise in dynamic modeling of equilibrium stage processes, and in solution of partial differential equations via the numerical method of lines. These formulae can be used to efficiently produce the model sensitivity coefficients, simultaneously with the solution of the model. \textcopyright{} 1985.},
  file = {/home/yanni/Zotero/storage/LU5URR6W/Leis, Kramer - 1985 - Sensitivity analysis of systems of differential and algebraic equations.pdf}
}

@techreport{leisSimultaneousSolutionSensitivity,
  title = {The {{Simultaneous Solution}} and {{Sensitivity Analysis}} of {{Systems Described}} by {{Ordinary Differential Equations}}},
  author = {Leis, Jorge R and Kramer, Mark A},
  abstract = {The methodology for the simultaneous solution of ordinary differential equations and the associated first-order parametric sensitivity equations is presented, and a detailed description of its implementation as a modification of a widely disseminated implicit ODE solver is given. The error control strategy ensures that local error criteria are independently satisfied by both the model and sensitivity solutions. The internal logic effectuated by this implementation is detailed. Numerical testing of the algorithm is reported, results indicate that greater reliability and improved efficiency is offered over other sensitivity analysis methods.},
  keywords = {164 [Simulation and Modeling]: Model Validation and Analysis,error analysis,G17 [Numerical Analysis]: Ordinary Differential Equations-LSODE,G4 [Mathematics of Computing]: Mathematical Software-efficiency General Terms: Algorithms,initial value problems,Measurement,model prediction uncertainty,ODESSA,parameter variation,Performance Additional Key Words and Phrases: Model error,sensitivity analysis,stiff equations},
  file = {/home/yanni/Zotero/storage/G53P7TE4/Leis, Kramer - Unknown - The Simultaneous Solution and Sensitivity Analysis of Systems Described by Ordinary Differential Equations.pdf}
}

@article{leisSimultaneousSolutionSensitivity1988,
  title = {The Simultaneous Solution and Sensitivity Analysis of Systems Described by Ordinary Differential Equations},
  author = {Leis, Jorge R and Kramer, Mark A},
  year = {1988},
  journal = {ACM Transactions on Mathematical Software (TOMS)},
  volume = {14},
  number = {1},
  pages = {45--60},
  publisher = {{ACM}}
}

@article{liangParameterEstimationDifferential2008,
  title = {Parameter {{Estimation}} for {{Differential Equation Models Using}} a {{Framework}} of {{Measurement Error}} in {{Regression Models}}},
  author = {Liang, Hua and Wu, Hulin},
  year = {2008},
  issn = {0162-1459},
  doi = {10.1198/016214508000000797},
  abstract = {Differential equation (DE) models are widely used in many scientific fields, including engineering, physics, and biomedical sciences. The so-called "forward problem," the problem of simulations and predictions of state variables for given parameter values in the DE models, has been extensively studied by mathematicians, physicists, engineers, and other scientists. However, the "inverse problem," the problem of parameter estimation based on the measurements of output variables, has not been well explored using modern statistical methods, although some least squares-based approaches have been proposed and studied. In this article we propose parameter estimation methods for ordinary differential equation (ODE) models based on the local smoothing approach and a pseudo-least squares (PsLS) principle under a framework of measurement error in regression models. The asymptotic properties of the proposed PsLS estimator are established. We also compare the PsLS method to the corresponding simulation-extrapolation (SIMEX) method and evaluate their finite-sample performances via simulation studies. We illustrate the proposed approach using an application example from an HIV dynamic study.},
  keywords = {AIDS,HIV viral dynamics,Local polynomial smoothing,Measurement error models,Nonparametric regression,Ordi-nary differential equations,Principal differential analysis,Regression calibration,SIMEX},
  file = {/home/yanni/Zotero/storage/PUKQ7FKI/Liang, Wu - 2008 - Parameter Estimation for Differential Equation Models Using a Framework of Measurement Error in Regression Models.pdf}
}

@article{lieStrongConvergenceRates,
  title = {Strong Convergence Rates of Probabilistic Integrators for Ordinary Differential Equations},
  author = {Lie, Han Cheng and Stuart, {$\cdot$} A M and Sullivan, {$\cdot$} T J and Stuart, A M and Sullivan, T J},
  doi = {10.1007/s11222-019-09898-6},
  abstract = {Probabilistic integration of a continuous dy-namical system is a way of systematically introducing discretisation error, at scales no larger than errors introduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic in-tegrators for deterministic ordinary differential equations , as proposed by Conrad et al. (Stat. Comput., 2017), to establish mean-square convergence in the uniform norm on discrete-or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator. These and similar results are proven for probabilistic integrators where the random perturbations may be state-dependent, non-Gaussian, or non-centred random variables.},
  keywords = {Mathematics,Subject},
  file = {/home/yanni/Zotero/storage/REDGIC3I/Lie et al. - Unknown - Strong convergence rates of probabilistic integrators for ordinary differential equations.pdf}
}

@misc{liFourierNeuralOperator2021,
  title = {Fourier {{Neural Operator}} for {{Parametric Partial Differential Equations}}},
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  year = {2021},
  month = may,
  number = {arXiv:2010.08895},
  eprint = {arXiv:2010.08895},
  publisher = {{arXiv}},
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/9GNG8R8G/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf;/home/yanni/Zotero/storage/ECTENMIR/2010.html}
}

@book{lifshitsLecturesGaussianProcesses2012,
  title = {Lectures on {{Gaussian Processes}}},
  author = {Lifshits, Mikhail},
  year = {2012},
  series = {{{SpringerBriefs}} in {{Mathematics}}},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-24939-6},
  isbn = {978-3-642-24938-9 978-3-642-24939-6},
  langid = {english},
  file = {/home/yanni/Zotero/storage/TVH2TBVU/Lifshits - 2012 - Lectures on Gaussian Processes.pdf}
}

@article{liMStatisticKernelChangePoint,
  title = {M-{{Statistic}} for {{Kernel Change-Point Detection}}},
  author = {Li, Shuang and Xie, Yao and Dai, Hanjun and Song, Le},
  pages = {9},
  abstract = {Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach. However, none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic. Such characterization is crucial for setting the detection threshold, to control the significance level in the offline case as well as the average run length in the online case. In this paper we propose two related computationally efficient M -statistics for kernel-based change-point detection when the amount of background data is large. A novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure. Such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner, without the need to resort to the more expensive simulations such as bootstrapping. We show that our methods perform well in both synthetic and real world data.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/ZNBNE7Q2/Li et al. - M-Statistic for Kernel Change-Point Detection.pdf}
}

@article{lindgrenExplicitLinkGaussian2011,
  title = {An Explicit Link between {{Gaussian}} Fields and {{Gaussian Markov}} Random Fields: The Stochastic Partial Differential Equation Approach},
  shorttitle = {An Explicit Link between {{Gaussian}} Fields and {{Gaussian Markov}} Random Fields},
  author = {Lindgren, Finn and Rue, H{\aa}vard and Lindstr{\"o}m, Johan},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {4},
  pages = {423--498},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2011.00777.x},
  abstract = {Summary. Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Mat\'ern class, provide an explicit link, for any triangulation of , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
  langid = {english},
  keywords = {Approximate Bayesian inference,Covariance functions,Gaussian fields,Gaussian Markov random fields,Latent Gaussian models,Sparse matrices,Stochastic partial differential equations},
  file = {/home/yanni/Zotero/storage/BNM7CTHG/Lindgren et al. - 2011 - An explicit link between Gaussian fields and Gauss.pdf;/home/yanni/Zotero/storage/2G3M545I/j.1467-9868.2011.00777.html}
}

@article{lindgrenExplicitLinkGaussian2011a,
  title = {An Explicit Link between {{Gaussian}} Fields and {{Gaussian Markov}} Random Fields: The Stochastic Partial Differential Equation Approach: {{Link}} between {{Gaussian Fields}} and {{Gaussian Markov Random Fields}}},
  shorttitle = {An Explicit Link between {{Gaussian}} Fields and {{Gaussian Markov}} Random Fields},
  author = {Lindgren, Finn and Rue, H{\aa}vard and Lindstr{\"o}m, Johan},
  year = {2011},
  month = sep,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {4},
  pages = {423--498},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2011.00777.x},
  abstract = {Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in R2 only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Mat\'ern class, provide an explicit link , for any triangulation of Rd , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/I8JNKSDU/Lindgren et al. - 2011 - An explicit link between Gaussian fields and Gauss.pdf}
}

@article{liParameterEstimationOrdinary2005,
  title = {Parameter Estimation of Ordinary Differential Equations},
  author = {Li, Zhengfeng and Osborne, Michael R and Prvan, Tania},
  year = {2005},
  journal = {IMA Journal of Numerical Analysis},
  volume = {25},
  pages = {264--285},
  doi = {10.1093/imanum/drh016},
  abstract = {This paper addresses the development of a new algorithm for parameter estimation of ordinary differential equations. Here, we show that (1) the simultaneous approach combined with orthogonal cyclic reduction can be used to reduce the estimation problem to an optimization problem subject to a fixed number of equality constraints without the need for structural information to devise a stable embedding in the case of non-trivial dichotomy and (2) the Newton approximation of the Hessian information of the Lagrangian function of the estimation problem should be used in cases where hypothesized models are incorrect or only a limited amount of sample data is available. A new algorithm is proposed which includes the use of the sequential quadratic programming (SQP) Gauss-Newton approximation but also encompasses the SQP Newton approximation along with tests of when to use this approximation. This composite approach relaxes the restrictions on the SQP Gauss-Newton approximation that the hypothesized model should be correct and the sample data set large enough. This new algorithm has been tested on two standard problems.},
  keywords = {constrained optimization,data fitting,Gauss-Newton approximation,ordinary differential equations,orthogonal cyclic reduction,parameter estimation,SQP methods},
  file = {/home/yanni/Zotero/storage/VRA95TL8/Li, Osborne, Prvan - 2005 - Parameter estimation of ordinary differential equations.pdf}
}

@article{liParameterEstimationOrdinary2005a,
  title = {Parameter Estimation of Ordinary Differential Equations},
  author = {Li, Zhengfeng and Osborne, Michael R and Prvan, Tania},
  year = {2005},
  journal = {IMA Journal of Numerical Analysis},
  volume = {25},
  number = {2},
  pages = {264--285},
  publisher = {{Oxford University Press}}
}

@article{liSlicedInverseRegression1991,
  title = {Sliced {{Inverse Regression}} for {{Dimension Reduction}}},
  author = {Li, Ker-Chau},
  year = {1991},
  journal = {Journal of the American Statistical Association},
  volume = {86},
  number = {414},
  eprint = {2290563},
  eprinttype = {jstor},
  pages = {316--327},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290563},
  abstract = {Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f({$\beta$}1x, ..., {$\beta$}Kx, {$\epsilon$}), where the {$\beta$}k's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the {$\beta$}k's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the {$\beta$}k's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x {$\mid$} y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.},
  file = {/home/yanni/Zotero/storage/75XYGJ5F/Li - 1991 - Sliced Inverse Regression for Dimension Reduction.pdf}
}

@article{liTransportInformationGeometry2021,
  title = {Transport Information Geometry: {{Riemannian}} Calculus on Probability Simplex},
  shorttitle = {Transport Information Geometry},
  author = {Li, Wuchen},
  year = {2021},
  month = nov,
  journal = {Info. Geo.},
  issn = {2511-2481, 2511-249X},
  doi = {10.1007/s41884-021-00059-1},
  abstract = {We formulate the Riemannian calculus of the probability set embedded with L2-Wasserstein metric. This is an initial work of transport information geometry. Our investigation starts with the probability simplex (probability manifold) supported on vertices of a finite graph. The main idea is to embed the probability manifold as a submanifold of the positive measure space with a nonlinear metric tensor. Here the nonlinearity comes from the linear weighted Laplacian operator. By this viewpoint, we establish torsion\textendash free Christoffel symbols, Levi-Civita connections, curvature tensors and volume forms in the probability manifold by Euclidean coordinates. As a consequence, the Jacobi equation, Laplace-Beltrami and Hessian operators on the probability manifold are derived. These geometric computations are also provided in the infinite-dimensional density space (density manifold) supported on a finite-dimensional manifold. In particular, an identity is given connecting the Baker-E\textasciiacute mery {$\Gamma$}2 operator (carr\textasciiacute e du champ it\textasciiacute er\textasciiacute e) by connecting Fisher-Rao information metric and optimal transport metric. Several examples are demonstrated.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/H5SKAN68/Li - 2021 - Transport information geometry Riemannian calculu.pdf}
}

@article{liTransportInformationGeometry2021a,
  title = {Transport Information Geometry: {{Riemannian}} Calculus on Probability Simplex},
  shorttitle = {Transport Information Geometry},
  author = {Li, Wuchen},
  year = {2021},
  month = nov,
  journal = {Info. Geo.},
  issn = {2511-2481, 2511-249X},
  doi = {10.1007/s41884-021-00059-1},
  abstract = {We formulate the Riemannian calculus of the probability set embedded with L2-Wasserstein metric. This is an initial work of transport information geometry. Our investigation starts with the probability simplex (probability manifold) supported on vertices of a finite graph. The main idea is to embed the probability manifold as a submanifold of the positive measure space with a nonlinear metric tensor. Here the nonlinearity comes from the linear weighted Laplacian operator. By this viewpoint, we establish torsion\textendash free Christoffel symbols, Levi-Civita connections, curvature tensors and volume forms in the probability manifold by Euclidean coordinates. As a consequence, the Jacobi equation, Laplace-Beltrami and Hessian operators on the probability manifold are derived. These geometric computations are also provided in the infinite-dimensional density space (density manifold) supported on a finite-dimensional manifold. In particular, an identity is given connecting the Baker-E\textasciiacute mery {$\Gamma$}2 operator (carr\textasciiacute e du champ it\textasciiacute er\textasciiacute e) by connecting Fisher-Rao information metric and optimal transport metric. Several examples are demonstrated.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/WJPU2CHP/Li - 2021 - Transport information geometry Riemannian calculu.pdf}
}

@article{liuChangePointDetectionTimeSeries2013,
  title = {Change-{{Point Detection}} in {{Time-Series Data}} by {{Relative Density-Ratio Estimation}}},
  author = {Liu, Song and Yamada, Makoto and Collier, Nigel and Sugiyama, Masashi},
  year = {2013},
  month = jul,
  journal = {Neural Networks},
  volume = {43},
  eprint = {1203.0453},
  pages = {72--83},
  issn = {08936080},
  doi = {10.1016/j.neunet.2013.01.012},
  abstract = {The objective of change-point detection is to discover abrupt property changes lying behind time-series data. In this paper, we present a novel statistical changepoint detection algorithm based on non-parametric divergence estimation between time-series samples from two retrospective segments. Our method uses the relative Pearson divergence as a divergence measure, and it is accurately and efficiently estimated by a method of direct density-ratio estimation. Through experiments on artificial and real-world datasets including human-activity sensing, speech, and Twitter messages, we demonstrate the usefulness of the proposed method.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/2IF2XNB2/Liu et al. - 2013 - Change-Point Detection in Time-Series Data by Rela.pdf}
}

@article{lloydAutomaticConstructionNaturalLanguage,
  title = {Automatic {{Construction}} and {{Natural-Language Description}} of {{Nonparametric Regression Models}}},
  author = {Lloyd, James Robert and Duvenaud, David and Grosse, Roger and Tenenbaum, Joshua B and Ghahramani, Zoubin},
  pages = {9},
  abstract = {This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and naturallanguage text.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/G5M3UHA6/Lloyd et al. - Automatic Construction and Natural-Language Descri.pdf}
}

@book{loggAutomatedSolutionDifferential2012,
  title = {Automated {{Solution}} of {{Differential Equations}} by the {{Finite Element Method}}},
  editor = {Logg, Anders and Mardal, Kent-Andre and Wells, Garth},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computational Science}} and {{Engineering}}},
  volume = {84},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23099-8},
  isbn = {978-3-642-23098-1 978-3-642-23099-8},
  langid = {english},
  file = {/home/yanni/Zotero/storage/TNP54VAV/Logg et al. - 2012 - Automated Solution of Differential Equations by th.pdf}
}

@article{lordenProceduresReactingChange1971,
  title = {Procedures for {{Reacting}} to a {{Change}} in {{Distribution}}},
  author = {Lorden, G.},
  year = {1971},
  month = dec,
  journal = {Ann. Math. Statist.},
  volume = {42},
  number = {6},
  pages = {1897--1908},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177693055},
  langid = {english},
  file = {/home/yanni/Zotero/storage/PIQF8HTP/Lorden - 1971 - Procedures for Reacting to a Change in Distributio.pdf}
}

@book{lordIntroductionComputationalStochastic2014,
  title = {An {{Introduction}} to {{Computational Stochastic PDEs}}},
  author = {Lord, Gabriel J. and Powell, Catherine E. and Shardlow, Tony},
  year = {2014},
  month = aug,
  publisher = {{Cambridge University Press}},
  abstract = {This book gives a comprehensive introduction to numerical methods and analysis of stochastic processes, random fields and stochastic differential equations, and offers graduate students and researchers powerful tools for understanding uncertainty quantification for risk analysis. Coverage includes traditional stochastic ODEs with white noise forcing, strong and weak approximation, and the multi-level Monte Carlo method. Later chapters apply the theory of random fields to the numerical solution of elliptic PDEs with correlated random data, discuss the Monte Carlo method, and introduce stochastic Galerkin finite-element methods. Finally, stochastic parabolic PDEs are developed. Assuming little previous exposure to probability and statistics, theory is developed in tandem with state-of the art computational methods through worked examples, exercises, theorems and proofs. The set of MATLAB codes included (and downloadable) allows readers to perform computations themselves and solve the test problems discussed. Practical examples are drawn from finance, mathematical biology, neuroscience, fluid flow modeling and materials science.},
  googlebooks = {GX\_RAwAAQBAJ},
  isbn = {978-0-521-89990-1},
  langid = {english},
  keywords = {Business \& Economics / Finance / General,Mathematics / Differential Equations / General,Mathematics / Discrete Mathematics,Mathematics / Game Theory,Mathematics / General,Mathematics / Numerical Analysis,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{louniciOracleInequalitiesOptimal2011,
  title = {Oracle Inequalities and Optimal Inference under Group Sparsity},
  author = {Lounici, Karim and Pontil, Massimiliano and Van De Geer, Sara and Tsybakov, Alexandre B.},
  year = {2011},
  journal = {Annals of Statistics},
  volume = {39},
  number = {4},
  pages = {2164--2204},
  issn = {00905364},
  doi = {10.1214/11-AOS896},
  abstract = {We consider the problem of estimating a sparse linear regression vector {$\beta$} {${_\ast}$} under a Gaussian noise model, for the purpose of both prediction and model selection. We assume that prior knowledge is available on the sparsity pattern, namely the set of variables is partitioned into prescribed groups, only few of which are relevant in the estimation process. This group sparsity assumption suggests us to consider the Group Lasso method as a means to estimate {$\beta$} {${_\ast}$}.We establish oracle inequalities for the prediction and {$\mathscr{l}$}2 estimation errors of this estimator. These bounds hold under a restricted eigenvalue condition on the design matrix. Under a stronger condition, we derive bounds for the estimation error for mixed (2,p)-norms with 1 {$\leq$} p{$\leq\infty$}. When p={$\infty$}, this result implies that a thresholded version of the Group Lasso estimator selects the sparsity pattern of {$\beta$} {${_\ast}$} with high probability. Next, we prove that the rate of convergence of our upper bounds is optimal in a minimax sense, up to a logarithmic factor, for all estimators over a class of group sparse vectors. Furthermore, we establish lower bounds for the prediction and {$\mathscr{l}$}2 estimation errors of the usual Lasso estimator. Using this result, we demonstrate that the Group Lasso can achieve an improvement in the prediction and estimation errors as compared to the Lasso.},
  keywords = {Group Lasso,Group sparsity,Minimax risk,Moment inequality,Oracle inequalities,Penalized least squares,Statistical learning.},
  file = {/home/yanni/Zotero/storage/5LALDT9E/11-AOS896.pdf}
}

@article{lozanoGroupedGraphicalGranger2009,
  title = {Grouped Graphical Granger Modeling Methods for Temporal Causal Modeling},
  author = {Lozano, Aur{\'e}lie C. and Abe, Naoki and Liu, Yan and Rosset, Saharon},
  year = {2009},
  journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages = {577--585},
  doi = {10.1145/1557019.1557085},
  abstract = {We develop and evaluate an approach to causal modeling based on time series data, collectively referred to as\textbackslash grouped graphical Granger modeling methods." Graphical Granger modeling uses graphical modeling techniques on time series data and invokes the notion of \textbackslash Granger causality" to make assertions on causality among a potentially large number of time series variables through inference on time-lagged effects. The present paper proposes a novel enhancement to the graphical Granger methodology by developing and applying families of regression methods that are sensitive to group information among variables, to leverage the group structure present in the lagged temporal variables according to the time series they belong to. Additionally, we propose a new family of algorithms we call group boosting, as an improved component of grouped graphical Granger modeling over the existing regression methods with grouped variable selection in the literature (e.g group Lasso). The introduction of group boosting methods is primarily motivated by the need to deal with non-linearity in the data. We perform empirical evaluation to confirm the advantage of the grouped graphical Granger methods over the standard (non-grouped) methods, as well as that specific to the methods based on group boosting. This advantage is also demonstrated for the real world application of gene regulatory network discovery from time-course microarray data. Copyright 2009 ACM.},
  isbn = {9781605584959},
  keywords = {Boosting,Granger causality,Graphical modeling,Temporal causal modeling,Variable group selection},
  file = {/home/yanni/Zotero/storage/IITCU2ND/1557019.1557085.pdf}
}

@article{luDeepONetLearningNonlinear2021,
  title = {{{DeepONet}}: {{Learning}} Nonlinear Operators for Identifying Differential Equations Based on the Universal Approximation Theorem of Operators},
  shorttitle = {{{DeepONet}}},
  author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  year = {2021},
  month = mar,
  journal = {Nat Mach Intell},
  volume = {3},
  number = {3},
  eprint = {1910.03193},
  primaryclass = {cs, stat},
  pages = {218--229},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00302-5},
  abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,\textbackslash dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/AJ3EZ37A/Lu et al. - 2021 - DeepONet Learning nonlinear operators for identif.pdf;/home/yanni/Zotero/storage/XUGR4J8B/1910.html}
}

@techreport{lunardiInfiniteDimensionalAnalysis2015,
  title = {Infinite {{Dimensional Analysis}}},
  author = {Lunardi, Alessandra and Miranda, Michele and Pallara, Diego},
  year = {2015},
  abstract = {ii We collect here the revised edition of the 19 th Internet Seminar on "Infinite dimensional analysis". In the lectures, we consider separable infinite dimensional Banach spaces endowed with Gaussian measures and we describe their main properties; in particular we are interested in integration by parts formulae that allow the definition of gradient and divergence operators. Once these tools are introduced, we study Sobolev spaces. In the context of Gaussian analysis the role of the Laplacian ({$\increment$} = div) is played by the Ornstein-Uhlenbeck operator. We study the realisation of the Ornstein-Uhlenbeck operator and the Ornstein-Uhlenbeck semigroup in spaces of continuous functions and in L p spaces. In particular, for p = 2 the Ornstein-Uhlenbeck operator is self-adjoint and we show that there exists an orthogonal basis consisting of explicit eigenfunctions (the Hermite polynomials) that give raise to the "Wiener Chaos Decomposition". In the present revision we have taken into account the feedback coming from all the participants to the discussion board and we are grateful for all the contributions, which have corrected many mistakes and improved the presentation. We warmly thank in particular , Prof. J\"urgen Voigt and the whole Dresden group for their careful reading and their constructive criticism. We are planning to prepare another revision after the final workshop in Casalmaggiore, where we shall take into account possible further commnets.},
  file = {/home/yanni/Zotero/storage/I7M7GHMI/Lunardi, Miranda, Pallara - 2015 - Infinite Dimensional Analysis.pdf}
}

@article{luongHiddenMarkovModel2012,
  title = {Hidden {{Markov Model Applications}} in {{Change-Point Analysis}}},
  author = {Luong, The Minh and Perduca, Vittorio and Nuel, Gregory},
  year = {2012},
  month = dec,
  journal = {arXiv:1212.1778 [stat]},
  eprint = {1212.1778},
  primaryclass = {stat},
  abstract = {The detection of change-points in heterogeneous sequences is a statistical challenge with many applications in fields such as finance, signal analysis and biology. A wide variety of literature exists for finding an ideal set of change-points for characterizing the data. In this tutorial we elaborate on the Hidden Markov Model (HMM) and present two different frameworks for applying HMM to change-point models. Then we provide a summary of two procedures for inference in change-point analysis, which are particular cases of the forward-backward algorithm for HMMs, and discuss common implementation problems. Lastly, we provide two examples of the HMM methods on available data sets and we shortly discuss about the applications to current genomics studies. The R code used in the examples is provided in the appendix.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Computation},
  file = {/home/yanni/Zotero/storage/ED235G7I/Luong et al. - 2012 - Hidden Markov Model Applications in Change-Point A.pdf}
}

@article{luongHiddenMarkovModel2012a,
  title = {Hidden {{Markov Model Applications}} in {{Change-Point Analysis}}},
  author = {Luong, The Minh and Perduca, Vittorio and Nuel, Gregory},
  year = {2012},
  month = dec,
  journal = {arXiv:1212.1778 [stat]},
  eprint = {1212.1778},
  primaryclass = {stat},
  abstract = {The detection of change-points in heterogeneous sequences is a statistical challenge with many applications in fields such as finance, signal analysis and biology. A wide variety of literature exists for finding an ideal set of change-points for characterizing the data. In this tutorial we elaborate on the Hidden Markov Model (HMM) and present two different frameworks for applying HMM to change-point models. Then we provide a summary of two procedures for inference in change-point analysis, which are particular cases of the forward-backward algorithm for HMMs, and discuss common implementation problems. Lastly, we provide two examples of the HMM methods on available data sets and we shortly discuss about the applications to current genomics studies. The R code used in the examples is provided in the appendix.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Computation},
  file = {/home/yanni/Zotero/storage/VJ49JQZN/Luong et al. - 2012 - Hidden Markov Model Applications in Change-Point A.pdf}
}

@inproceedings{maFunctionalVariationalInference2021,
  title = {Functional {{Variational Inference}} Based on {{Stochastic Process Generators}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ma, Chao and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2021},
  volume = {34},
  pages = {21795--21807},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Bayesian inference in the space of functions has been an important topic for Bayesian modeling in the past. In this paper, we propose a new solution to this problem called Functional Variational Inference (FVI). In FVI, we minimize a divergence in function space between the variational distribution and the posterior process. This is done by using as functional variational family a new class of flexible distributions called Stochastic Process Generators (SPGs), which are cleverly designed so that the functional ELBO can be estimated efficiently using analytic solutions and mini-batch sampling. FVI can be applied to stochastic process priors when random function samples from those priors are available. Our experiments show that FVI consistently outperforms weight-space and function space VI methods on several tasks, which validates the effectiveness of our approach.},
  file = {/home/yanni/Zotero/storage/XR46Z5IS/Ma_Hern√°ndez-Lobato_2021_Functional Variational Inference based on Stochastic Process Generators.pdf}
}

@techreport{magnaniBAYESIANFILTERINGODES,
  title = {{{BAYESIAN FILTERING FOR ODES WITH BOUNDED DERIVATIVES}}},
  author = {Magnani, Emilia and Kersting, Hans and Schober, Michael and Hennig, Philipp},
  abstract = {Recently there has been increasing interest in probabilistic solvers for ordinary differential equations (ODEs) that return full probability measures, instead of point estimates, over the solution and can incorporate uncertainty over the ODE at hand, e.g. if the vector field or the initial value is only approximately known or evaluable. The ODE filter proposed in [9, 16] models the solution of the ODE by a Gauss-Markov process which serves as a prior in the sense of Bayesian statistics. While previous work employed a Wiener process prior on the (possibly multiple times) differentiated solution of the ODE and established equivalence of the corresponding solver with classical numerical methods, this paper raises the question whether other priors also yield practically useful solvers. To this end, we discuss a range of possible priors which enable fast filtering and propose a new prior-the Integrated Ornstein Uhlenbeck Process (IOUP)-that complements the existing Integrated Wiener process (IWP) filter by encoding the property that a derivative in time of the solution is bounded in the sense that it tends to drift back to zero. We provide experiments comparing IWP and IOUP filters which support the belief that IWP approximates better divergent ODE's solutions whereas IOUP is a better prior for trajectories with bounded derivatives.},
  keywords = {62F15,62M05,65C20,65L05,65L06,Bayesian filter-ing,Gaussian processes,initial value problems,Markov processes AMS subject classifications 60H30,numerical analysis,probabilistic numerics},
  file = {/home/yanni/Zotero/storage/W8JFNFTW/Magnani et al. - Unknown - BAYESIAN FILTERING FOR ODES WITH BOUNDED DERIVATIVES.pdf}
}

@inproceedings{malladiOnlineBayesianChange2013,
  title = {Online {{Bayesian}} Change Point Detection Algorithms for Segmentation of Epileptic Activity},
  booktitle = {2013 {{Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  author = {Malladi, Rakesh and Kalamangalam, Giridhar P and Aazhang, Behnaam},
  year = {2013},
  month = nov,
  pages = {1833--1837},
  publisher = {{IEEE}},
  address = {{Pacific Grove, CA, USA}},
  doi = {10.1109/ACSSC.2013.6810619},
  abstract = {Epilepsy is a dynamic disease in which the brain transitions between different states. In this paper, we focus on the problem of identifying the time points, referred to as change points, where the transitions between these different states happen. A Bayesian change point detection algorithm that does not require the knowledge of the total number of states or the parameters of the probability distribution modeling the activity of epileptic brain in each of these states is developed in this paper. This algorithm works in online mode making it amenable for real-time monitoring. To reduce the quadratic complexity of this algorithm, an approximate algorithm with linear complexity in the number of data points is also developed. Finally, we use these algorithms on ECoG recordings of an epileptic patient to locate the change points and determine segments corresponding to different brain states.},
  isbn = {978-1-4799-2390-8 978-1-4799-2388-5},
  langid = {english},
  file = {/home/yanni/Zotero/storage/2RHEDV5J/Malladi et al. - 2013 - Online Bayesian change point detection algorithms .pdf}
}

@inproceedings{malladiOnlineBayesianChange2013a,
  title = {Online {{Bayesian}} Change Point Detection Algorithms for Segmentation of Epileptic Activity},
  booktitle = {2013 {{Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  author = {Malladi, Rakesh and Kalamangalam, Giridhar P and Aazhang, Behnaam},
  year = {2013},
  month = nov,
  pages = {1833--1837},
  publisher = {{IEEE}},
  address = {{Pacific Grove, CA, USA}},
  doi = {10.1109/ACSSC.2013.6810619},
  abstract = {Epilepsy is a dynamic disease in which the brain transitions between different states. In this paper, we focus on the problem of identifying the time points, referred to as change points, where the transitions between these different states happen. A Bayesian change point detection algorithm that does not require the knowledge of the total number of states or the parameters of the probability distribution modeling the activity of epileptic brain in each of these states is developed in this paper. This algorithm works in online mode making it amenable for real-time monitoring. To reduce the quadratic complexity of this algorithm, an approximate algorithm with linear complexity in the number of data points is also developed. Finally, we use these algorithms on ECoG recordings of an epileptic patient to locate the change points and determine segments corresponding to different brain states.},
  isbn = {978-1-4799-2390-8 978-1-4799-2388-5},
  langid = {english},
  file = {/home/yanni/Zotero/storage/4YV6ISQD/Malladi et al. - 2013 - Online Bayesian change point detection algorithms .pdf}
}

@article{ManifoldStatisticsFrechet,
  title = {Manifold {{Statistics}}: {{Fr\'echet Mean}}},
  langid = {english},
  file = {/home/yanni/Zotero/storage/9K2D2ASN/Manifold Statistics Fr√©chet Mean.pdf}
}

@article{mantonPrimerReproducingKernel2015,
  title = {A {{Primer}} on {{Reproducing Kernel Hilbert Spaces}}},
  author = {Manton, Jonathan H and Amblard, Pierre-Olivier},
  year = {2015},
  journal = {Foundations and Trends \textregistered{} in Signal Processing},
  volume = {XX, No. XX},
  pages = {1--130},
  doi = {10.1561/XXXXXXXXXX},
  file = {/home/yanni/Zotero/storage/J7Y2XDYE/Manton, Amblard - 2015 - A Primer on Reproducing Kernel Hilbert Spaces.pdf}
}

@article{marinApproximateBayesianComputational2012,
  title = {Approximate {{Bayesian}} Computational Methods},
  author = {Marin, Jean-Michel and Pudlo, Pierre and Robert, Christian P and Ryder, Robin J},
  year = {2012},
  journal = {Stat Comput},
  volume = {22},
  pages = {1167--1180},
  doi = {10.1007/s11222-011-9288-2},
  abstract = {Approximate Bayesian Computation (ABC) methods , also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years.},
  keywords = {Bayesian model choice,Bayesian statistics ¬∑ ABC methodology ¬∑ DIYABC ¬∑,Likelihood-free methods ¬∑},
  file = {/home/yanni/Zotero/storage/ZG9FXXIY/Marin et al. - 2012 - Approximate Bayesian computational methods.pdf}
}

@inproceedings{marrinanFindingSubspaceMean2014,
  title = {Finding the {{Subspace Mean}} or {{Median}} to {{Fit Your Need}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Marrinan, Tim and Draper, Bruce and Beveridge, J. Ross and Kirby, Michael and Peterson, Chris},
  year = {2014},
  month = jun,
  pages = {1082--1089},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.142},
  abstract = {Many computer vision algorithms employ subspace models to represent data. Many of these approaches benefit from the ability to create an average or prototype for a set of subspaces. The most popular method in these situations is the Karcher mean, also known as the Riemannian center of mass. The prevalence of the Karcher mean may lead some to assume that it provides the best average in all scenarios. However, other subspace averages that appear less frequently in the literature may be more appropriate for certain tasks. The extrinsic manifold mean, the L2-median, and the flag mean are alternative averages that can be substituted directly for the Karcher mean in many applications. This paper evaluates the characteristics and performance of these four averages on synthetic and real-world data. While the Karcher mean generalizes the Euclidean mean to the Grassman manifold, we show that the extrinsic manifold mean, the L2-median, and the flag mean behave more like medians and are therefore more robust to the presence of outliers among the subspaces being averaged. We also show that while the Karcher mean and L2-median are computed using iterative algorithms, the extrinsic manifold mean and flag mean can be found analytically and are thus orders of magnitude faster in practice. Finally, we show that the flag mean is a generalization of the extrinsic manifold mean that permits subspaces with different numbers of dimensions to be averaged. The result is a ''cookbook'' that maps algorithm constraints and data properties to the most appropriate subspace mean for a given application.},
  isbn = {978-1-4799-5118-5},
  langid = {english},
  file = {/home/yanni/Zotero/storage/TZDDKUXP/Marrinan et al. - 2014 - Finding the Subspace Mean or Median to Fit Your Ne.pdf}
}

@misc{martinoIntegratedNestedLaplace2019,
  title = {Integrated {{Nested Laplace Approximations}} ({{INLA}})},
  author = {Martino, Sara and Riebler, Andrea},
  year = {2019},
  month = jul,
  number = {arXiv:1907.01248},
  eprint = {arXiv:1907.01248},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.01248},
  abstract = {This is a short description and basic introduction to the Integrated nested Laplace approximations (INLA) approach. INLA is a deterministic paradigm for Bayesian inference in latent Gaussian models (LGMs) introduced in Rue et al. (2009). INLA relies on a combination of analytical approximations and efficient numerical integration schemes to achieve highly accurate deterministic approximations to posterior quantities of interest. The main benefit of using INLA instead of Markov chain Monte Carlo (MCMC) techniques for LGMs is computational; INLA is fast even for large, complex models. Moreover, being a deterministic algorithm, INLA does not suffer from slow convergence and poor mixing. INLA is implemented in the R package R-INLA, which represents a user-friendly and versatile tool for doing Bayesian inference. R-INLA returns posterior marginals for all model parameters and the corresponding posterior summary information. Model choice criteria as well as predictive diagnostics are directly available. Here, we outline the theory behind INLA, present the R-INLA package and describe new developments of combining INLA with MCMC for models that are not possible to fit with R-INLA.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation},
  file = {/home/yanni/Zotero/storage/4QZXPI6J/Martino and Riebler - 2019 - Integrated Nested Laplace Approximations (INLA).pdf;/home/yanni/Zotero/storage/D8Q34W93/1907.html}
}

@article{masarottoProcrustesMetricsCovariance2018,
  title = {Procrustes {{Metrics}} on {{Covariance Operators}} and {{Optimal Transportation}} of {{Gaussian Processes}}},
  author = {Masarotto, Valentina and Panaretos, Victor M. and Zemel, Yoav},
  year = {2018},
  month = jan,
  abstract = {Covariance operators are fundamental in functional data analysis, providing the canonical means to analyse functional variation via the celebrated Karhunen\textendash Lo\`eve expansion. These operators may themselves be subject to variation, for instance in contexts where multiple functional populations are to be compared. Statistical techniques to analyse such variation are intimately linked with the choice of metric on covariance operators, and the intrinsic infinite-dimensionality of these operators. In this paper, we describe the manifold geometry of the space of trace-class infinite-dimensional covariance operators and associated key statistical properties, under the recently proposed infinite-dimensional version of the Procrustes metric. We identify this space with that of centred Gaussian processes equipped with the Wasserstein metric of optimal transportation. The identification allows us to provide a complete description of those aspects of this manifold geometry that are important in terms of statistical inference, and establish key properties of the Fr\'echet mean of a random sample of covariances, as well as generative models that are canonical for such metrics and link with the problem of registration of functional data.},
  file = {/home/yanni/Zotero/storage/XCMU3AFU/Masarotto, Panaretos, Zemel - 2018 - Procrustes Metrics on Covariance Operators and Optimal Transportation of Gaussian Processes.pdf}
}

@article{masarottoProcrustesMetricsCovariance2019,
  title = {Procrustes {{Metrics}} on {{Covariance Operators}} and {{Optimal Transportation}} of {{Gaussian Processes}}},
  author = {Masarotto, Valentina and Panaretos, Victor M. and Zemel, Yoav},
  year = {2019},
  month = feb,
  journal = {Sankhya A},
  volume = {81},
  number = {1},
  pages = {172--213},
  issn = {0976-8378},
  doi = {10.1007/s13171-018-0130-1},
  abstract = {Covariance operators are fundamental in functional data analysis, providing the canonical means to analyse functional variation via the celebrated Karhunen\textendash Lo\`eve expansion. These operators may themselves be subject to variation, for instance in contexts where multiple functional populations are to be compared. Statistical techniques to analyse such variation are intimately linked with the choice of metric on covariance operators, and the intrinsic infinite-dimensionality of these operators. In this paper, we describe the manifold-like geometry of the space of trace-class infinite-dimensional covariance operators and associated key statistical properties, under the recently proposed infinite-dimensional version of the Procrustes metric (Pigoli et al. Biometrika101, 409\textendash 422, 2014). We identify this space with that of centred Gaussian processes equipped with the Wasserstein metric of optimal transportation. The identification allows us to provide a detailed description of those aspects of this manifold-like geometry that are important in terms of statistical inference; to establish key properties of the Fr\'echet mean of a random sample of covariances; and to define generative models that are canonical for such metrics and link with the problem of registration of warped functional data.},
  langid = {english},
  keywords = {60D05 Geometric probability and stochastic geometry,62M99 None of the above,but in this section.,Fr√©chet mean,Functional data analysis,Manifold statistics,Optimal coupling,Primary 60G15 Gaussian processes,Secondary 60H25 Random operators and equations,Tangent space PCA,Trace-class operator.},
  file = {/home/yanni/Zotero/storage/UI7LHEX9/Masarotto et al. - 2019 - Procrustes Metrics on Covariance Operators and Opt.pdf}
}

@article{matthiesUncertaintiesProbabilisticNumerical1997,
  title = {Uncertainties in Probabilistic Numerical Analysis of Structures and Solids-{{Stochastic}} Finite Elements},
  author = {Matthies, Hermann G. and Brenner, Christoph E. and Bucher, Christian G. and Guedes Soares, C.},
  year = {1997},
  month = jan,
  journal = {Structural Safety},
  volume = {19},
  number = {3},
  pages = {283--336},
  issn = {01674730},
  doi = {10.1016/S0167-4730(97)00013-1},
  langid = {english},
  file = {/home/yanni/Zotero/storage/4PYWSU65/Matthies et al. - 1997 - Uncertainties in probabilistic numerical analysis .pdf}
}

@article{matthiesUncertaintiesProbabilisticNumerical1997a,
  title = {Uncertainties in Probabilistic Numerical Analysis of Structures and Solids-{{Stochastic}} Finite Elements},
  author = {Matthies, Hermann G. and Brenner, Christoph E. and Bucher, Christian G. and Soares, C. Guedes},
  year = {1997},
  journal = {Structural Safety},
  volume = {19},
  number = {3},
  pages = {283--336},
  issn = {0167-4730},
  doi = {10.1016/S0167-4730(97)00013-1},
  abstract = {The main sources of uncertainties involved in the analysis of structures and solids are shown and the tools available to deal with them. While trying to cover the complete modeling process, ranging from the problem formulation via the mathematical model all the way to the numerical approximation, we have tried to expose areas in need of further research. The techniques and methods involved in stochastic modeling are explained in somewhat more detail, as they are newer and less known than those used for the deterministic modeling.},
  keywords = {Numerical approximations,Representation of stochastic fields,Stochastic finite elements,Uncertainty modeling}
}

@article{melidonisStatisticalAnalysisManifoldValued,
  title = {Statistical {{Analysis}} of {{Manifold-Valued Random Fields}}},
  author = {Melidonis, Savvas},
  pages = {54},
  langid = {english},
  file = {/home/yanni/Zotero/storage/DS5BARWS/Melidonis - Statistical Analysis of Manifold-Valued Random Fie.pdf}
}

@article{melidonisStatisticalAnalysisManifoldValueda,
  title = {Statistical {{Analysis}} of {{Manifold-Valued Random Fields}}},
  author = {Melidonis, Savvas},
  pages = {54},
  langid = {english},
  file = {/home/yanni/Zotero/storage/LLL66DYM/Melidonis - Statistical Analysis of Manifold-Valued Random Fie.pdf}
}

@article{melidonisStatisticalAnalysisManifoldValuedb,
  title = {Statistical {{Analysis}} of {{Manifold-Valued Random Fields}}},
  author = {Melidonis, Savvas},
  pages = {54},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GMP6PQSG/Melidonis - Statistical Analysis of Manifold-Valued Random Fie.pdf}
}

@article{mengStatisticalParadisesParadoxes2018,
  title = {Statistical Paradises and Paradoxes in Big Data ({{I}}): {{Law}} of Large Populations, Big Data Paradox, and the 2016 Us Presidential Election},
  author = {Meng, Xiao Li},
  year = {2018},
  journal = {Annals of Applied Statistics},
  volume = {12},
  number = {2},
  pages = {685--726},
  issn = {19417330},
  doi = {10.1214/18-AOAS1161SF},
  abstract = {Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualifications for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: ``Which one should I trust more: a 1\% survey with 60\% response rate or a self-reported administrative dataset covering 80\% of the population?'' A 5-element Euler-formula-like identity shows that for any dataset of size n, probabilistic or not, the difference between the sample average Xn and the population average XN is the product of three terms: (1) a data quality measure, {$\rho$}R,X, the correlation between{$\surd$}Xj and the response/recording indicator Rj; (2) a data quantity measure, {$\surd$}(N - n)/n, where N is the population size; and (3) a problem difficulty measure, {$\sigma$}X, the standard deviation of X. This decomposition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling {$\rho$}R,X at the level of N-1/2; (II) When we lose this control, the impact of N is no longer canceled by {$\rho$}R,X, leading to a Law of Large Populations{$\surd$} (LLP), that is, our estimation error, relative to the benchmarking rate 1/{$\surd$}n, increases with {$\surd$}N; and (III) the ``bigness'' of such Big Data (for population inferences) should be measured by the relative size f = n/N, not the absolute size n; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by their sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a {$\rho$}R,X {$\approx-$}0.005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly minuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1\% of the US eligible voters, that is, n {$\approx$} 2,300,000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n {$\approx$} 400, a 99.98\% reduction of sample size (and hence our confidence). The CCES data demonstrate LLP vividly: on average, the larger the state's voter populations, the further away the actual Trump vote shares from the usual 95\% confidence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves.},
  keywords = {Bias-variance tradeoff,Data confidentiality and privacy,Data defect correlation,Data defect index (d.d.i.),Data quality-quantity tradeoff,Euler identity,Monte Carlo and Quasi Monte Carlo (MCQMC),Non-response bias},
  file = {/home/yanni/Zotero/storage/9ZASRVPJ/statistical_paradises_and_paradoxes.pdf}
}

@article{merhavIntegralRepresentationLogarithmic2019,
  title = {An {{Integral Representation}} of the {{Logarithmic Function}} with {{Applications}} in {{Information Theory}}},
  author = {Merhav, Neri and Sason, Igal},
  year = {2019},
  month = dec,
  journal = {Entropy},
  volume = {22},
  number = {1},
  eprint = {1912.05812},
  primaryclass = {cs, math},
  pages = {51},
  issn = {1099-4300},
  doi = {10.3390/e22010051},
  abstract = {We explore a well-known integral representation of the logarithmic function, and demonstrate its usefulness in obtaining compact, easily-computable exact formulas for quantities that involve expectations and higher moments of the logarithm of a positive random variable (or the logarithm of a sum of positive random variables). The integral representation of the logarithm is proved useful in a variety of information-theoretic applications, including universal lossless data compression, entropy and differential entropy evaluations, and the calculation of the ergodic capacity of the single-input, multiple-output (SIMO) Gaussian channel with random parameters (known to both transmitter and receiver). This integral representation and its variants are anticipated to serve as a useful tool in additional applications, as a rigorous alternative to the popular (but non-rigorous) replica method (at least in some situations).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Mathematics - Probability},
  file = {/home/yanni/Zotero/storage/8DZFI7II/Merhav and Sason - 2019 - An Integral Representation of the Logarithmic Func.pdf;/home/yanni/Zotero/storage/CQ9QKBWI/1912.html}
}

@techreport{mohamedLearningImplicitGenerative2017,
  title = {Learning in {{Implicit Generative Models}}},
  author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2017},
  abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing gen-erative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classi-fiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning-to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models-models that only specify a stochastic procedure with which to generate data-and relate these ideas to modelling problems in related fields, such as econo-metrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density-ratio and density-difference estimation. There are four approaches for density comparison, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination. * Equal contribution 1 DeepMind, London, UK. Correspondence to: Shakir Mohamed {$<$}shakir@google.com{$>$}, Balaji Lakshmi-narayanan {$<$}balajiln@google.com{$>$}. 1. Implicit Generative Models It is useful to make a distinction between two types of prob-abilistic models: prescribed and implicit models (Diggle and Gratton, 1984). Prescribed probabilistic models are those that provide an explicit parametric specification of the distribution of an observed random variable x, specifying a log-likelihood function log q \$\th eta\$ (x) with parameters \$\th eta\$. Most models in machine learning and statistics are of this form, whether they be state-of-the-art classifiers for object recognition , complex sequence models for machine translation, or fine-grained spatio-temporal models tracking the spread of disease. Alternatively, we can specify implicit probabilis-tic models that define a stochastic procedure that directly generates data. Such models are the natural approach for problems in climate and weather, population genetics, and ecology, since the mechanistic understanding of such systems can be used to directly create a data simulator, and hence the model. It is exactly because implicit models are more natural for many problems that they are of interest and importance. Implicit generative models use a latent variable z and transform it using a deterministic function G \$\th eta\$ that maps from R m \textrightarrow{} R d using parameters \$\th eta\$. Such models are amongst the most fundamental of models, e.g., many of the basic methods for generating non-uniform random variates are based on simple implicit models and one-line transformations (Devroye, 2006). In general, implicit generative models specify a valid density on the output space that forms an effective likelihood function: x = G \$\th eta\$ (z); z {$\sim$} q(z) (1) q \$\th eta\$ (x) = {$\partial$} {$\partial$}x 1. .. {$\partial$} {$\partial$}x d G \$\th eta\$ (z){$\leq$}x q(z)dz, (2) where q(z) is a latent variable that provides the external source of randomness and equation (2) is the definition of the transformed density as the derivative of the cumulative distribution function. When the function G is well-defined, such as when the function is invertible, or has dimensions m = d with easily characterised roots, we recover the familiar rule for transformations of probability distributions. We are interested in developing more general and flexible implicit generative models where the function G is a non-linear function with d {$>$} m, specified by deep networks.},
  file = {/home/yanni/Zotero/storage/7LCSW7WJ/Mohamed, Lakshminarayanan - 2017 - Learning in Implicit Generative Models.pdf}
}

@article{molerNineteenDubiousWays2003,
  title = {Nineteen {{Dubious Ways}} to {{Compute}} the {{Exponential}} of a {{Matrix}}, {{Twenty-Five Years Later}}},
  author = {Moler, Cleve and Van Loan, Charles},
  year = {2003},
  month = jan,
  journal = {SIAM Rev.},
  volume = {45},
  number = {1},
  pages = {3--49},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/S00361445024180},
  abstract = {In principle, the exponential of a matrix could be computed in many ways. Methods involving approximation theory, differential equations, the matrix eigenvalues, and the matrix characteristic polynomial have been proposed. In practice, consideration of computational stability and efficiency indicates that some of the methods are preferable to others, but that none are completely satisfactory.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GLNG6HK9/Moler and Van Loan - 2003 - Nineteen Dubious Ways to Compute the Exponential o.pdf}
}

@techreport{MoreMeasureTheory,
  title = {More {{Measure Theory}}},
  abstract = {In this set of notes we sketch some results in measure theory that we don't have time to cover in full. Most of the results can be found in Rudin's Real \& Complex Analysis. Some of the results here require a certain technical assumption on measures. Definition: {$\sigma$}-Finite Measure Space A measure space (X, \textmu ) is said to be {$\sigma$}-finite if X can be expressed as a countable union of measurable sets of finite measure. For example, the real line is {$\sigma$}-finite with respect to Lebesgue measure, since R = n{$\in$}N [-n, n] and each set [-n, n] has finite measure. Similarly, the natural numbers N are {$\sigma$}-finite with respect to counting measure. Not every measure space is {$\sigma$}-finite. For example, if we put counting measure on R, then the resulting measure space is not {$\sigma$}-finite, since R cannot be expressed as a countable union of finite sets. However, most measure spaces that are important in mathematics are {$\sigma$}-finite, and it is considered a very reasonable restriction to place on a measure space. Throughout these notes we will assume that all measure spaces under consideration are {$\sigma$}-finite. Product Measures We would like to be able to use the Lebesgue integral to integrate functions on R 2 and R 3. This involves defining measures on R 2 and R 3 that correspond to areas and volume, respectively. The following theorem treats this construction from a general point of view.},
  file = {/home/yanni/Zotero/storage/4XHQUCRM/Unknown - Unknown - More Measure Theory.pdf}
}

@article{moskvinaAlgorithmBasedSingular2003,
  title = {An {{Algorithm Based}} on {{Singular Spectrum Analysis}} for {{Change-Point Detection}}},
  author = {Moskvina, Valentina and Zhigljavsky, Anatoly},
  year = {2003},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {32},
  number = {2},
  pages = {319--352},
  issn = {0361-0918, 1532-4141},
  doi = {10.1081/SAC-120017494},
  abstract = {This paper is devoted to application of the singular-spectrum analysis to sequential detection of changes in time series. An algorithm of change-point detection in time series, based on sequential application of the singular-spectrum analysis is developed and studied. The algorithm is applied to different data sets and extensively studied numerically. For specific models, several numerical approximations to the error probabilities and the power function of the algorithm are obtained. Numerical comparisons with other methods are given.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/8LGGUD34/Moskvina and Zhigljavsky - 2003 - An Algorithm Based on Singular Spectrum Analysis f.pdf}
}

@techreport{muandetKernelMeanEmbedding2017,
  title = {Kernel {{Mean Embedding}} of {{Distributions}}: {{A Review}} and {{Beyond}}},
  author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  keywords = {()},
  file = {/home/yanni/Zotero/storage/WGBPZBJT/Muandet et al. - 2017 - Kernel Mean Embedding of Distributions A Review and Beyond.pdf}
}

@article{murphyConjugateBayesianAnalysis,
  title = {Conjugate {{Bayesian}} Analysis of the {{Gaussian}} Distribution},
  author = {Murphy, Kevin P},
  pages = {29},
  langid = {english},
  file = {/home/yanni/Zotero/storage/GY2929A6/Murphy - Conjugate Bayesian analysis of the Gaussian distri.pdf}
}

@article{naessethElementsSequentialMonte2022,
  title = {Elements of {{Sequential Monte Carlo}}},
  author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch{\"o}n, Thomas B.},
  year = {2022},
  month = mar,
  journal = {arXiv:1903.04797 [cs, stat]},
  eprint = {1903.04797},
  primaryclass = {cs, stat},
  abstract = {A core problem in statistics and probabilistic machine learning is to compute probability distributions and expectations. This is the fundamental problem of Bayesian statistics and machine learning, which frames all inference as expectations with respect to the posterior distribution. The key challenge is to approximate these intractable expectations. In this tutorial, we review sequential Monte Carlo (SMC), a random-sampling-based class of methods for approximate inference. First, we explain the basics of SMC, discuss practical issues, and review theoretical results. We then examine two of the main user design choices: the proposal distributions and the so called intermediate target distributions. We review recent results on how variational inference and amortization can be used to learn efficient proposals and target distributions. Next, we discuss the SMC estimate of the normalizing constant, how this can be used for pseudo-marginal inference and inference evaluation. Throughout the tutorial we illustrate the use of SMC on various models commonly used in machine learning, such as stochastic recurrent neural networks, probabilistic graphical models, and probabilistic programs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/NSXXYU9J/Naesseth et al. - 2022 - Elements of Sequential Monte Carlo.pdf;/home/yanni/Zotero/storage/F3PKWVUD/1903.html}
}

@inproceedings{namoanoOnlineChangeDetection2019,
  title = {Online Change Detection Techniques in Time Series: {{An}} Overview},
  shorttitle = {Online Change Detection Techniques in Time Series},
  booktitle = {2019 {{IEEE International Conference}} on {{Prognostics}} and {{Health Management}} ({{ICPHM}})},
  author = {Namoano, Bernadin and Starr, Andrew and Emmanouilidis, Christos and Cristobal, Ruiz Carcel},
  year = {2019},
  month = jun,
  pages = {1--10},
  doi = {10.1109/ICPHM.2019.8819394},
  abstract = {Time-series change detection has been studied in several fields. From sensor data, engineering systems, medical diagnosis, and financial markets to user actions on a network, huge amounts of temporal data are generated. There is a need for a clear separation between normal and abnormal behaviour of the system in order to investigate causes or forecast change. Characteristics include irregularities, deviations, anomalies, outliers, novelties or surprising patterns. The efficient detection of such patterns is challenging, especially when constraints need to be taken into account, such as the data velocity, volume, limited time for reacting to events, and the details of the temporal sequence.This paper reviews the main techniques for time series change point detection, focusing on online methods. Performance criteria including complexity, time granularity, and robustness is used to compare techniques, followed by a discussion about current challenges and open issues.},
  keywords = {abnormality detection,Complexity theory,Data models,Engines,Hidden Markov models,Monitoring,Online change detection,Real-time systems,Time series analysis,time series segmentation},
  file = {/home/yanni/Zotero/storage/WXR7ZZMT/Namoano et al. - 2019 - Online change detection techniques in time series.pdf;/home/yanni/Zotero/storage/PX79UL23/8819394.html}
}

@book{nealBayesianLearningNeural1996,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  year = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {118},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-0745-0},
  abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties | the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
  isbn = {978-0-387-94724-2 978-1-4612-0745-0},
  langid = {english},
  file = {/home/yanni/Zotero/storage/WZN25PSN/Neal - 1996 - Bayesian Learning for Neural Networks.pdf}
}

@book{nealBayesianLearningNeural1996a,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  year = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {118},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-0745-0},
  abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties | the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
  isbn = {978-0-387-94724-2 978-1-4612-0745-0},
  langid = {english},
  file = {/home/yanni/Zotero/storage/NPL7QQ57/Neal - 1996 - Bayesian Learning for Neural Networks.pdf}
}

@techreport{niekumCHAMPChangepointDetection2014,
  title = {{{CHAMP}}: {{Changepoint Detection Using Approximate Model Parameters}}:},
  shorttitle = {{{CHAMP}}},
  author = {Niekum, Scott and Osentoski, Sarah and Atkeson, Christopher G. and Barto, Andrew G.},
  year = {2014},
  month = jun,
  address = {{Fort Belvoir, VA}},
  institution = {{Defense Technical Information Center}},
  doi = {10.21236/ADA605983},
  abstract = {We introduce CHAMP, an algorithm for online Bayesian changepoint detection in settings where it is difficult or undesirable to integrate over the parameters of candidate models. Rather than requiring integration of the parameters of candidate models as in several other Bayesian approaches, we require only the ability to fit model parameters to data segments. This approach greatly simplifies the use of Bayesian changepoint detection, allows it to be used with many more types of models, and improves performance when detecting parameter changes within a single model. Experimental analysis compares CHAMP to another state-of-the-art online Bayesian changepoint detection method.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/C6DRYY4L/Niekum et al. - 2014 - CHAMP Changepoint Detection Using Approximate Mod.pdf}
}

@inproceedings{niekumOnlineBayesianChangepoint2015,
  title = {Online {{Bayesian}} Changepoint Detection for Articulated Motion Models},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Niekum, Scott and Osentoski, Sarah and Atkeson, Christopher G. and Barto, Andrew G.},
  year = {2015},
  month = may,
  pages = {1468--1475},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/ICRA.2015.7139383},
  abstract = {We introduce CHAMP, an algorithm for online Bayesian changepoint detection in settings where it is difficult or undesirable to integrate over the parameters of candidate models. CHAMP is used in combination with several articulation models to detect changes in articulated motion of objects in the world, allowing a robot to infer physically-grounded task information. We focus on three settings where a changepoint model is appropriate: objects with intrinsic articulation relationships that can change over time, object-object contact that results in quasi-static articulated motion, and assembly tasks where each step changes articulation relationships. We experimentally demonstrate that this system can be used to infer various types of information from demonstration data including causal manipulation models, human-robot grasp correspondences, and skill verification tests.},
  isbn = {978-1-4799-6923-4},
  langid = {english},
  file = {/home/yanni/Zotero/storage/CGSICJ7J/Niekum et al. - 2015 - Online Bayesian changepoint detection for articula.pdf}
}

@article{nielsenIntroductionCompactOperators2017,
  title = {An {{Introduction}} to {{Compact Operators}}},
  author = {Nielsen, Jeff},
  year = {2017},
  volume = {4301},
  pages = {1--22},
  file = {/home/yanni/Zotero/storage/3VIHZLZA/Nielsen.pdf}
}

@article{nirwanRotationInvariantHouseholder2019,
  title = {Rotation {{Invariant Householder Parameterization}} for {{Bayesian PCA}}},
  author = {Nirwan, Rajbir S. and Bertschinger, Nils},
  year = {2019},
  month = may,
  journal = {arXiv:1905.04720 [cs, stat]},
  eprint = {1905.04720},
  primaryclass = {cs, stat},
  abstract = {We consider probabilistic PCA and related factor models from a Bayesian perspective. These models are in general not identifiable as the likelihood has a rotational symmetry. This gives rise to complicated posterior distributions with continuous subspaces of equal density and thus hinders efficiency of inference as well as interpretation of obtained parameters. In particular, posterior averages over factor loadings become meaningless and only model predictions are unambiguous. Here, we propose a parameterization based on Householder transformations, which remove the rotational symmetry of the posterior. Furthermore, by relying on results from random matrix theory, we establish the parameter distribution which leaves the model unchanged compared to the original rotationally symmetric formulation. In particular, we avoid the need to compute the Jacobian determinant of the parameter transformation. This allows us to efficiently implement probabilistic PCA in a rotation invariant fashion in any state of the art toolbox. Here, we implemented our model in the probabilistic programming language Stan and illustrate it on several examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/SRT7UY7B/Nirwan and Bertschinger - 2019 - Rotation Invariant Householder Parameterization fo.pdf;/home/yanni/Zotero/storage/PJ2TUB7D/1905.html}
}

@techreport{nocedalNumericalOptimization,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Springer, Stephen J Wright},
  file = {/home/yanni/Zotero/storage/4LWZDJFW/Nocedal, Springer - Unknown - Numerical Optimization.pdf}
}

@book{nualartMalliavinCalculusRelated2006,
  title = {The {{Malliavin}} Calculus and Related Topics},
  author = {Nualart, David},
  year = {2006},
  series = {Probability and Its Applications},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-28328-7},
  langid = {english},
  lccn = {QA274.2 .N83 2006},
  keywords = {Malliavin calculus},
  file = {/home/yanni/Zotero/storage/DH4IDHZC/Nualart - 2006 - The Malliavin calculus and related topics.pdf}
}

@article{oatesBayesianProbabilisticNumerical2017,
  title = {Bayesian {{Probabilistic Numerical Methods}} for {{Industrial Process Monitoring}}},
  author = {Oates, {\relax Chris. J} and Cockayne, Jon and Aykroyd, Robert G.},
  year = {2017},
  pages = {1--30},
  abstract = {The use of high-power industrial equipment, such as large-scale mixing equipment or a hydrocyclone for separation of particles in liquid suspension, demands careful monitoring to ensure correct operation. The task of monitoring the liquid suspension can be posed as a time-evolving inverse problem and solved with Bayesian statistical methods. In this paper, we extend Bayesian methods to incorporate statistical models for the error that is incurred in the numerical solution of the physical governing equations. This enables full uncertainty quantification within a principled computation-precision trade-off, in contrast to the over-confident inferences that are obtained when numerical error is ignored. The method is cast with a sequential Monte Carlo framework and an optimised implementation is provided in Python.},
  keywords = {bilistic meshless methods,electrical tomography,inverse problems,partial differential equations,proba-,sequential monte carlo},
  file = {/home/yanni/Zotero/storage/S7TSQ97T/Oates, Cockayne, Aykroyd - 2017 - Bayesian Probabilistic Numerical Methods for Industrial Process Monitoring.pdf}
}

@article{ohaganBayesianNumericalAnalysis,
  title = {Some {{Bayesian Numerical Analysis}}},
  author = {O'Hagan, A.},
  file = {/home/yanni/Zotero/storage/9JAH3QTQ/O'Hagan - Unknown - Some Bayesian Numerical Analysis.pdf}
}

@book{oksendalStochasticDifferentialEquations2003,
  title = {Stochastic {{Differential Equations}}},
  author = {{\O}ksendal, Bernt},
  year = {2003},
  series = {Universitext},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14394-6},
  isbn = {978-3-540-04758-2 978-3-642-14394-6},
  keywords = {Boundary value problem,differential equations,filtering problem,filtering theory,linear optimization,Martingale,mathematical finance,optimal filtering,partial differential equations,Random variable,Stochastic calculus,stochastic control,stochastic differential equations,Uniform integrability},
  file = {/home/yanni/Zotero/storage/UFZ4ZQ9C/√òksendal - 2003 - Stochastic Differential Equations.pdf}
}

@article{olkinDistanceTwoRandom1982,
  title = {The Distance between Two Random Vectors with given Dispersion Matrices},
  author = {Olkin, I. and Pukelsheim, F.},
  year = {1982},
  journal = {Linear Algebra and Its Applications},
  issn = {00243795},
  doi = {10.1016/0024-3795(82)90112-4},
  abstract = {For two p-dimensional random vectors X and Y with dispersion matrices {$\Sigma$}11 and {$\Sigma$}22, respectively, we determine that covariance matrix {$\Psi$}0 of X and Y that minimizes the L2-distance between X and Y. There is a dual to this problem that is of interest in another context. \textcopyright{} 1982.},
  file = {/home/yanni/Zotero/storage/MKMX2FFF/Olkin, Pukelsheim - 1982 - The distance between two random vectors with given dispersion matrices.pdf}
}

@article{osborneGaussianProcessesPrediction,
  title = {Gaussian {{Processes}} for {{Prediction}}},
  author = {Osborne, Michael},
  pages = {69},
  abstract = {We propose a powerful prediction algorithm built upon Gaussian processes (GPs). They are particularly useful for their flexibility, facilitating accurate prediction even in the absence of strong physical models.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VJHDYBC6/Osborne - Gaussian Processes for Prediction.pdf}
}

@inproceedings{osborneRealTimeInformationProcessing2008,
  title = {Towards {{Real-Time Information Processing}} of {{Sensor Network Data Using Computationally Efficient Multi-output Gaussian Processes}}},
  booktitle = {2008 {{International Conference}} on {{Information Processing}} in {{Sensor Networks}} (Ipsn 2008)},
  author = {Osborne, M. A. and Roberts, S. J. and Rogers, A. and Ramchurn, S. D. and Jennings, N. R.},
  year = {2008},
  month = apr,
  pages = {109--120},
  doi = {10.1109/IPSN.2008.25},
  abstract = {In this paper, we describe a novel, computationally efficient algorithm that facilitates the autonomous acquisition of readings from sensor networks (deciding when and which sensor to acquire readings from at any time), and which can, with minimal domain knowledge, perform a range of information processing tasks including modelling the accuracy of the sensor readings, predicting the value of missing sensor readings, and predicting how the monitored environmental variables will evolve into the future. Our motivating scenario is the need to provide situational awareness support to first responders at the scene of a large scale incident, and to this end, we describe a novel iterative formulation of a multi-output Gaussian process that can build and exploit a probabilistic model of the environmental variables being measured (including the correlations and delays that exist between them). We validate our approach using data collected from a network of weather sensors located on the south coast of England.},
  keywords = {Chemical sensors,Computer networks,Computerized monitoring,Gaussian processes,information processing,Information processing,Mobile computing,Personal digital assistants,Predictive models,sensor network,Sensor phenomena and characterization,Sensor systems},
  file = {/home/yanni/Zotero/storage/5NEPLZKX/Osborne et al. - 2008 - Towards Real-Time Information Processing of Sensor.pdf;/home/yanni/Zotero/storage/GHFRYAXN/4505467.html}
}

@techreport{owhadiBayesianNumericalHomogenization2015,
  title = {Bayesian {{Numerical Homogenization}}},
  author = {Owhadi, Houman},
  year = {2015},
  abstract = {Numerical homogenization, i.e. the finite-dimensional approximation of solution spaces of PDEs with arbitrary rough coefficients, requires the identification of accurate basis elements. These basis elements are oftentimes found after a laborious process of scientific investigation and plain guesswork. Can this identification problem be facilitated? Is there a general recipe/decision framework for guiding the design of basis elements? We suggest that the answer to the above questions could be positive based on the reformulation of numerical homogenization as a Bayesian Inference problem in which a given PDE with rough coefficients (or multi-scale operator) is excited with noise (random right hand side/source term) and one tries to estimate the value of the solution at a given point based on a finite number of observations. We apply this reformulation to the identification of bases for the numerical homogenization of arbitrary integro-differential equations and show that these bases have optimal recovery properties. In particular we show how Rough Polyharmonic Splines can be rediscovered as the optimal solution of a Gaussian filtering problem.},
  file = {/home/yanni/Zotero/storage/Q2J4IF96/Owhadi - 2015 - Bayesian Numerical Homogenization.pdf}
}

@techreport{owhadiBrittlenessBayesianInference2015,
  title = {On the {{Brittleness}} of {{Bayesian Inference}}},
  author = {Owhadi, Houman and Scovel, Clint and Sullivan, Tim},
  year = {2015},
  abstract = {With the advent of high-performance computing, Bayesian methods are increasingly popular tools for the quantification of uncertainty throughout science and industry. Since these methods impact the making of sometimes critical decisions in increasingly complicated contexts, the sensitivity of their posterior conclusions with respect to the underlying models and prior beliefs is a pressing question for which there currently exist positive and negative results. We report new results suggesting that, although Bayesian methods are robust when the number of possible outcomes is finite or when only a finite number of marginals of the data-generating distribution are unknown, they could be generically brittle when applied to continuous systems (and their discretizations) with finite information on the data-generating distribution. If closeness is defined in terms of the total variation metric or the matching of a finite system of generalized moments, then (1) two practitioners who use arbitrarily close models and observe the same (possibly arbitrarily large amount of) data may reach opposite conclusions; and (2) any given prior and model can be slightly perturbed to achieve any desired posterior conclusions. The mechanism causing brittlenss/robustness suggests that learning and robustness are antagonistic requirements and raises the question of a missing stability condition for using Bayesian Inference in a continuous world under finite information. The application of Bayes' theorem in the form of Bayesian inference has fueled an ongoing debate with practical consequences in science, industry, medicine and law [21]. One commonly-cited justification for the application of Bayesian reasoning is Cox's theorem [15], which has been interpreted as stating that any 'natural' extension of Aristotelian logic to uncertain contexts must be Bayesian [34]. It has now been shown that Cox's theorem as originally formulated is incomplete [28] and there is debate about the 'naturality' of the additional assumptions required for its validity [1, 20, 29, 31], e.g. the assumption that knowledge can be always represented in the form of a {$\sigma$}-additive probability measure that assigns to each measurable event a single real-valued probability.},
  file = {/home/yanni/Zotero/storage/FXA6LZKL/Owhadi, Scovel, Sullivan - 2015 - On the Brittleness of Bayesian Inference.pdf}
}

@techreport{owhadiConditioningGaussianMeasure2015,
  title = {Conditioning {{Gaussian}} Measure on {{Hilbert}} Space},
  author = {Owhadi, Houman and Scovel, Clint},
  year = {2015},
  abstract = {For a Gaussian measure on a separable Hilbert space with covariance operator C, we show that the family of conditional measures associated with conditioning on a closed subspace S {$\perp$} are Gaussian with covariance operator the short S(C) of the operator C to S. We provide two proofs. The first uses the theory of Gaussian Hilbert spaces and a characterization of the shorted operator by Andersen and Trapp. The second uses recent developments by Corach, Maestripieri and Stojanoff on the relationship between the shorted operator and C-symmetric oblique projections onto S {$\perp$}. To obtain the assertion when such projections do not exist, we develop an approximation result for the shorted operator by showing, for any positive operator A, how to construct a sequence of approximating operators A n which possess A n-symmetric oblique projections onto S {$\perp$} such that the sequence of shorted operators S(A n) converges to S(A) in the weak operator topology. This result combined with the martingale convergence of random variables associated with the corresponding approximations C n establishes the main assertion in general. Moreover, it in turn strengthens the approximation theorem for shorted operator when the operator is trace class; then the sequence of shorted operators S(A n) converges to S(A) in trace norm.},
  isbn = {1506.04208v2},
  keywords = {()},
  file = {/home/yanni/Zotero/storage/ZPN2CIEZ/Owhadi, Scovel - 2015 - Conditioning Gaussian measure on Hilbert space.pdf}
}

@techreport{owhadiKernelFlowsLearning2018,
  title = {Kernel {{Flows}}: From Learning Kernels from Data into the Abyss},
  author = {Owhadi, Houman and Yoo, Gene Ryan},
  year = {2018},
  abstract = {Learning can be seen as approximating an unknown function by interpolating the training data. Kriging offers a solution to this problem based on the prior specification of a kernel. We explore a numerical approximation approach to kernel selection/construction based on the simple premise that a kernel must be good if the number of interpolation points can be halved without significant loss in accuracy (measured using the intrinsic RKHS norm {$\cdot$} {$\cdot$} associated with the kernel). We first test and motivate this idea on a simple problem of recovering the Green's function of an elliptic PDE (with inhomogeneous coefficients) from the sparse observation of one of its solutions. Next we consider the problem of learning non-parametric families of deep kernels of the form K 1 (F n (x), F n (x)) with F n+1 = (I d + G n+1) \textbullet{} F n and G n+1 {$\in$} spanK 1 (F n (x i), {$\cdot$}). With the proposed approach constructing the kernel becomes equivalent to integrating a stochastic data driven dynamical system, which allows for the training of very deep (bottomless) networks and the exploration of their properties. These networks learn by constructing flow maps in the kernel and input spaces via incremental data-dependent deformations/perturbations (appearing as the cooperative counterpart of adversarial examples) and, at profound depths, they (1) can achieve accurate classification from only one data point per class (2) appear to learn archetypes of each class (3) expand distances between points that are in different classes and contract distances between points in the same class. For kernels parameterized by the weights of Convolutional Neural Networks, minimizing approximation errors incurred by halving random subsets of interpolation points, appears to outperform training (the same CNN architecture) with relative entropy and dropout.},
  file = {/home/yanni/Zotero/storage/ZWGIAK69/Owhadi, Yoo - 2018 - Kernel Flows from learning kernels from data into the abyss.pdf}
}

@article{owhadiQualitativeRobustnessBayesian2017,
  title = {Qualitative {{Robustness}} in {{Bayesian Inference}}},
  author = {Owhadi, Houman and Scovel, Clint},
  year = {2017},
  journal = {ESAIM - Probability and Statistics},
  volume = {21},
  pages = {251--274},
  issn = {12623318},
  doi = {10.1051/ps/2017014},
  abstract = {The practical implementation of Bayesian inference requires numerical approximation when closed-form expressions are not available. What types of accuracy (convergence) of the numerical approximations guarantee robustness and what types do not? In particular, is the recursive application of Bayes' rule robust when subsequent data or posteriors are approximated? When the prior is the push forward of a distribution by the map induced by the solution of a PDE, in which norm should that solution be approximated? Motivated by such questions, we investigate the sensitivity of the distribution of posterior distributions (i.e. of posterior distribution-valued random variables, randomized through the data) with respect to perturbations of the prior and data-generating distributions in the limit when the number of data points grows towards infinity.},
  keywords = {Bayesian inference,Hampel,qualitative robustness,stability},
  file = {/home/yanni/Zotero/storage/8GAMYVDE/1411.3984.pdf}
}

@techreport{panaretosStatisticalAspectsWasserstein2018,
  title = {Statistical {{Aspects}} of {{Wasserstein Distances}}},
  author = {Panaretos, Victor M and Zemel, Yoav},
  year = {2018},
  abstract = {Wasserstein distances are metrics on probability distributions inspired by the problem of optimal mass transportation. Roughly speaking, they measure the minimal effort required to reconfigure the probability mass of one distribution in order to recover the other distribution. They are ubiquitous in mathematics, with a long history that has seen them catal-yse core developments in analysis, optimization, and probability. Beyond their intrinsic mathematical richness, they possess attractive features that make them a versatile tool for the statistician: they can be used to derive weak convergence and convergence of moments, and can be easily bounded; they are well-adapted to quantify a natural notion of perturbation of a probability distribution; and they seamlessly incorporate the geometry of the domain of the distributions in question, thus being useful for contrasting complex objects. Consequently, they frequently appear in the development of statistical theory and inferential methodology, and have recently become an object of inference in themselves. In this review, we provide a snapshot of the main concepts involved in Wasserstein distances and optimal transportation, and a succinct overview of some of their many statistical aspects.},
  keywords = {62G99,62M99 (secondary),deformation map,empirical optimal transport,Fr√©chet mean,goodness-of-fit,inference,Monge-Kantorovich problem,optimal coupling,prob-ability metric,transportation of measure,warping and registration,Wasserstein space AMS subject classification: 62-00 (primary)},
  file = {/home/yanni/Zotero/storage/83CE2X7U/Panaretos, Zemel - 2018 - Statistical Aspects of Wasserstein Distances.pdf}
}

@article{pangAsymptoticallyEfficientParameter2017,
  title = {Asymptotically Efficient Parameter Estimation for Ordinary Differential Equations},
  author = {Pang, TianXiao and Yan, PeiSi and Zhou, Harrison H.},
  year = {2017},
  month = nov,
  journal = {Sci. China Math.},
  volume = {60},
  number = {11},
  pages = {2263--2286},
  issn = {1674-7283, 1869-1862},
  doi = {10.1007/s11425-017-9155-0},
  abstract = {Parameter estimation for ordinary differential equations arises in many fields of science and engineering. To be the best of our knowledge, traditional methods are often either computationally intensive or inaccurate for statistical inference. Ramsay et al. (2007) proposed a generalized profiling procedure. It is easily implementable and has been demonstrated to have encouraging numerical performance. However, little is known about statistical properties of this procedure. In this paper, we provide a theoretical justification of the generalized profiling procedure. Under some regularity conditions, the procedure is shown to be consistent for a broad range of tuning parameters. When the tuning parameters are sufficiently large, the procedure can be further shown to be asymptotically normal and efficient.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/MK69XPX5/Pang et al. - 2017 - Asymptotically efficient parameter estimation for .pdf}
}

@article{PDEFinanceNotes2011,
  title = {{{PDE}} for {{Finance}} Notes {{NYU}}},
  year = {2011},
  pages = {1--14},
  file = {/home/yanni/Zotero/storage/QZULZ7NJ/Unknown - 2011 - PDE for Finance notes NYU.pdf}
}

@article{peiferInstitutionEngineeringTechnology2007,
  title = {The {{Institution}} of {{Engineering}} and {{Technology}}},
  author = {Peifer, M and Timmer, J},
  year = {2007},
  journal = {IET Syst. Biol},
  number = {1},
  pages = {78},
  doi = {10.1049/iet-syb:20060067},
  abstract = {In silico investigations by simulating dynamical models of biochemical processes play an important role in systems biology. If the parameters of a model are unknown, results from simulation studies can be misleading. Such a scenario can be avoided by estimating the parameters before analysing the system. Almost all approaches for estimating parameters in ordinary differential equations have either a small convergence region or suffer from an immense computational cost. The method of multiple shooting can be situated in between of these extremes. In spite of its good convergence and stability properties, the literature regarding the practical implementation and providing some theoretical background is rarely available. All necessary information for a successful implementation is supplied here and the basic facts of the involved numerics are discussed. To show the performance of the method, two illustrative examples are discussed. 1 Introduction The central idea of systems biology to learn about biological systems by the analysis of mathematical models of these systems is hampered by the fact that parameters like rate constants are not known. The challenging problem of estimating parameters in ordinary differential equations (ODEs) from partially observed noisy data appears therefore in systems biology. Since most of the ODEs are non-linear, all methods regarding parameter estimation are showing an interplay between simulating the trajectory and optimisation. The simulation of the trajectory is usually done by convenient ODE solvers; whereas the optimisation differs drastically and can be classified into global or local optimisation procedures. Methods based on global minimisation routines are for example random search and adaptive stochastic methods [1-4], clustering methods [5], evolutionary computation [6] and simulated annealing. A detailed discussion of these methods with respect to parameter identification in ODEs is given in the work of Banga et al. [7]. The disadvantage of stochastic optimisers is mainly their immense computational cost which is the price for the flexibility and stability of these methods. On the other side, local optimisation procedures such as sequential quadratic programming (SQP), Newton methods, quasi-Newton methods and so on are computa-tionally efficient, but they tend to converge to local minima. In the case of parameter identification in ODEs, the problem of convergence to local minima is predominant if the so-called initial value approach is considered. This approach utilises the fact that the trajectory is uniquely determined by the parameters and initial values. Minimising a maximum-likelihood functional with respect to parameters and initial values should therefore solve the inverse problem. The situation stated earlier further suggests that there is a trade-off between computational efficiency and stability for estimating parameters in ODEs. In comparison to the initial value approach, multiple shooting provides enhanced stability with only a slight increase of the computational cost. The method was introduced by Stoer and Bulirsch in the early seventies [8] and was substantially enhanced and mathematically analysed by Bock [9-11]. Here, some of the well-elaborated mathematical details are presented, but always in scope of practically implementing these ideas. Keeping track on the algorithmic issues can be regarded as the major intension of this article. Since this aspect is neglected in the literature so far, the accessibility and re-implementation of multiple shooting is currently limited. 2 Estimation problem Suppose that a dynamical system is given by the d-dimensional state variable x(t) [ R d at time t [ I {$\frac{1}{4}$} [t 0 , t f ], which is the unique and differentiable solution of the initial value problem \_ x\dh t\TH{} {$\frac{1}{4}$} f \dh x\dh t\TH; t; p\TH{} x\dh t 0 \TH{} {$\frac{1}{4}$} x 0 \dh 1\TH{} The right-hand side of the ODE depends on some parameters p [ R n p. It is further assumed that f is continuously differentiable with respect to state x and parameters p. Let Y ij denote the data of measurement i {$\frac{1}{4}$} 1,. .. , n and of observable j {$\frac{1}{4}$} 1,. .. , obs, whereas n represents the total amount of data and obs is the number of observables. Moreover, data Y ij satisfy the following observation equation Y ij {$\frac{1}{4}$} g j \dh t i ; p\TH{} \th{} s ij e ij j {$\frac{1}{4}$} 1;. .. ; obs \dh 2\TH{} for some observation function g: R d ! R obs , d ! obs, s ij. 0, and e ij s are independent and standard Gaussian distributed random variables. Sample points t i are ordered such that t 0 t 1 , \'A \'A \'A , t n , t f and observation function g(.) is \#},
  file = {/home/yanni/Zotero/storage/F3U3XD49/Peifer, Timmer - 2007 - The Institution of Engineering and Technology(2).pdf}
}

@article{peiferInstitutionEngineeringTechnology2007a,
  title = {The {{Institution}} of {{Engineering}} and {{Technology}}},
  author = {Peifer, M and Timmer, J},
  year = {2007},
  journal = {IET Syst. Biol},
  number = {1},
  pages = {78},
  doi = {10.1049/iet-syb:20060067},
  abstract = {In silico investigations by simulating dynamical models of biochemical processes play an important role in systems biology. If the parameters of a model are unknown, results from simulation studies can be misleading. Such a scenario can be avoided by estimating the parameters before analysing the system. Almost all approaches for estimating parameters in ordinary differential equations have either a small convergence region or suffer from an immense computational cost. The method of multiple shooting can be situated in between of these extremes. In spite of its good convergence and stability properties, the literature regarding the practical implementation and providing some theoretical background is rarely available. All necessary information for a successful implementation is supplied here and the basic facts of the involved numerics are discussed. To show the performance of the method, two illustrative examples are discussed. 1 Introduction The central idea of systems biology to learn about biological systems by the analysis of mathematical models of these systems is hampered by the fact that parameters like rate constants are not known. The challenging problem of estimating parameters in ordinary differential equations (ODEs) from partially observed noisy data appears therefore in systems biology. Since most of the ODEs are non-linear, all methods regarding parameter estimation are showing an interplay between simulating the trajectory and optimisation. The simulation of the trajectory is usually done by convenient ODE solvers; whereas the optimisation differs drastically and can be classified into global or local optimisation procedures. Methods based on global minimisation routines are for example random search and adaptive stochastic methods [1-4], clustering methods [5], evolutionary computation [6] and simulated annealing. A detailed discussion of these methods with respect to parameter identification in ODEs is given in the work of Banga et al. [7]. The disadvantage of stochastic optimisers is mainly their immense computational cost which is the price for the flexibility and stability of these methods. On the other side, local optimisation procedures such as sequential quadratic programming (SQP), Newton methods, quasi-Newton methods and so on are computa-tionally efficient, but they tend to converge to local minima. In the case of parameter identification in ODEs, the problem of convergence to local minima is predominant if the so-called initial value approach is considered. This approach utilises the fact that the trajectory is uniquely determined by the parameters and initial values. Minimising a maximum-likelihood functional with respect to parameters and initial values should therefore solve the inverse problem. The situation stated earlier further suggests that there is a trade-off between computational efficiency and stability for estimating parameters in ODEs. In comparison to the initial value approach, multiple shooting provides enhanced stability with only a slight increase of the computational cost. The method was introduced by Stoer and Bulirsch in the early seventies [8] and was substantially enhanced and mathematically analysed by Bock [9-11]. Here, some of the well-elaborated mathematical details are presented, but always in scope of practically implementing these ideas. Keeping track on the algorithmic issues can be regarded as the major intension of this article. Since this aspect is neglected in the literature so far, the accessibility and re-implementation of multiple shooting is currently limited. 2 Estimation problem Suppose that a dynamical system is given by the d-dimensional state variable x(t) [ R d at time t [ I {$\frac{1}{4}$} [t 0 , t f ], which is the unique and differentiable solution of the initial value problem \_ x\dh t\TH{} {$\frac{1}{4}$} f \dh x\dh t\TH; t; p\TH{} x\dh t 0 \TH{} {$\frac{1}{4}$} x 0 \dh 1\TH{} The right-hand side of the ODE depends on some parameters p [ R n p. It is further assumed that f is continuously differentiable with respect to state x and parameters p. Let Y ij denote the data of measurement i {$\frac{1}{4}$} 1,. .. , n and of observable j {$\frac{1}{4}$} 1,. .. , obs, whereas n represents the total amount of data and obs is the number of observables. Moreover, data Y ij satisfy the following observation equation Y ij {$\frac{1}{4}$} g j \dh t i ; p\TH{} \th{} s ij e ij j {$\frac{1}{4}$} 1;. .. ; obs \dh 2\TH{} for some observation function g: R d ! R obs , d ! obs, s ij. 0, and e ij s are independent and standard Gaussian distributed random variables. Sample points t i are ordered such that t 0 t 1 , \'A \'A \'A , t n , t f and observation function g(.) is \#},
  file = {/home/yanni/Zotero/storage/EQZ3MI3X/Peifer, Timmer - 2007 - The Institution of Engineering and Technology.pdf}
}

@article{peiferParameterEstimationOrdinary2007,
  title = {Parameter Estimation in Ordinary Differential Equations for Biochemical Processes Using the Method of Multiple Shooting},
  author = {Peifer, M and Timmer, J},
  year = {2007},
  journal = {IET Systems Biology},
  volume = {1},
  number = {2},
  pages = {78--88},
  publisher = {{IET}}
}

@techreport{pekarevShortsOperatorsExtremal1992,
  title = {Shorts of Operators and Some Extremal Problems},
  author = {Pekarev, E L},
  year = {1992},
  journal = {Acta Sei. Math},
  volume = {56},
  pages = {147--163},
  abstract = {Let J(? be a Hilbert space, if its closed subspace, and A a non-negative operator in As M. G. KREIN [1] showed, the set of operators contains a maximum element, denoted by A x and called the short of A to HP: The properties of the correspondence A{$>$}-*A\# were studied in detail in [2] and found various applications to the theory of characteristic operator-functions [3,4], electrical networks [5, 6], Lebesgue decomposition of nonnegative operators and positive definite operator-functions [7-9], operator means [10] and other problems. The notion of short was generalized to the case of non-closed if, which is the range of a bounded operator, and it became clear that shorting is closely connected with the operation of parallel addition arising in the theory of electrical networks and with its inverse operation, parallel subtraction. A recent work of S. L. ERIKSSON and LEUTWILLER [13], related to parallel addition , indicates essential connection between shorts and extreme points of some set of operators. In the present note we continue the study of this connection, and also give proofs to some assertions, announced earlier in [12, 14]. (0.1) X(A, \&) = X: 0 sXsA, 0i(X) a if (0.2) A z = max 3C(A, if). 10\guillemotleft},
  file = {/home/yanni/Zotero/storage/WH3F5D26/Pekarev - 1992 - Shorts of operators and some extremal problems.pdf}
}

@article{petersenHttpMatrixcookbookCom,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72},
  langid = {english},
  file = {/home/yanni/Zotero/storage/URHHE7BY/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@article{petraINFINITEDIMENSIONALBAYESIANINVERSE2014,
  title = {{{INFINITE-DIMENSIONAL BAYESIAN INVERSE PROBLEMS}}},
  author = {Petra, Noemi and Martin, James and Stadler, Georg and Ghattas, Omar},
  year = {2014},
  volume = {36},
  number = {4},
  pages = {1525--1555},
  keywords = {10,1137,130934805,35q62,35q93,35r30,49m15,62f15,65c40,65c60,86a40,ams subject classifications,bayesian inference,doi,ice sheet dynamics,infinite-dimensional inverse problems,low-rank approximation,mcmc,stochastic newton,tion,uncertainty quantifica-},
  file = {/home/yanni/Zotero/storage/JBNUHZ2D/130934805.pdf}
}

@misc{peyreComputationalOptimalTransport2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2020},
  month = mar,
  number = {arXiv:1803.00567},
  eprint = {arXiv:1803.00567},
  publisher = {{arXiv}},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/DDAB23E4/Peyr√© and Cuturi - 2020 - Computational Optimal Transport.pdf}
}

@article{pigoliDistancesInferenceCovariance2014,
  title = {Distances and Inference for Covariance Operators},
  author = {Pigoli, Davide and Aston, John A D and Dryden, Ian L and Secchi, Piercesare},
  year = {2014},
  journal = {Biometrika},
  volume = {101},
  number = {2},
  pages = {409--422},
  doi = {10.1093/biomet/asu008},
  abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fr\'echet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative pho-netic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages.},
  keywords = {Functional data analysis,Procrustes analysis,Shape analysis,Some key words: Distance metric},
  file = {/home/yanni/Zotero/storage/MP9GZZFM/Pigoli et al. - 2014 - Distances and inference for covariance operators.pdf}
}

@article{pigoliKrigingPredictionManifoldvalued2016,
  title = {Kriging Prediction for Manifold-Valued Random Fields},
  author = {Pigoli, Davide and Menafoglio, Alessandra and Secchi, Piercesare},
  year = {2016},
  month = mar,
  journal = {Journal of Multivariate Analysis},
  volume = {145},
  pages = {117--131},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2015.12.006},
  abstract = {The statistical analysis of data belonging to Riemannian manifolds is becoming increasingly important in many applications, such as shape analysis, diffusion tensor imaging and the analysis of covariance matrices. In many cases, data are spatially distributed but it is not trivial to take into account spatial dependence in the analysis because of the non linear geometry of the manifold. This work proposes a solution to the problem of spatial prediction for manifold valued data, with a particular focus on the case of positive definite symmetric matrices. Under the hypothesis that the dispersion of the observations on the manifold is not too large, data can be projected on a suitably chosen tangent space, where an additive model can be used to describe the relationship between response variable and covariates. Thus, we generalize classical kriging prediction, dealing with the spatial dependence in this tangent space, where well established Euclidean methods can be used. The proposed kriging prediction is applied to the matrix field of covariances between temperature and precipitation in Quebec, Canada.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/54U7P4TK/Pigoli et al. - 2016 - Kriging prediction for manifold-valued random fiel.pdf}
}

@misc{PII0047259X82,
  title = {{{PII}}: 0047-{{259X}}(82)90077-{{X}} | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  issn = {0047-259X},
  doi = {10.1016/0047-259X(82)90077-X},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/0047259X8290077X?token=98401E27D8C279150FED91B90457771A3124941FD1F3524711058D9F0501997F63CB8438572488953EB5CEA2E70D7A4E\&originRegion=eu-west-1\&originCreation=20221124143358},
  langid = {english},
  file = {/home/yanni/Zotero/storage/S4ST6S68/PII 0047-259X(82)90077-X  Elsevier Enhanced Read.pdf;/home/yanni/Zotero/storage/DXY7HHI5/0047259X8290077X.html}
}

@misc{PII0378375891,
  title = {{{PII}}: 0378-3758(91)90002-{{V}} | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  issn = {0378-3758},
  doi = {10.1016/0378-3758(91)90002-V},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/037837589190002V?token=E5074FD29931BF61E251721754F3F4627C4EE4EC063FF74C07D97C6C9CC3CB833391FAD3E6531F3F59CC5A5061D44C1B\&originRegion=eu-west-1\&originCreation=20210909134544},
  langid = {english},
  file = {/home/yanni/Zotero/storage/FMB9CJBR/037837589190002V.html}
}

@misc{PIIS0167473097,
  title = {{{PII}}: {{S0167-4730}}(97)00013-1 | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  doi = {10.1016/S0167-4730(97)00013-1},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0167473097000131?token=797C0ED25B6663969436B4180F90A931AE963BF59BD8D11A7D958568C856C771CEFBE527E3065A174B6569AE7094EE84\&originRegion=eu-west-1\&originCreation=20210611132722},
  langid = {english},
  file = {/home/yanni/Zotero/storage/Z7Z4E33D/S0167473097000131.html}
}

@misc{PIIS0167473097a,
  title = {{{PII}}: {{S0167-4730}}(97)00013-1 | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  doi = {10.1016/S0167-4730(97)00013-1},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0167473097000131?token=797C0ED25B6663969436B4180F90A931AE963BF59BD8D11A7D958568C856C771CEFBE527E3065A174B6569AE7094EE84\&originRegion=eu-west-1\&originCreation=20210611132722},
  langid = {english},
  file = {/home/yanni/Zotero/storage/WLYE3N6H/S0167473097000131.html}
}

@techreport{pillaiCharacterizingFunctionSpace2007,
  title = {Characterizing the {{Function Space}} for {{Bayesian Kernel Models Sayan Mukherjee}}},
  author = {Pillai, Natesh S and Wu, Qiang and Liang, Feng and Wolpert, Robert L and Pillai, S and Mukherjee, Sayan and Wolpert PILLAI, Robert L},
  year = {2007},
  journal = {Journal of Machine Learning Research},
  volume = {8},
  pages = {1769--1797},
  abstract = {Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator defined as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior specifications in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L\'evy processes, with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classification problem.},
  keywords = {Dirichlet processes,Gaussian processes,integral operator,L√©vy processes,non-parametric Bayesian methods,reproducing kernel Hilbert space},
  file = {/home/yanni/Zotero/storage/6VDVZJRT/Pillai et al. - 2007 - Characterizing the Function Space for Bayesian Kernel Models Sayan Mukherjee.pdf}
}

@misc{pinkusApproximatingRidgeFunctions1997,
  title = {Approximating by {{Ridge Functions}}},
  author = {Pinkus, Allan},
  year = {1997},
  abstract = {This paper surveys certain aspects of the study of ridge functions. We hope it will also encourage some readers to consider researching problems in this area. After first explaining what ridge functions are and giving various motivations for their study, we turn to the problem of presenting algorithms for approximating by ridge functions. We then touch upon the topic of determining the degree of approximation by ridge functions, and that of recognizing functions which are linear combinations of ridge functions.},
  file = {/home/yanni/Zotero/storage/LKJGHYW4/Pinkus - 1997 - Approximating by Ridge Functions.pdf;/home/yanni/Zotero/storage/TGN3CUT7/summary.html}
}

@article{powersFreeStatesCanonical1970,
  title = {Free States of the Canonical Anticommutation Relations},
  author = {Powers, Robert T. and St{\o}rmer, Erling},
  year = {1970},
  month = mar,
  journal = {Commun.Math. Phys.},
  volume = {16},
  number = {1},
  pages = {1--33},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/BF01645492},
  abstract = {Each gauge invariant generalized flee state \textasciitilde oA of the anticommutation relation algebra over a complex Hilbert space K is characterized by an operator A on K. It is shown that the cyclic representations induced by two gauge invariant generalized free states \textcent oA and e)B are quasi-equivalent if and only if the operators A\textasciitilde -- B {$\div$} and (I - A)\textasciitilde - (I - B)\textasciitilde{} are of Hilbert-Schmidt class. The combination of this result with results from the theory of isomorphisms of yon Neumann algebras yield necessary and sufficient conditions for the unitary equivalence of the cyclic representations induced by gauge invariant generalized free states.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/SSNTT3BL/Powers and St√∏rmer - 1970 - Free states of the canonical anticommutation relat.pdf}
}

@article{poytonParameterEstimationContinuoustime2006,
  title = {Parameter Estimation in Continuous-Time Dynamic Models Using Principal Differential Analysis},
  author = {Poyton, Aa and Varziri, {\relax MS} and McAuley, {\relax KB} and McLellan, {\relax PJ} and Ramsay, Jo},
  year = {2006},
  journal = {Computers and Chemical Engineering},
  volume = {30},
  pages = {698--708},
  doi = {10.1016/j.compchemeng.2005.11.008},
  abstract = {Principal differential analysis (PDA) is an alternative parameter estimation technique for differential equation models in which basis functions (e.g., B-splines) are fitted to dynamic data. Derivatives of the resulting empirical expressions are used to avoid solving differential equations when estimating parameters. Benefits and shortcomings of PDA were examined using a simple continuous stirred-tank reactor (CSTR) model. Although PDA required considerably less computational effort than traditional nonlinear regression, parameter estimates from PDA were less precise. Sparse and noisy data resulted in poor spline fits and misleading derivative information, leading to poor parameter estimates. These problems are addressed by a new iterative algorithm (iPDA) in which the spline fits are improved using model-based penalties. Parameter estimates from iPDA were unbiased and more precise than those from standard PDA. Issues that need to be resolved before iPDA can be used for more complex models are discussed.},
  keywords = {Dynamic models,Parameter estimation,Principal differential analysis},
  file = {/home/yanni/Zotero/storage/L9U8B5VN/Poyton et al. - 2006 - Parameter estimation in continuous-time dynamic models using principal differential analysis.pdf}
}

@book{pratoStochasticEquationsInfinite2014,
  title = {Stochastic {{Equations}} in {{Infinite Dimensions}}},
  author = {Prato, Giuseppe Da and Zabczyk, Jerzy},
  year = {2014},
  month = apr,
  publisher = {{Cambridge University Press}},
  abstract = {Now in its second edition, this book gives a systematic and self-contained presentation of basic results on stochastic evolution equations in infinite dimensional, typically Hilbert and Banach, spaces. In the first part the authors give a self-contained exposition of the basic properties of probability measure on separable Banach and Hilbert spaces, as required later; they assume a reasonable background in probability theory and finite dimensional stochastic processes. The second part is devoted to the existence and uniqueness of solutions of a general stochastic evolution equation, and the third concerns the qualitative properties of those solutions. Appendices gather together background results from analysis that are otherwise hard to find under one roof. This revised edition includes two brand new chapters surveying recent developments in the area and an even more comprehensive bibliography, making this book an essential and up-to-date resource for all those working in stochastic differential equations.},
  googlebooks = {bxkmAwAAQBAJ},
  isbn = {978-1-107-05584-1},
  langid = {english},
  keywords = {Mathematics / Differential Equations / General,Mathematics / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/home/yanni/Documents/Imperial/Books/Stochastic Equations in Infinite Dimensions Prato.pdf}
}

@article{qianMomentumTermGradient1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  year = {1999},
  journal = {Neural Networks},
  issn = {08936080},
  doi = {10.1016/S0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  keywords = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,Momentum,Speed of convergence},
  file = {/home/yanni/Zotero/storage/BLNWVGZE/Qian - 1999 - On the momentum term in gradient descent learning algorithms.pdf}
}

@misc{QuickStartGuide,
  title = {Quick Start Guide [{{Zotero Documentation}}]},
  howpublished = {https://www.zotero.org/support/quick\_start\_guide},
  file = {/home/yanni/Zotero/storage/ZEH44YSG/quick_start_guide.html}
}

@techreport{rackauckasComparisonAutomaticDifferentiation,
  title = {A {{Comparison}} of {{Automatic Differentiation}} and {{Continuous Sensitivity Analysis}} for {{Derivatives}} of {{Differential Equation Solutions}}},
  author = {Rackauckas, Christopher and Ma, Yingbo and Dixit, Vaibhav and Guo, Xingjian and Innes, Mike and Revels, Jarrett and Nyberg, Joakim and Ivaturi, Vijay},
  abstract = {The derivatives of differential equation solutions are commonly used as model diagnostics and as part of parameter estimation routines. In this manuscript we investigate an implementation of Discrete local Sensitivity Analysis via Automatic Differentiation (DSAAD). A non-stiff Lotka-Volterra model, a discretization of the two dimensional (N \texttimes{} N) Brusselator stiff reaction-diffusion PDE, a stiff non-linear air pollution and a non-stiff pharmacokinetic/pharmacodynamic (PK/PD) model were used as prototype models for this investigation. Our benchmarks show that on sufficiently small ({$<$}100 parameters) stiff and non-stiff systems of ODEs, forward-mode DSAAD is more efficient than both reverse-mode DSAAD and continuous forward/adjoint sensitivity analysis. The scalability of continuous adjoint methods is shown to result in better efficiency for larger ODE systems such as PDE discretizations. In addition to testing efficiency, results on test equations demonstrate the applicability of DSAAD to differential-algebraic equations, delay differential equations, and hybrid differential equation systems where the event timing and effects are dependent on model parameters. Together, these results show that language-level automatic differentiation is an efficient method for calculating local sensitivities of a wide range of differential equation models.},
  file = {/home/yanni/Zotero/storage/HZ9YAZB8/Rackauckas et al. - Unknown - A Comparison of Automatic Differentiation and Continuous Sensitivity Analysis for Derivatives of Different.pdf}
}

@article{rackauckasDiffEqFluxJlJulia2019,
  title = {{{DiffEqFlux}}.Jl - {{A Julia Library}} for {{Neural Differential Equations}}},
  author = {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and Bettencourt, Jesse and White, Lyndon and Dixit, Vaibhav},
  year = {2019},
  pages = {1--17},
  abstract = {DiffEqFlux.jl is a library for fusing neural networks and differential equations. In this work we describe differential equations from the viewpoint of data science and discuss the complementary nature between machine learning models and differential equations. We demonstrate the ability to incorporate DifferentialEquations.jl-defined differential equation problems into a Flux-defined neural network, and vice versa. The advantages of being able to use the entire DifferentialEquations.jl suite for this purpose is demonstrated by counter examples where simple integration strategies fail, but the sophisticated integration strategies provided by the DifferentialEquations.jl library succeed. This is followed by a demonstration of delay differential equations and stochastic differential equations inside of neural networks. We show high-level functionality for defining neural ordinary differential equations (neural networks embedded into the differential equation) and describe the extra models in the Flux model zoo which includes neural stochastic differential equations. We conclude by discussing the various adjoint methods used for backpropogation of the differential equation solvers. DiffEqFlux.jl is an important contribution to the area, as it allows the full weight of the differential equation solvers developed from decades of research in the scientific computing field to be readily applied to the challenges posed by machine learning and data science.},
  file = {/home/yanni/Zotero/storage/U86CTTUN/Rackauckas et al. - 2019 - DiffEqFlux.jl - A Julia Library for Neural Differential Equations.pdf}
}

@article{rackauckasGeneralizedPhysicsInformedLearning,
  title = {Generalized {{Physics-Informed Learning Through Language-Wide Differentiable Programming}}},
  author = {Rackauckas, Chris and Edelman, Alan and Fischer, Keno and Innes, Mike and Saba, Elliot and Shah, Viral B and Tebbutt, Will},
  pages = {6},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning to allow for data-driven physicsinformed modeling approaches. However, re-targeting existing scientific computing workloads to machine learning frameworks is both costly and limiting, as scientific simulations tend to use the full feature set of a general purpose programming language. In this manuscript we develop an infrastructure for incorporating deep learning into existing scientific computing code through Differentiable Programming ({$\partial$}P ). We describe a {$\partial$}P system that is able to take gradients of full Julia programs, making Automatic Differentiation a first class language feature and compatibility with deep learning pervasive. Our system utilizes the one-language nature of Julia package development to augment the existing package ecosystem with deep learning, supporting almost all language constructs (control flow, recursion, mutation, etc.) while generating highperformance code without requiring any user intervention or refactoring to stage computations. We showcase several examples of physics-informed learning which directly utilizes this extension to existing simulation code: neural surrogate models, machine learning on simulated quantum hardware, and data-driven stochastic dynamical model discovery with neural stochastic differential equations.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/RY5AR4EG/Rackauckas et al. - Generalized Physics-Informed Learning Through Lang.pdf}
}

@techreport{rackauckasUniversalDifferentialEquations2020,
  title = {Universal {{Differential Equations}} for {{Scientific Machine Learning}}},
  author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
  year = {2020},
  abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring "big data". In this work we develop a new methodology, universal differential equations (UDEs), which augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations,},
  file = {/home/yanni/Zotero/storage/RD87Y82H/Rackauckas et al. - 2020 - Universal Differential Equations for Scientific Machine Learning.pdf}
}

@techreport{raissiMachineLearningLinear,
  title = {Machine {{Learning}} of {{Linear Differential Equations}} Using {{Gaussian Processes}}},
  author = {Raissi, Maziar and Karniadakis, George Em},
  abstract = {This work leverages recent advances in probabilistic machine learning to discover conservation laws expressed by parametric linear equations. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or "black-box" computer simulations.},
  keywords = {differential equations,Gaussian processes,inverse problems,probabilistic machine learning,uncertainty quantification},
  file = {/home/yanni/Zotero/storage/N38G2IFX/Raissi, Karniadakis - Unknown - Machine Learning of Linear Differential Equations using Gaussian Processes.pdf}
}

@article{raissiNumericalGaussianProcesses2018,
  title = {Numerical {{Gaussian}} Processes for Time-Dependent and Nonlinear Partial Differential Equations},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  year = {2018},
  journal = {SIAM Journal on Scientific Computing},
  volume = {40},
  number = {1},
  pages = {A172--A198},
  issn = {10957197},
  doi = {10.1137/17M1120762},
  abstract = {We introduce the concept of numerical Gaussian processes, which we define as Gaussian processes with covariance functions resulting from temporal discretization of time-dependent partial differential equations. Numerical Gaussian processes, by construction, are designed to deal with cases where (a) all we observe are noisy data on black-box initial conditions, and (b) we are interested in quantifying the uncertainty associated with such noisy data in our solutions to time-dependent partial differential equations. Our method circumvents the need for spatial discretization of the differential operators by proper placement of Gaussian process priors. This is an attempt to construct structured and data-efficient learning machines, which are explicitly informed by the underlying physics that possibly generated the observed data. The effectiveness of the proposed approach is demonstrated through several benchmark problems involving linear and nonlinear time-dependent operators. In all examples, we are able to recover accurate approximations of the latent solutions, and consistently propagate uncertainty, even in cases involving very long time integration.},
  keywords = {Bayesian modeling,Linear multistep methods,Probabilistic machine learning,Runge-Kutta methods,Uncertainty quantification},
  file = {/home/yanni/Zotero/storage/9IFCBY66/Raissi, Perdikaris, Karniadakis - 2018 - Numerical Gaussian processes for time-dependent and nonlinear partial differential equations.pdf}
}

@article{ramsayParameterEstimationDifferential2007,
  title = {Parameter Estimation for Differential Equations: A Generalized Smoothing Approach},
  author = {Ramsay, J O and Hooker, G and Campbell, D and Cao, J},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {5},
  pages = {741--796},
  doi = {10.1111/j.1467-9868.2007.00610.x},
  abstract = {Summary. We propose a new method for estimating parameters in models that are defined by a system of non-linear differential equations. Such equations represent changes in system outputs by linking the behaviour of derivatives of a process to the behaviour of the process itself. Current methods for estimating parameters in differential equations from noisy data are computationally intensive and often poorly suited to the realization of statistical objectives such as inference and interval estimation. The paper describes a new method that uses noisy measurements on a subset of variables to estimate the parameters defining a system of non-linear differential equations. The approach is based on a modification of data smoothing methods along with a generalization of profiled estimation. We derive estimates and confidence intervals, and show that these have low bias and good coverage properties respectively for data that are simulated from models in chemical engineering and neurobiology. The performance of the method is demonstrated by using real world data from chemistry and from the progress of the autoimmune disease lupus.},
  keywords = {Differential equation,Dynamic system,Estimating equation,Functional data analysis,Gauss,Newton method,Parameter cascade,Profiled estimation}
}

@techreport{RangeClosednessInfsup2013,
  title = {Range Closedness and the Inf-Sup Conditions},
  year = {2013},
  abstract = {Notes by Ibrahim Al Balushi \textbackslash S1 Necessity and sufficiency for invertibility In this section we derive a sufficient and necessary condition for existence of an inverse of a bounded linear map on Banach spaces. The following result, known as Banach's bounded inverse theorem, is an immediate consequence of the open mapping theorem: Let X, Y be Banach spaces and let A : X \textrightarrow{} Y be a bounded linear map. Suppose that A is invertible, then A -1 : Y \textrightarrow{} X is also bounded. Suppose that A is as in the context of the previous result. Then x = A -1 Ax {$\leq$} cAx {$\forall$}x {$\in$} X. (1.1) Definition 1.1. We say that A is bounded below if x {$\leq$} cAx for all x {$\in$} X for some c {$>$} 0. Remark. Note that if A is as such, then Ker(A) = 0, i.e., A is injective. It turns out that (1.1) is not sufficient to guarantee invertibility of A. Lemma 1.2. Let X, Y be Banach spaces and let A : X \textrightarrow{} Y be bounded and linear. A is bounded below if and only if A is injective and the range of A is closed. Proof. Suppose that A is bounded below and that x n {$\in$} X with Ax n \textrightarrow{} y {$\in$} Y. Then x n - x m {$\leq$} cAx n - Ax m \textrightarrow{} 0 as n, m \textrightarrow{} {$\infty$}; the sequence (x n) n{$\geq$}1 is Cauchy. By completeness x n \textrightarrow{} x {$\in$} X and so Ax n \textrightarrow{} Ax which makes y = Ax. Therefore y belongs to the range of A. Conversely, suppose that A : X \textrightarrow{} Ran(A) is invertible. Since Ran(A) is a Banach space, we can use the bounded inverse theorem x = A -1 Ax {$\leq$} cAx Ran(A) = cAx Y. Remark. Injectivity here can be substituted by injectivity of\texttildelow Aof\texttildelow{} of\texttildelow A : X/Ker(A) \textrightarrow{} Y to derive a condition similar to (1.1) that is necessary and sufficient for a general (i.e., not necessarily injective) operator to have a closed range.},
  file = {/home/yanni/Zotero/storage/AG2JJ6CF/Unknown - 2013 - Range closedness and the inf-sup conditions.pdf}
}

@incollection{rasmussenGaussianProcessesMachine2004,
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2 - 14, 2003, {{T\"ubingen}}, {{Germany}}, {{August}} 4 - 16, 2003, {{Revised Lectures}}},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and {von Luxburg}, Ulrike and R{\"a}tsch, Gunnar},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {63--71},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28650-9_4},
  abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
  isbn = {978-3-540-28650-9},
  langid = {english},
  keywords = {Covariance Function,Gaussian Process,Joint Gaussian Distribution,Marginal Likelihood,Posterior Variance},
  file = {/home/yanni/Zotero/storage/35GC2JH9/Rasmussen - 2004 - Gaussian Processes in Machine Learning.pdf}
}

@article{reeceAnomalyDetectionRemoval2015,
  title = {Anomaly {{Detection}} and {{Removal Using Non-Stationary Gaussian Processes}}},
  author = {Reece, Steven and Garnett, Roman and Osborne, Michael and Roberts, Stephen},
  year = {2015},
  month = jul,
  journal = {arXiv:1507.00566 [stat]},
  eprint = {1507.00566},
  primaryclass = {stat},
  abstract = {This paper proposes a novel Gaussian process approach to fault removal in time-series data. Fault removal does not delete the faulty signal data but, instead, massages the fault from the data. We assume that only one fault occurs at any one time and model the signal by two separate non-parametric Gaussian process models for both the physical phenomenon and the fault. In order to facilitate fault removal we introduce the Markov Region Link kernel for handling non-stationary Gaussian processes. This kernel is piece-wise stationary but guarantees that functions generated by it and their derivatives (when required) are everywhere continuous. We apply this kernel to the removal of drift and bias errors in faulty sensor data and also to the recovery of EOG artifact corrupted EEG signals.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/MLRJHY98/Reece et al. - 2015 - Anomaly Detection and Removal Using Non-Stationary.pdf}
}

@article{reidProbabilisticNumericalExtension2020,
  title = {A {{Probabilistic Numerical Extension}} of the {{Conjugate Gradient Method}}},
  author = {Reid, Tim W. and Ipsen, Ilse C. F. and Cockayne, Jon and Oates, Chris J.},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.03225 [cs, math]},
  eprint = {2008.03225},
  primaryclass = {cs, math},
  abstract = {We present a Conjugate Gradient (CG) implementation of the probabilistic numerical solver BayesCG, whose error estimates are a fully integrated design feature, easy to compute, and competitive with the best existing estimators.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {65F10; 62F15; 65F50; 15A06; 15A10,Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/6KPABURR/Reid et al. - 2020 - A Probabilistic Numerical Extension of the Conjuga.pdf}
}

@article{rentmeestersFilteringTechniqueGrassmann2010,
  title = {A {{Filtering Technique}} on the {{Grassmann Manifold}}},
  author = {Rentmeesters, Quentin and Absil, Pierre-Antoine},
  year = {2010},
  pages = {4},
  abstract = {In this paper, a filtering technique that deals with subspaces, i.e., points on the Grassmann manifold, is proposed. This technique is based on an observer design where the data points are seen as the outputs of a constant velocity dynamical model. An explicit algorithm is given to efficiently compute this observer on the Grassmann manifold. This approach is compared to a particle filtering technique and similar results are obtained for a lower computational cost. Some extensions of the filter are also proposed.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/KQW4E7ZL/Rentmeesters and Absil - 2010 - A Filtering Technique on the Grassmann Manifold.pdf}
}

@misc{RetrospectiveMutipleChangePoint,
  title = {Retrospective {{Mutiple Change-Point Estimation}} with {{Kernels}}},
  abstract = {This contribution proposes an extension of the classic dynamic programming algorithm for detecting jumps in noisily observed piecewise-constant signals. The proposed algorithm operates (virtually) in a reproducing kernel Hilbert space through the use of an arbitrary kernel mapping. The resulting approach provides a computationally efficient an versatile tool for segmenting complex signals whose structure is not appropriately captured by standard parametric models. 1.},
  file = {/home/yanni/Zotero/storage/VEENSYZL/Retrospective Mutiple Change-Point Estimation with.pdf;/home/yanni/Zotero/storage/8UZMKVYS/download.html}
}

@article{reuboldLatentBehaviorSpaceA2017,
  title = {The {{Latent Behavior Space-A Vector Space}} for {{Time-Series Data}}},
  author = {Reubold, Jan and Escher, Stephan and Strufe, Thorsten},
  year = {2017},
  journal = {Icml 2017},
  abstract = {The timing and temporal order are two characteristic properties that are frequently omitted in machine learning approaches, but carry crucial information. Their consideration is currently limited to algorithms that are specialized to sequential data, but it takes a projection into a vector space to employ the wealth of ML algorithms that are known and understood. Projections inevitably cause a loss of detail. A na\textasciidieresis\i vena\textasciidieresis\i ve application of bag-of-words, as a prominent example, utilizes neither order nor timing of events. In this paper we introduce a projection strategy that retains order and timings. It identifies the latent space that the generating processes underlying the time series are spanning.},
  file = {/home/yanni/Zotero/storage/7CPYEGDE/TSW2017_paper_19.pdf}
}

@article{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  journal = {arXiv:1602.04938 [cs, stat]},
  eprint = {1602.04938},
  primaryclass = {cs, stat},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/CA6FUX7B/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{ribeiroWhyShouldTrust2016a,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  journal = {arXiv:1602.04938 [cs, stat]},
  eprint = {1602.04938},
  primaryclass = {cs, stat},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/SM7CQJ34/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{riutort-mayolPracticalHilbertSpace2023,
  title = {Practical {{Hilbert}} Space Approximate {{Bayesian Gaussian}} Processes for Probabilistic Programming},
  author = {{Riutort-Mayol}, Gabriel and B{\"u}rkner, Paul-Christian and Andersen, Michael R. and Solin, Arno and Vehtari, Aki},
  year = {2023},
  month = feb,
  journal = {Stat Comput},
  volume = {33},
  number = {1},
  pages = {17},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-022-10167-2},
  abstract = {Gaussian processes are powerful non-parametric probabilistic models for stochastic functions. However, the direct implementation entails a complexity that is computationally intractable when the number of observations is large, especially when estimated with fully Bayesian methods such as Markov chain Monte Carlo. In this paper, we focus on a low-rank approximate Bayesian Gaussian processes, based on a basis function approximation via Laplace eigenfunctions for stationary covariance functions. The main contribution of this paper is a detailed analysis of the performance, and practical recommendations for how to select the number of basis functions and the boundary factor. Intuitive visualizations and recommendations, make it easier for users to improve approximation accuracy and computational performance. We also propose diagnostics for checking that the number of basis functions and the boundary factor are adequate given the data. The approach is simple and exhibits an attractive computational complexity due to its linear structure, and it is easy to implement in probabilistic programming frameworks. Several illustrative examples of the performance and applicability of the method in the probabilistic programming language Stan are presented together with the underlying Stan model code.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/Y4D6P9PU/Riutort-Mayol et al. - 2023 - Practical Hilbert space approximate Bayesian Gauss.pdf}
}

@article{robertsGaussianProcessesTimeseries2013,
  title = {Gaussian Processes for Time-Series Modelling},
  author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
  year = {2013},
  month = feb,
  journal = {Phil. Trans. R. Soc. A.},
  volume = {371},
  number = {1984},
  pages = {20110550},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2011.0550},
  abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/A5RQD2P3/Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf}
}

@article{ruggieriExactApproachBayesian2016,
  title = {An Exact Approach to {{Bayesian}} Sequential Change Point Detection},
  author = {Ruggieri, Eric and Antonellis, Marcus},
  year = {2016},
  month = may,
  journal = {Computational Statistics \& Data Analysis},
  volume = {97},
  pages = {71--86},
  issn = {01679473},
  doi = {10.1016/j.csda.2015.11.010},
  langid = {english},
  file = {/home/yanni/Zotero/storage/D42LIAAR/Ruggieri and Antonellis - 2016 - An exact approach to Bayesian sequential change po.pdf}
}

@article{ruthottoDeepNeuralNetworks2020,
  title = {Deep {{Neural Networks Motivated}} by {{Partial Differential Equations}}},
  author = {Ruthotto, Lars and Haber, Eldad},
  year = {2020},
  month = apr,
  journal = {J Math Imaging Vis},
  volume = {62},
  number = {3},
  pages = {352--364},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-019-00903-1},
  abstract = {Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs. Posing image processing problems in the infinite-dimensional setting provides powerful tools for their analysis and solution. For the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction. In this paper, we establish a new PDE interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning. Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs. We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/WPGCA4RL/Ruthotto and Haber - 2020 - Deep Neural Networks Motivated by Partial Differen.pdf}
}

@article{saatciGaussianProcessChange,
  title = {Gaussian {{Process Change Point Models}}},
  author = {Saat{\c c}i, Yunus and Turner, Ryan and Rasmussen, Carl Edward},
  pages = {8},
  abstract = {We combine Bayesian online change point detection with Gaussian processes to create a nonparametric time series model which can handle change points. The model can be used to locate change points in an online manner; and, unlike other Bayesian online change point detection algorithms, is applicable when temporal correlations in a regime are expected. We show three variations on how to apply Gaussian processes in the change point context, each with their own advantages. We present methods to reduce the computational burden of these models and demonstrate it on several real world data sets.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/3GMSTY95/Saat√ßi et al. - Gaussian Process Change Point Models.pdf}
}

@inproceedings{salvadorDeterminingNumberClusters2004,
  title = {Determining the Number of Clusters/Segments in Hierarchical Clustering/Segmentation Algorithms},
  booktitle = {16th {{IEEE International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {Salvador, S. and Chan, P.},
  year = {2004},
  month = nov,
  pages = {576--584},
  issn = {1082-3409},
  doi = {10.1109/ICTAI.2004.50},
  abstract = {Many clustering and segmentation algorithms both suffer from the limitation that the number of clusters/segments is specified by a human user. It is often impractical to expect a human with sufficient domain knowledge to be available to select the number of clusters/segments to return. We investigate techniques to determine the number of clusters or segments to return from hierarchical clustering and segmentation algorithms. We propose an efficient algorithm, the L method that finds the "knee" in a '\# of clusters vs. clustering evaluation metric' graph. Using the knee is well-known, but is not a particularly well-understood method to determine the number of clusters. We explore the feasibility of this method, and attempt to determine in which situations it will and will not work. We also compare the L method to existing methods based on the accuracy of the number of clusters that are determined and efficiency. Our results show favorable performance for these criteria compared to the existing methods that were evaluated.},
  keywords = {Clustering algorithms,Error correction,Humans,Knee,Machine learning algorithms,Multidimensional systems,Runtime,Statistical analysis,Testing,Unsupervised learning},
  file = {/home/yanni/Zotero/storage/J7LRPY7S/Salvador and Chan - 2004 - Determining the number of clusterssegments in hie.pdf;/home/yanni/Zotero/storage/6R5YRPQT/1374239.html}
}

@article{samarovExploringRegressionStructure1993,
  title = {Exploring {{Regression Structure Using Nonparametric Functional Estimation}}},
  author = {Samarov, Alexander M.},
  year = {1993},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {423},
  pages = {836--847},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1993.10476348},
  abstract = {Average derivative functionals of regression are proposed for nonparametric model selection and diagnostics. The functionals are of the integral type, which under certain conditions allows their estimation at the usual parametric rate of n \textendash 1/2. We analyze asymptotic properties of the estimators of these functionals, based on kernel regression. These estimators can then be used for assessing the validity of various restrictions imposed on the form of regression. In particular, we show how they could be used to reduce the dimensionality of the model, assess the relative importance of predictors, measure the extent of nonlinearity and nonadditivity, and, under certain conditions, help identify projection directions in projection pursuit models and decide on the number of these directions.},
  keywords = {Kernel estimation,Model selection,Nonparametric functional estimation,Nonparametric regression},
  file = {/home/yanni/Zotero/storage/FGUWZQQH/Samarov - 1993 - Exploring Regression Structure Using Nonparametric.pdf}
}

@book{santambrogioOptimalTransportApplied2015,
  title = {Optimal {{Transport}} for {{Applied Mathematicians}}: {{Calculus}} of {{Variations}}, {{PDEs}}, and {{Modeling}}},
  shorttitle = {Optimal {{Transport}} for {{Applied Mathematicians}}},
  author = {Santambrogio, Filippo},
  year = {2015},
  series = {Progress in {{Nonlinear Differential Equations}} and {{Their Applications}}},
  volume = {87},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-20828-2},
  isbn = {978-3-319-20827-5 978-3-319-20828-2},
  langid = {english},
  file = {/home/yanni/Zotero/storage/4ME77G4W/Santambrogio - 2015 - Optimal Transport for Applied Mathematicians Calc.pdf}
}

@misc{sanz-alonsoFiniteElementRepresentations2022,
  title = {Finite {{Element Representations}} of {{Gaussian Processes}}: {{Balancing Numerical}} and {{Statistical Accuracy}}},
  shorttitle = {Finite {{Element Representations}} of {{Gaussian Processes}}},
  author = {{Sanz-Alonso}, Daniel and Yang, Ruiyi},
  year = {2022},
  month = apr,
  number = {arXiv:2109.02777},
  eprint = {arXiv:2109.02777},
  publisher = {{arXiv}},
  abstract = {The stochastic partial differential equation approach to Gaussian processes (GPs) represents Mat\'ern GP priors in terms of {$\mathsl{n}$} finite element basis functions and Gaussian coefficients with sparse precision matrix. Such representations enhance the scalability of GP regression and classification to datasets of large size {$\mathsl{N}$} by setting {$\mathsl{n}$} {$\approx$} {$\mathsl{N}$} and exploiting sparsity. In this paper we reconsider the standard choice {$\mathsl{n}$} {$\approx$} {$\mathsl{N}$} through an analysis of the estimation performance. Our theory implies that, under certain smoothness assumptions, one can reduce the computation and memory cost without hindering the estimation accuracy by setting {$\mathsl{n}$} {$\ll$} {$\mathsl{N}$} in the large {$\mathsl{N}$} asymptotics. Numerical experiments illustrate the applicability of our theory and the effect of the prior lengthscale in the pre-asymptotic regime.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation},
  file = {/home/yanni/Zotero/storage/WXSUMNZV/Sanz-Alonso and Yang - 2022 - Finite Element Representations of Gaussian Process.pdf}
}

@misc{sanz-alonsoFiniteElementRepresentations2022a,
  title = {Finite {{Element Representations}} of {{Gaussian Processes}}: {{Balancing Numerical}} and {{Statistical Accuracy}}},
  shorttitle = {Finite {{Element Representations}} of {{Gaussian Processes}}},
  author = {{Sanz-Alonso}, Daniel and Yang, Ruiyi},
  year = {2022},
  month = apr,
  number = {arXiv:2109.02777},
  eprint = {arXiv:2109.02777},
  publisher = {{arXiv}},
  abstract = {The stochastic partial differential equation approach to Gaussian processes (GPs) represents Mat\'ern GP priors in terms of {$\mathsl{n}$} finite element basis functions and Gaussian coefficients with sparse precision matrix. Such representations enhance the scalability of GP regression and classification to datasets of large size {$\mathsl{N}$} by setting {$\mathsl{n}$} {$\approx$} {$\mathsl{N}$} and exploiting sparsity. In this paper we reconsider the standard choice {$\mathsl{n}$} {$\approx$} {$\mathsl{N}$} through an analysis of the estimation performance. Our theory implies that, under certain smoothness assumptions, one can reduce the computation and memory cost without hindering the estimation accuracy by setting {$\mathsl{n}$} {$\ll$} {$\mathsl{N}$} in the large {$\mathsl{N}$} asymptotics. Numerical experiments illustrate the applicability of our theory and the effect of the prior lengthscale in the pre-asymptotic regime.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation},
  file = {/home/yanni/Zotero/storage/35IV4VIL/Sanz-Alonso and Yang - 2022 - Finite Element Representations of Gaussian Process.pdf}
}

@article{sarkarCovNetCovarianceNetworks2021,
  title = {{{CovNet}}: {{Covariance Networks}} for {{Functional Data}} on {{Multidimensional Domains}}},
  author = {Sarkar, Soham and Panaretos, Victor M.},
  year = {2021},
  pages = {1--67},
  abstract = {Covariance estimation is ubiquitous in functional data analysis. Yet, the case of functional observations over multidimensional domains introduces computational and statistical challenges, rendering the standard methods effectively inapplicable. To address this problem, we introduce Covariance Networks (CovNet) as a modeling and estimation tool. The CovNet model is universal \textendash{} it can be used to approximate any covariance up to desired precision. Moreover, the model can be fitted efficiently to the data and its neural network architecture allows us to employ modern computational tools in the implementation. The CovNet model also admits a closed-form eigen-decomposition, which can be computed efficiently, without constructing the covariance itself. This facilitates easy storage and subsequent manipulation in the context of the CovNet. Moreover, we establish consistency of the proposed estimator and derive its rate of convergence. The usefulness of the proposed method is demonstrated by means of an extensive simulation study.},
  keywords = {and phrases,deep learning,fda,neural network,nonparametric model,universal ap-},
  file = {/home/yanni/Zotero/storage/L7DF44HI/2104.05021.pdf}
}

@inproceedings{satopaaFindingKneedleHaystack2011,
  title = {Finding a "{{Kneedle}}" in a {{Haystack}}: {{Detecting Knee Points}} in {{System Behavior}}},
  shorttitle = {Finding a "{{Kneedle}}" in a {{Haystack}}},
  booktitle = {2011 31st {{International Conference}} on {{Distributed Computing Systems Workshops}}},
  author = {Satopaa, Ville and Albrecht, Jeannie and Irwin, David and Raghavan, Barath},
  year = {2011},
  month = jun,
  pages = {166--171},
  publisher = {{IEEE}},
  address = {{Minneapolis, MN, USA}},
  doi = {10.1109/ICDCSW.2011.20},
  abstract = {Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These ``knees'' typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to online and offline knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications.},
  isbn = {978-1-4577-0384-3},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VNWIMHXE/Satopaa et al. - 2011 - Finding a Kneedle in a Haystack Detecting Knee .pdf}
}

@article{scheuererRegularitySamplePaths2010,
  title = {Regularity of the Sample Paths of a General Second Order Random Field},
  author = {Scheuerer, Michael},
  year = {2010},
  month = sep,
  journal = {Stochastic Processes and their Applications},
  volume = {120},
  number = {10},
  pages = {1879--1897},
  issn = {03044149},
  doi = {10.1016/j.spa.2010.05.009},
  abstract = {We study the sample path regularity of a second-order random field (Xt )t{$\in$}T where T is an open subset of Rd . It is shown that the conditions on its covariance function, known to be equivalent to mean square differentiability of order k, imply that the sample paths are a.s. in the local Sobolev space Wlko,c2(T ). We discuss their necessity, and give additional conditions for the sample paths to be in a local Sobolev space Wl\textmu oc,2(T ) of fractional order \textmu. This finally allows, via Sobolev embeddings, to draw conclusions about a.s. continuous differentiability of the sample paths.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/Y8NKG5QC/Scheuerer - 2010 - Regularity of the sample paths of a general second.pdf}
}

@article{schoberProbabilisticModelNumerical2017,
  title = {A Probabilistic Model for the Numerical Solution of Initial Value Problems},
  author = {Schober, Michael and S{\"a}rkk{\"a}, Simo and Hennig, Philipp},
  year = {2017},
  month = aug,
  journal = {arXiv:1610.05261 [cs, math, stat]},
  eprint = {1610.05261},
  primaryclass = {cs, math, stat},
  abstract = {We study connections between ordinary differential equation (ODE) solvers and probabilistic regression methods in statistics. We provide a new view of probabilistic ODE solvers as active inference agents operating on stochastic differential equation models that estimate the unknown initial value problem (IVP) solution from approximate observations of the solution derivative, as provided by the ODE dynamics. Adding to this picture, we show that several multistep methods of Nordsieck form can be recasted as Kalman filtering on q-times integrated Wiener processes. Doing so provides a family of IVP solvers that return a Gaussian posterior measure, rather than a point estimate. We show that some such methods have low computational overhead, nontrivial convergence order, and that the posterior has a calibrated concentration rate. Additionally, we suggest a step size adaptation algorithm which completes the proposed method to a practically useful implementation, which we experimentally evaluate using a representative set of standard codes in the DETEST benchmark set.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/U2K3ID5K/Schober et al. - 2017 - A probabilistic model for the numerical solution o.pdf}
}

@techreport{schoberProbabilisticODESolvers,
  title = {Probabilistic {{ODE Solvers}} with {{Runge-Kutta Means}}},
  author = {Schober, Michael and Duvenaud, David and Hennig, Philipp},
  abstract = {Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods , they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.},
  file = {/home/yanni/Zotero/storage/WURJ6ASG/Schober, Duvenaud, Hennig - Unknown - Probabilistic ODE Solvers with Runge-Kutta Means.pdf}
}

@article{schullerLecturesGeometricAnatomy,
  title = {Lectures on the {{Geometric Anatomy}} of {{Theoretical Physics}}},
  author = {Schuller, Dr Frederic P},
  pages = {218},
  langid = {english},
  file = {/home/yanni/Zotero/storage/BITL4EY4/Schuller - Lectures on the Geometric Anatomy of Theoretical P.pdf}
}

@article{schwabSparseTensorDiscretizations2011,
  title = {Sparse Tensor Discretizations of High-Dimensional Parametric and Stochastic {{PDEs}}},
  author = {Schwab, Christoph and Gittelson, Claude Jeffrey},
  year = {2011},
  journal = {Acta Numerica},
  volume = {20},
  number = {2011},
  pages = {291--467},
  issn = {09624929},
  doi = {10.1017/S0962492911000055},
  abstract = {Partial differential equations (PDEs) with random input data, such as random loadings and coefficients, are reformulated as parametric, deterministic PDEs on parameter spaces of high, possibly infinite dimension. Tensorized operator equations for spatial and temporal k-point correlation functions of their random solutions are derived. Parametric, deterministic PDEs for the laws of the random solutions are derived. Representations of the random solutions' laws on infinite-dimensional parameter spaces in terms of 'generalized polynomial chaos' (GPC) series are established. Recent results on the regularity of solutions of these parametric PDEs are presented. Convergence rates of best N-term approximations, for adaptive stochastic Galerkin and collocation discretizations of the parametric, deterministic PDEs, are established. Sparse tensor products of hierarchical (multi-level) discretizations in physical space (and time), and GPC expansions in parameter space, are shown to converge at rates which are independent of the dimension of the parameter space. A convergence analysis of multi-level Monte Carlo (MLMC) discretizations of PDEs with random coefficients is presented. Sufficient conditions on the random inputs for superiority of sparse tensor discretizations over MLMC discretizations are established for linear elliptic, parabolic and hyperbolic PDEs with random coefficients. \textcopyright{} 2011 Cambridge University Press.},
  file = {/home/yanni/Zotero/storage/RATCBIDE/Schwab, Gittelson - 2011 - Sparse tensor discretizations of high-dimensional parametric and stochastic PDEs.pdf}
}

@article{scillitoePolynomialRidgeFlowfield2021,
  title = {Polynomial Ridge Flowfield Estimation},
  author = {Scillitoe, A. and Seshadri, P. and Wong, C. Y. and Duncan, A.},
  year = {2021},
  month = dec,
  journal = {Physics of Fluids},
  volume = {33},
  number = {12},
  pages = {127110},
  issn = {1070-6631, 1089-7666},
  doi = {10.1063/5.0064000},
  abstract = {Computational fluid dynamics plays a key role in the design process across many industries. Recently, there has been increasing interest in data-driven methods in order to exploit the large volume of data generated by such computations. This paper introduces the idea of using spatially correlated polynomial ridge functions for rapid flowfield estimation. Dimension reducing ridge functions are obtained for numerous points within training flowfields. The functions can then be used to predict flow variables for new, previously unseen, flowfields. Their dimension reducing nature alleviates the problems associated with visualizing high-dimensional datasets, enabling improved understanding of design spaces and potentially providing valuable physical insights. The proposed framework is computationally efficient; consisting of either readily parallelizable tasks or linear algebra operations. To further reduce the computational cost, ridge functions need only be computed at a small number of subsampled locations. The flow physics encoded within covariance matrices obtained from the training flowfields can then be used to predict flow quantities, conditional upon those predicted by the ridge functions at the sampled points. To demonstrate the efficacy of the framework, the incompressible flow around an ensemble of airfoils is used as a test case. The ridge functions' predictive accuracy is found to be competitive with a state-of-the-art convolutional neural network. The local ridge functions can also be reused to obtain surrogate models for integral quantities, avoiding the need for long-term storage of the training data. Finally, use of the ridge framework with varying boundary conditions is demonstrated on a transonic wing.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/AGHU83TN/Scillitoe et al. - 2021 - Polynomial ridge flowfield estimation.pdf}
}

@article{scitovskiMethodSolvingParameter1996,
  title = {A Method for Solving the Parameter Identification Problem for Ordinary Differential Equations of the Second Order},
  author = {Scitovski, Rudolf and Juki{\'c}, Dragan},
  year = {1996},
  journal = {Applied Mathematics and Computation},
  issn = {00963003},
  doi = {10.1016/0096-3003(95)00098-4},
  abstract = {We give a method for solving the parameter identification problem for ordinary differential equations of the second order using a noninterpolated moving least squares method. The method is tested in two practical examples. \textcopyright{} Elsevier Science Inc., 1996.},
  file = {/home/yanni/Downloads/1-s2.0-0096300395000984-main.pdf}
}

@article{scottClusterAnalysisMethod1974,
  title = {A {{Cluster Analysis Method}} for {{Grouping Means}} in the {{Analysis}} of {{Variance}}},
  author = {Scott, A. J. and Knott, M.},
  year = {1974},
  month = sep,
  journal = {Biometrics},
  volume = {30},
  number = {3},
  eprint = {2529204},
  eprinttype = {jstor},
  pages = {507},
  issn = {0006341X},
  doi = {10.2307/2529204},
  abstract = {It is sometimes useful in an analysis of variance to split the treatments into reasonably homogeneous groups. Multiple comparison procedures are often used for this purpose, but a more direct method is to use the techniques of cluster analysis. This approach is illustrated for several sets of data, and a likelihood ratio test is developed for judging the significance of differences among the resulting groups.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VVSE8ZJH/Scott and Knott - 1974 - A Cluster Analysis Method for Grouping Means in th.pdf}
}

@techreport{sejdinovicWhatRKHS2014,
  title = {What Is an {{RKHS}}?},
  author = {Sejdinovic, Dino and Gretton, Arthur},
  year = {2014},
  abstract = {1 Outline \textbullet{} Normed and inner product spaces. Cauchy sequences and completeness. Banach and Hilbert spaces. \textbullet{} Linearity, continuity and boundedness of operators. Riesz representation of functionals. \textbullet{} Definition of an RKHS and reproducing kernels. \textbullet{} Relationship with positive definite functions. Moore-Aronszajn theorem. 2 Some functional analysis We start by reviewing some elementary Banach and Hilbert space theory. Two key results here will prove useful in studying the properties of reproducing kernel Hilbert spaces: (a) that a linear operator on a Banach space is continuous if and only if it is bounded, and (b) that all continuous linear functionals on a Hilbert space arise from the inner product. The latter is often termed Riesz representation theorem. 2.1 Definitions of Banach and Hilbert spaces We will focus on real Banach and Hilbert spaces, which are, first of all, vector spaces 1 over the field R of real numbers. We remark that the theory remains valid in the context of complex Banach and Hilbert spaces, defined over the field C of complex numbers, modulo appropriately placed complex conjugates. In particular, the complex inner product satisfies conjugate symmetry instead of symmetry. Definition 1 (Norm). Let F be a vector space over R. A function {$\cdot\cdot$} F : F \textrightarrow{} [0, {$\infty$}) is said to be a norm on F if 1. f F = 0 if and only if f = 0 (norm separates points), 1 A vector space can also be known as a linear space Kreyszig (1989, Definition 2.1-1).},
  file = {/home/yanni/Zotero/storage/SFEIS4W5/Sejdinovic, Gretton - 2014 - What is an RKHS.pdf}
}

@article{seshadriDimensionReductionGaussian2019,
  title = {Dimension {{Reduction}} via {{Gaussian Ridge Functions}}},
  author = {Seshadri, Pranay and Yuchi, Shaowu and Parks, Geoffrey T.},
  year = {2019},
  month = jan,
  journal = {SIAM/ASA J. Uncertainty Quantification},
  volume = {7},
  number = {4},
  pages = {1301--1322},
  issn = {2166-2525},
  doi = {10.1137/18M1168571},
  abstract = {Ridge functions have recently emerged as a powerful set of ideas for subspace-based dimension reduction. In this paper we begin by drawing parallels between ridge subspaces, sufficient dimension reduction and active subspaces, contrasting between techniques rooted in statistical regression and those rooted in approximation theory. This sets the stage for our new algorithm that approximates what we call a Gaussian ridge function---the posterior mean of a Gaussian process on a dimensionreducing subspace---suitable for both regression and approximation problems. To compute this subspace we develop an iterative algorithm that alternates between optimizing over the Stiefel manifold to compute the subspace and optimizing the hyperparameters of the Gaussian process. We demonstrate the utility of the algorithm on two analytical functions, where we obtain near exact ridge recovery, and a turbomachinery case study, where we compare the efficacy of our approach with three well-known sufficient dimension reduction methods: SIR, SAVE, and CR. The comparisons motivate the use of the posterior variance as a heuristic for identifying the suitability of a dimension-reducing subspace.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/UDKTX8ZV/Seshadri et al. - 2019 - Dimension Reduction via Gaussian Ridge Functions.pdf}
}

@techreport{shentalGaussianBeliefPropagation,
  title = {Gaussian {{Belief Propagation Solver}} for {{Systems}} of {{Linear Equations}}},
  author = {Shental, Ori and Siegel, Paul H and Wolf, Jack K and Bickson, Danny and Dolev, Danny},
  abstract = {The canonical problem of solving a system of linear equations arises in numerous contexts in information theory, communication theory, and related fields. In this contribution, we develop a solution based upon Gaussian belief propagation (GaBP) that does not involve direct matrix inversion. The iterative nature of our approach allows for a distributed message-passing implementation of the solution algorithm. We also address some properties of the GaBP solver, including convergence, exactness, its max-product version and relation to classical solution methods. The application example of decorrelation in CDMA is used to demonstrate the faster convergence rate of the proposed solver in comparison to conventional linear-algebraic iterative solution methods.},
  file = {/home/yanni/Zotero/storage/6G4ZEDX2/Shental et al. - Unknown - Gaussian Belief Propagation Solver for Systems of Linear Equations.pdf}
}

@article{shiIntrinsicRegressionModels2012,
  title = {Intrinsic {{Regression Models}} for {{Medial Representation}} of {{Subcortical Structures}}},
  author = {Shi, Xiaoyan and Zhu, Hongtu and Ibrahim, Joseph G. and Liang, Faming and Lieberman, Jeffrey and Styner, Martin},
  year = {2012},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {107},
  number = {497},
  pages = {12--23},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2011.643710},
  abstract = {The aim of this article is to develop a semiparametric model to describe the variability of the medial representation of subcortical structures, which belongs to a Riemannian manifold, and establish its association with covariates of interest, such as diagnostic status, age, and gender. We develop a two-stage estimation procedure to calculate the parameter estimates. The first stage is to calculate an intrinsic least squares estimator of the parameter vector using the annealing evolutionary stochastic approximation Monte Carlo algorithm, and then the second stage is to construct a set of estimating equations to obtain a more efficient estimate with the intrinsic least squares estimate as the starting point. We use Wald statistics to test linear hypotheses of unknown parameters and establish their limiting distributions. Simulation studies are used to evaluate the accuracy of our parameter estimates and the finite sample performance of the Wald statistics. We apply our methods to the detection of the difference in the morphological changes of the left and right hippocampi between schizophrenia patients and healthy controls using a medial shape description. This article has online supplementary material.},
  pmid = {23794769},
  keywords = {Intrinsic least squares estimator,Medial representation,Semiparametric model,Wald statistic},
  file = {/home/yanni/Zotero/storage/Q4X3V9SQ/Shi et al. - 2012 - Intrinsic Regression Models for Medial Representat.pdf}
}

@misc{shiSpectralApproachGradient2018,
  title = {A {{Spectral Approach}} to {{Gradient Estimation}} for {{Implicit Distributions}}},
  author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
  year = {2018},
  month = jun,
  number = {arXiv:1806.02925},
  eprint = {arXiv:1806.02925},
  publisher = {{arXiv}},
  abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr\textbackslash "om method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr\textbackslash "om method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/64HMHF6Y/Shi et al_2018_A Spectral Approach to Gradient Estimation for Implicit Distributions.pdf}
}

@techreport{sigristStochasticPartialDifferential2015,
  title = {Stochastic Partial Differential Equation Based Modelling of Large Space-Time Data Sets},
  author = {Sigrist, Fabio and K{\"u}nsch, Hans R and Stahel, Werner A},
  year = {2015},
  journal = {Source: Journal of the Royal Statistical Society. Series B (Statistical Methodology},
  volume = {77},
  number = {1},
  eprint = {24774723},
  eprinttype = {jstor},
  pages = {3--33}
}

@article{sinnDetectingChangePointsTime,
  title = {Detecting {{Change-Points}} in {{Time Series}} by {{Maximum Mean Discrepancy}} of {{Ordinal Pattern Distributions}}},
  author = {Sinn, Mathieu and Ghodsi, Ali and Keller, Karsten},
  pages = {9},
  abstract = {As a new method for detecting change-points in high-resolution time series, we apply Maximum Mean Discrepancy to the distributions of ordinal patterns in different parts of a time series. The main advantage of this approach is its computational simplicity and robustness with respect to (non-linear) monotonic transformations, which makes it particularly well-suited for the analysis of long biophysical time series where the exact calibration of measurement devices is unknown or varies with time. We establish consistency of the method and evaluate its performance in simulation studies. Furthermore, we demonstrate the application to the analysis of electroencephalography (EEG) and electrocardiography (ECG) recordings.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/MZDM8YSL/Sinn et al. - Detecting Change-Points in Time Series by Maximum .pdf}
}

@article{solinExplicitLinkPeriodic,
  title = {Explicit {{Link Between Periodic Covariance Functions}} and {{State Space Models}}},
  author = {Solin, Arno and Sarkka, Simo},
  abstract = {This paper shows how periodic covariance functions in Gaussian process regression can be reformulated as state space models, which can be solved with classical Kalman filtering theory. This reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The representation is based on expanding periodic covariance functions into a series of stochastic resonators. The explicit representation of the canonical periodic covariance function is written out and the expansion is shown to uniformly converge to the exact covariance function with a known convergence rate. The framework is generalized to quasi-periodic covariance functions by introducing damping terms in the system and applied to two sets of real data. The approach could be easily extended to nonstationary and spatio-temporal variants.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/SD5ZI5BT/Solin and Sarkka - Explicit Link Between Periodic Covariance Function.pdf}
}

@article{solinHilbertSpaceMethods2020,
  title = {Hilbert Space Methods for Reduced-Rank {{Gaussian}} Process Regression},
  author = {Solin, Arno and S{\"a}rkk{\"a}, Simo},
  year = {2020},
  journal = {Statistics and Computing},
  volume = {30},
  number = {2},
  pages = {419--446},
  issn = {15731375},
  doi = {10.1007/s11222-019-09886-w},
  abstract = {This paper proposes a novel scheme for reduced-rank Gaussian process regression. The method is based on an approximate series expansion of the covariance function in terms of an eigenfunction expansion of the Laplace operator in a compact subset of Rd. On this approximate eigenbasis, the eigenvalues of the covariance function can be expressed as simple functions of the spectral density of the Gaussian process, which allows the GP inference to be solved under a computational cost scaling as O(nm2) (initial) and O(m3) (hyperparameter learning) with m basis functions and n data points. Furthermore, the basis functions are independent of the parameters of the covariance function, which allows for very fast hyperparameter learning. The approach also allows for rigorous error analysis with Hilbert space theory, and we show that the approximation becomes exact when the size of the compact subset and the number of eigenfunctions go to infinity. We also show that the convergence rate of the truncation error is independent of the input dimensionality provided that the differentiability order of the covariance function increases appropriately, and for the squared exponential covariance function it is always bounded by {$\sim$} 1 / m regardless of the input dimensionality. The expansion generalizes to Hilbert spaces with an inner product which is defined as an integral over a specified input density. The method is compared to previously proposed methods theoretically and through empirical tests with simulated and real data.},
  keywords = {Eigenfunction expansion,Gaussian process regression,Laplace operator,Pseudo-differential operator,Reduced-rank approximation},
  file = {/home/yanni/Zotero/storage/UJ79DA7I/1401.5508.pdf}
}

@article{sprungkLocalLipschitzStability2020,
  title = {On the Local {{Lipschitz}} Stability of {{Bayesian}} Inverse Problems},
  author = {Sprungk, Bj{\"o}rn},
  year = {2020},
  journal = {Inverse Problems},
  volume = {36},
  number = {5},
  pages = {1--28},
  issn = {13616420},
  doi = {10.1088/1361-6420/ab6f43},
  abstract = {In this note we consider the stability of posterior measures occurring in Bayesian inference w.r.t. perturbations of the prior measure and the log-likelihood function. This extends the well-posedness analysis of Bayesian inverse problems. In particular, we prove a general local Lipschitz continuous dependence of the posterior on the prior and the log-likelihood w.r.t. various common distances of probability measures. These include the total variation, Hellinger, and Wasserstein distance and the Kullback-Leibler divergence. We only assume the boundedness of the likelihoods and measure their perturbations in an L p-norm w.r.t. the prior. The obtained stability yields under mild assumptions the well-posedness of Bayesian inverse problems, in particular, a well-posedness w.r.t. the Wasserstein distance. Moreover, our results indicate an increasing sensitivity of Bayesian inference as the posterior becomes more concentrated, for example due to more or more accurate data. This confirms and extends previous observations made in the sensitivity analysis of Bayesian inference.},
  keywords = {Bayesian inference,Hellinger distance,inverse problems,Kullback-Leibler divergence,robust statistics,Wasserstein distance,wellposedness},
  file = {/home/yanni/Zotero/storage/SNNCVZ63/1906.07120.pdf}
}

@techreport{sriperumbudurbharathsvHilbertSpaceEmbeddings2010,
  title = {Hilbert {{Space Embeddings}} and {{Metrics}} on {{Probability Measures}}},
  author = {Sriperumbudur BHARATHSV, Bharath K and Gretton, Arthur and Sch{\"o}lkopf BERNHARDSCHOELKOPF, Bernhard and G Lanckriet, Gert R},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  pages = {1517--1561},
  abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be defined as the distance between distribution embeddings: we denote this as {$\gamma$} k , indexed by the kernel function k that defines the inner product in the RKHS. We present three theoretical properties of {$\gamma$} k. First, we consider the question of determining the conditions on the kernel k for which {$\gamma$} k is a metric: such k are denoted characteristic kernels. Unlike pseudometrics, a metric is zero only when two distributions coincide, thus ensuring the RKHS embedding maps all distributions uniquely (i.e., the embedding is injective). While previously published conditions may apply only in restricted circumstances (e.g., on compact domains), and are difficult to check, our conditions are straightforward and intuitive: integrally strictly positive definite kernels are characteristic. Alternatively, if a bounded continuous kernel is translation-invariant on R d , then it is characteristic if and only if the support of its Fourier transform is the entire R d. Second, we show that the distance between distributions under {$\gamma$} k results from an interplay between the properties of the kernel and the distributions, by demonstrating that distributions are close in the embedding space when their differences occur at higher frequencies. Third, to understand the *. nature of the topology induced by {$\gamma$} k , we relate {$\gamma$} k to other popular metrics on probability measures, and present conditions on the kernel k under which {$\gamma$} k metrizes the weak topology.},
  keywords = {characteristic kernels,FUKUMIZU,GRETTON,Hilbertian metric,homogeneity tests,independence tests,kernel methods,SCH¬®OLKOPFSCH¬® SCH¬®OLKOPF AND LANCKRIET Keywords:,SRIPERUMBUDUR,universal kernels,weak topology},
  file = {/home/yanni/Zotero/storage/PXX8GGCA/Sriperumbudur BHARATHSV et al. - 2010 - Hilbert Space Embeddings and Metrics on Probability Measures Kenji Fukumizu.pdf}
}

@article{stefanouStochasticFiniteElement2009,
  title = {The Stochastic Finite Element Method: {{Past}}, Present and Future},
  author = {Stefanou, George},
  year = {2009},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {198},
  number = {9-12},
  pages = {1031--1051},
  publisher = {{Elsevier B.V.}},
  issn = {00457825},
  doi = {10.1016/j.cma.2008.11.007},
  abstract = {A powerful tool in computational stochastic mechanics is the stochastic finite element method (SFEM). SFEM is an extension of the classical deterministic FE approach to the stochastic framework i.e. to the solution of static and dynamic problems with stochastic mechanical, geometric and/or loading properties. The considerable attention that SFEM received over the last decade can be mainly attributed to the spectacular growth of computing power rendering possible the efficient treatment of large-scale problems. This article aims at providing a state-of-the-art review of past and recent developments in the SFEM area and indicating future directions as well as some open issues to be examined by the computational mechanics community in the future. \textcopyright{} 2008 Elsevier B.V. All rights reserved.},
  keywords = {Monte Carlo simulation,Parallel processing,Solution techniques,Stochastic finite elements,Stochastic partial differential equations,Stochastic processes and fields},
  file = {/home/yanni/Zotero/storage/WT73RS8J/1-s2.0-S0045782508004118-main.pdf}
}

@article{steinkeKernelsRegularizationDifferential2008,
  title = {Kernels, Regularization and Differential Equations},
  author = {Steinke, Florian and Sch{\"o}lkopf, Bernhard},
  year = {2008},
  month = nov,
  journal = {Pattern Recognition},
  volume = {41},
  number = {11},
  pages = {3271--3286},
  issn = {00313203},
  doi = {10.1016/j.patcog.2008.06.011},
  abstract = {Many common machine learning methods such as support vector machines or Gaussian process inference make use of positive definite kernels, reproducing kernel Hilbert spaces, Gaussian processes, and regularization operators. In this work these objects are presented in a general, unifying framework and interrelations are highlighted.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/ZZSXXYIB/Steinke and Sch√∂lkopf - 2008 - Kernels, regularization and differential equations.pdf}
}

@article{steinwartInfluenceKernelConsistency,
  title = {On the {{Influence}} of the {{Kernel}} on the {{Consistency}} of {{Support Vector Machines}}},
  author = {Steinwart, Ingo},
  pages = {27},
  abstract = {In this article we study the generalization abilities of several classifiers of support vector machine (SVM) type using a certain class of kernels that we call universal. It is shown that the soft margin algorithms with universal kernels are consistent for a large class of classification problems including some kind of noisy tasks provided that the regularization parameter is chosen well. In particular we derive a simple sufficient condition for this parameter in the case of Gaussian RBF kernels. On the one hand our considerations are based on an investigation of an approximation property\textemdash the so-called universality\textemdash of the used kernels that ensures that all continuous functions can be approximated by certain kernel expressions. This approximation property also gives a new insight into the role of kernels in these and other algorithms. On the other hand the results are achieved by a precise study of the underlying optimization problems of the classifiers. Furthermore, we show consistency for the maximal margin classifier as well as for the soft margin SVM's in the presence of large margins. In this case it turns out that also constant regularization parameters ensure consistency for the soft margin SVM's. Finally we prove that even for simple, noise free classification problems SVM's with polynomial kernels can behave arbitrarily badly.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VY2M9KVA/Steinwart - On the InÔ¨Çuence of the Kernel on the Consistency o.pdf}
}

@article{stillLecturesParametricOptimization,
  title = {Lectures on {{Parametric Optimization}}: {{An Introduction}}},
  author = {Still, Georg},
  pages = {131},
  langid = {english},
  file = {/home/yanni/Zotero/storage/XDNDC2II/Still - Lectures on Parametric Optimization An Introducti.pdf}
}

@article{stillLecturesParametricOptimizationa,
  title = {Lectures on {{Parametric Optimization}}: {{An Introduction}}},
  author = {Still, Georg},
  pages = {131},
  langid = {english},
  file = {/home/yanni/Zotero/storage/6HVQJ8FK/Still - Lectures on Parametric Optimization An Introducti.pdf}
}

@article{stortelderParameterEstimationDynamic1996,
  title = {Parameter Estimation in Dynamic Systems},
  author = {Stortelder, Walter J.H.},
  year = {1996},
  journal = {Mathematics and Computers in Simulation},
  issn = {03784754},
  abstract = {In this paper we present an approach to the solution of parameter estimation problems in systems described mathematically by differential algebraic equations. The numerical solution of these equations is compared with measurements from experi- ments. By adaptation of the parameters in the differential algebraic equations we try to fit the solution to the measurements. For the fit we use a (weighted) least squares criterion. Not only the final estimate of the unknown parameters, but also additional information about the reliability is derived. A computer program, with an interactive graphical user interface, has been developed to steer through the computation in order to influence the precise formulation of the model and the data and to see the numerical results by direct visualisation.},
  file = {/home/yanni/Zotero/storage/IDQSXZK8/Stortelder - 1996 - Parameter estimation in dynamic systems.pdf}
}

@article{strangAnalysisFiniteElementMethod1974,
  title = {An {{Analysis}} of the {{Finite-Element Method}}},
  author = {Strang, Gilbert and Fix, George J. and Griffin, D. S.},
  year = {1974},
  month = mar,
  journal = {Journal of Applied Mechanics},
  volume = {41},
  number = {1},
  pages = {62},
  issn = {0021-8936},
  doi = {10.1115/1.3423272},
  file = {/home/yanni/Zotero/storage/K8N5JTTN/Strang et al. - 1974 - An Analysis of the Finite-Element Method.pdf;/home/yanni/Zotero/storage/7ALTKMXA/An-Analysis-of-the-Finite-Element-Method.html}
}

@article{stuartInverseProblemsBayesian2010,
  title = {Inverse Problems: {{A Bayesian}} Perspective},
  author = {Stuart, A M},
  year = {2010},
  journal = {Acta Numerica},
  volume = {19},
  pages = {451--559},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/S0962492910000061},
  abstract = {The subject of inverse problems in differential equations is of enormous practical importance, and has also generated substantial mathematical and computational innovation. Typically some form of regularization is required to ameliorate ill-posed behaviour. In this article we review the Bayesian approach to regularization, developing a function space viewpoint on the subject. This approach allows for a full characterization of all possible solutions, and their relative probabilities, whilst simultaneously forcing significant modelling issues to be addressed in a clear and precise fashion. Although expensive to implement, this approach is starting to lie within the range of the available computational resources in many application areas. It also allows for the quantification of uncertainty and risk, something which is increasingly demanded by these applications. Furthermore, the approach is conceptually important for the understanding of simpler, computationally expedient approaches to inverse problems. We demonstrate that, when formulated in a Bayesian fashion, a wide range of inverse problems share a common mathematical framework, and we highlight a theory of well-posedness which stems from this. The well-posedness theory provides the basis for a number of stability and approximation results which we describe. We also review a range of algorithmic approaches which are used when adopting the Bayesian approach to inverse problems. These include MCMC methods, filtering and the variational approach.},
  file = {/home/yanni/Zotero/storage/DN85SFXA/Stuart - 2010 - Inverse problems A Bayesian perspective.pdf}
}

@article{sudretStochasticFiniteElement,
  title = {Stochastic {{Finite Element Methods}} and {{Reliability A State-of-the-Art Report}}},
  author = {Sudret, Bruno},
  pages = {190},
  langid = {english},
  file = {/home/yanni/Zotero/storage/CHS32BNX/Sudret - Stochastic Finite Element Methods and Reliability .pdf}
}

@misc{sunFunctionalVariationalBayesian2019,
  title = {Functional {{Variational Bayesian Neural Networks}}},
  author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  year = {2019},
  month = mar,
  number = {arXiv:1903.05779},
  eprint = {arXiv:1903.05779},
  publisher = {{arXiv}},
  abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/GE2EUSVK/Sun et al_2019_Functional Variational Bayesian Neural Networks.pdf}
}

@article{sunnakerApproximateBayesianComputation2013,
  title = {Approximate {{Bayesian Computation}}},
  author = {Sunn{\aa}ker, Mikael and Busetto, Alberto Giovanni and Numminen, Elina and Corander, Jukka and Foll, Matthieu and Dessimoz, Christophe},
  editor = {Wodak, Shoshana},
  year = {2013},
  month = jan,
  journal = {PLoS Comput Biol},
  volume = {9},
  number = {1},
  pages = {e1002803},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002803},
  abstract = {Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences (e.g., in population genetics, ecology, epidemiology, and systems biology).},
  langid = {english},
  file = {/home/yanni/Zotero/storage/L85IH6IS/Sunn√•ker et al. - 2013 - Approximate Bayesian Computation.pdf}
}

@article{suttonReinforcementLearningIntroduction,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  langid = {english},
  file = {/home/yanni/Zotero/storage/MLALQP65/Sutton and Barto - Reinforcement Learning An Introduction.pdf}
}

@article{tankInterpretableSparseNeural2017,
  title = {An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery},
  author = {Tank, Alex and Covert, Ian C. and Foti, Nicholas J. and Shojaie, Ali and Fox, Emily B.},
  year = {2017},
  journal = {arXiv},
  number = {Nips},
  issn = {23318422},
  abstract = {While most classical approaches to Granger causality detection repose upon linear time series assumptions, many interactions in neuroscience and economics applications are nonlinear. We develop an approach to nonlinear Granger causality detection using multilayer perceptrons where the input to the network is the past time lags of all series and the output is the future value of a single series. A sufficient condition for Granger non-causality in this setting is that all of the outgoing weights of the input data, the past lags of a series, to the first hidden layer are zero. For estimation, we utilize a group lasso penalty to shrink groups of input weights to zero. We also propose a hierarchical penalty for simultaneous Granger causality and lag estimation. We validate our approach on simulated data from both a sparse linear autoregressive model and the sparse and nonlinear Lorenz-96 model.},
  file = {/home/yanni/Zotero/storage/BMQFN7KN/1711.08160.pdf}
}

@techreport{teckentrupFurtherAnalysisMultilevel,
  title = {Further Analysis of Multilevel {{Monte Carlo}} Methods for Elliptic {{PDEs}} with Random Coefficients},
  author = {Teckentrup, A L and Scheichl, R and Giles, M B and Ullmann, E},
  abstract = {We consider the application of multilevel Monte Carlo methods to elliptic PDEs with random coefficients. We focus on models of the random coefficient that lack uniform ellipticity and boundedness with respect to the random parameter, and that only have limited spatial regularity. We extend the finite element error analysis for this type of equation, carried out in [6], to more difficult problems, posed on non-smooth domains and with discontinuities in the coefficient. For this wider class of model problem, we prove convergence of the multilevel Monte Carlo algorithm for estimating any bounded, linear functional and any continuously Fr\'echet differentiable non-linear functional of the solution. We further improve the performance of the multilevel estimator by introducing level dependent truncations of the KarhunenL\`o eve expansion of the random coefficient. Numerical results complete the paper.},
  keywords = {discontinuous coefficients and corners,log-normal random fields,multilevel Monte Carlo,non-uniformly elliptic,output functionals,PDEs with stochastic coefficients,truncated KarhunenL√≤ eve expansion},
  file = {/home/yanni/Zotero/storage/E7TS3QCA/Teckentrup et al. - Unknown - Further analysis of multilevel Monte Carlo methods for elliptic PDEs with random coefficients.pdf}
}

@article{TestChangeParameter2021,
  title = {A {{Test}} for a {{Change}} in a {{Parameter Occurring}} at an {{Unknown Point}}},
  year = {2021},
  pages = {6},
  langid = {english},
  file = {/home/yanni/Zotero/storage/F2N5DN2Z/2021 - A Test for a Change in a Parameter Occurring at an.pdf}
}

@techreport{teymurImplicitProbabilisticIntegrators,
  title = {Implicit {{Probabilistic Integrators}} for {{ODEs}}},
  author = {Teymur, Onur and Calderhead, Ben and Lie, Han Cheng and Sullivan, T J},
  abstract = {We introduce a family of implicit probabilistic integrators for initial value problems (IVPs), taking as a starting point the multistep Adams-Moulton method. The implicit construction allows for dynamic feedback from the forthcoming time-step, in contrast to previous probabilistic integrators, all of which are based on explicit methods. We begin with a concise survey of the rapidly-expanding field of probabilistic ODE solvers. We then introduce our method, which builds on and adapts the work of Conrad et al. (2016) and Teymur et al. (2016), and provide a rigorous proof of its well-definedness and convergence. We discuss the problem of the calibration of such integrators and suggest one approach. We give an illustrative example highlighting the effect of the use of probabilistic integrators-including our new method-in the setting of parameter inference within an inverse problem.},
  file = {/home/yanni/Zotero/storage/67MCGYCS/Teymur et al. - Unknown - Implicit Probabilistic Integrators for ODEs.pdf}
}

@techreport{teymurProbabilisticLinearMultistep,
  title = {Probabilistic {{Linear Multistep Methods}}},
  author = {Teymur, Onur and Zygalakis, Konstantinos and Calderhead, Ben},
  abstract = {We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature [1, 2, 3, 4]. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations , so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice.},
  file = {/home/yanni/Zotero/storage/CL58IJGG/Teymur, Zygalakis, Calderhead - Unknown - Probabilistic Linear Multistep Methods.pdf}
}

@techreport{thickstunCHANGEMEASURE,
  title = {{{CHANGE OF MEASURE}}},
  author = {Thickstun, John},
  abstract = {Suppose P is be a {$\sigma$}-finite measure and X is a r.v. on ({$\Omega$}, F, P). Let B(R) and L(R) denote the Borel and Lebesgue {$\sigma$}-algebras respectively. We can define the pushforward measure X * P : L(R) \textrightarrow{} B(R) for any B {$\in$} L(R) by the map X * P (B) = P (X {$\in$} B) = {$\Omega$} 1 X{$\in$}B dP. This map is more commonly called the law of X, often denoted P X. Because X * P is itself a measure, we can naturally write X * P (B) = B d(X * P). We are now in a position to state a natural change of variable formulas (obviously we must do some work to prove that the equation holds; it conveniently does whenever the integrands are integrable) {$\Omega$} f \textbullet{} XdP = R f d(X * P). This is, of course, just the familiar law of the unconscious statistician: Ef (X) = {$\Omega$} f (X)dP = {$\infty$} -{$\infty$} f (x)d(X * P). Using this theorem with an indicator function, we can rewrite probabilities as expectations: P (X {$\in$} B) = B d(X * P) = {$\infty$} -{$\infty$} 1 x{$\in$}B d(X * P) = {$\Omega$} 1 X{$\in$}B dP = E1 X{$\in$}B . Let \textmu{} be Lebesgue measure. If X * P \textmu{} then by the Radon-Nikodym theorem dX * P/d\textmu{} exists and P (X {$\in$} B) = X * P (B) = B dX * P d\textmu{} d\textmu. This Radon-Nikodym derivative is more commonly referred to as the density of X; note the crucial requirement of absolute continuity which in this case (necessarily) exactly coincides with the existence of a density. By the chain rule, Ef (X) = {$\infty$} -{$\infty$} f (x)d(X * P) = {$\infty$} -{$\infty$} f (x) dX * P d\textmu{} d\textmu, P (X {$\in$} B) = {$\infty$} -{$\infty$} 1 x{$\in$}B d(X * P) = {$\infty$} -{$\infty$} 1 x{$\in$}B dX * P d\textmu{} d\textmu.},
  file = {/home/yanni/Zotero/storage/9EPD5YXG/Thickstun - Unknown - CHANGE OF MEASURE.pdf}
}

@article{thierfelderTrendingOrnsteinUhlenbeckProcess,
  title = {The Trending {{Ornstein-Uhlenbeck Process}} and Its {{Applications}} in {{Mathematical Finance}}},
  author = {Thierfelder, Dr Christian},
  pages = {93},
  langid = {english},
  file = {/home/yanni/Zotero/storage/JSQULJGU/Thierfelder - The trending Ornstein-Uhlenbeck Process and its Ap.pdf}
}

@article{thomasfletcherGeodesicRegressionTheory2013,
  title = {Geodesic {{Regression}} and the {{Theory}} of {{Least Squares}} on {{Riemannian Manifolds}}},
  author = {Thomas Fletcher, P.},
  year = {2013},
  month = nov,
  journal = {Int J Comput Vis},
  volume = {105},
  number = {2},
  pages = {171--185},
  issn = {1573-1405},
  doi = {10.1007/s11263-012-0591-y},
  abstract = {This paper develops the theory of geodesic regression and least-squares estimation on Riemannian manifolds. Geodesic regression is a method for finding the relationship between a real-valued independent variable and a manifold-valued dependent random variable, where this relationship is modeled as a geodesic curve on the manifold. Least-squares estimation is formulated intrinsically as a minimization of the sum-of-squared geodesic distances of the data to the estimated model. Geodesic regression is a direct generalization of linear regression to the manifold setting, and it provides a simple parameterization of the estimated relationship as an initial point and velocity, analogous to the intercept and slope. A nonparametric permutation test for determining the significance of the trend is also given. For the case of symmetric spaces, two main theoretical results are established. First, conditions for existence and uniqueness of the least-squares problem are provided. Second, a maximum likelihood criteria is developed for a suitable definition of Gaussian errors on the manifold. While the method can be generally applied to data on any manifold, specific examples are given for a set of synthetically generated rotation data and an application to analyzing shape changes in the corpus callosum due to age.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/NDV7YCS8/Thomas Fletcher - 2013 - Geodesic Regression and the Theory of Least Square.pdf}
}

@article{tianxiaoSCIENCECHINAMathematics2017,
  title = {{{SCIENCE CHINA Mathematics CrossMark Asymptotically}} Efficient Parameter Estimation for Ordinary Differential Equations},
  author = {TianXiao, Pang and PeiSi, Yan and Harrison, Zhou H},
  year = {2017},
  journal = {Sci China Math},
  volume = {60},
  number = {11},
  pages = {2263--2286},
  doi = {10.1007/s11425-017-9155-0},
  abstract = {Parameter estimation for ordinary differential equations arises in many fields of science and engineering. To be the best of our knowledge, traditional methods are often either computationally intensive or inaccurate for statistical inference. Ramsay et al. (2007) proposed a generalized profiling procedure. It is easily implementable and has been demonstrated to have encouraging numerical performance. However, little is known about statistical properties of this procedure. In this paper, we provide a theoretical justification of the generalized profiling procedure. Under some regularity conditions, the procedure is shown to be consistent for a broad range of tuning parameters. When the tuning parameters are sufficiently large, the procedure can be further shown to be asymptotically normal and efficient.},
  keywords = {65C60,asymptotic efficiency,consistency,generalized profiling procedure,ordinary differential equations,splines MSC(2010) 62H12},
  file = {/home/yanni/Zotero/storage/VCRLL9ER/TianXiao, PeiSi, Harrison - 2017 - SCIENCE CHINA Mathematics CrossMark Asymptotically efficient parameter estimation for ordinary differ.pdf}
}

@article{tonSpatialMappingGaussian2018,
  title = {Spatial Mapping with {{Gaussian}} Processes and Nonstationary {{Fourier}} Features},
  author = {Ton, Jean Francois and Flaxman, Seth and Sejdinovic, Dino and Bhatt, Samir},
  year = {2018},
  month = dec,
  journal = {Spatial Statistics},
  volume = {28},
  pages = {59--78},
  publisher = {{Elsevier B.V.}},
  issn = {22116753},
  doi = {10.1016/j.spasta.2018.02.002},
  abstract = {The use of covariance kernels is ubiquitous in the field of spatial statistics. Kernels allow data to be mapped into high-dimensional feature spaces and can thus extend simple linear additive methods to nonlinear methods with higher order interactions. However, until recently, there has been a strong reliance on a limited class of stationary kernels such as the Mat\'ern or squared exponential, limiting the expressiveness of these modelling approaches. Recent machine learning research has focused on spectral representations to model arbitrary stationary kernels and introduced more general representations that include classes of nonstationary kernels. In this paper, we exploit the connections between Fourier feature representations, Gaussian processes and neural networks to generalise previous approaches and develop a simple and efficient framework to learn arbitrarily complex nonstationary kernel functions directly from the data, while taking care to avoid overfitting using state-of-the-art methods from deep learning. We highlight the very broad array of kernel classes that could be created within this framework. We apply this to a time series dataset and a remote sensing problem involving land surface temperature in Eastern Africa. We show that without increasing the computational or storage complexity, nonstationary kernels can be used to improve generalisation performance and provide more interpretable results.},
  keywords = {Gaussian process,Nonstationary,Random Fourier features,Spatial statistics}
}

@techreport{TOPOLOGICALSUPPORTGAUSSIAN,
  title = {{{THE TOPOLOGICAL SUPPORT OF GAUSSIAN MEASURE IN BANACH SPACE N}}. {{N}}. {{VAKHANIA}}},
  file = {/home/yanni/Zotero/storage/8N5K5JWW/Unknown - Unknown - THE TOPOLOGICAL SUPPORT OF GAUSSIAN MEASURE IN BANACH SPACE N. N. VAKHANIA.pdf}
}

@article{tronarpProbabilisticSolutionsOrdinary2019,
  title = {Probabilistic Solutions to Ordinary Differential Equations as Nonlinear {{Bayesian}} Filtering: A New Perspective},
  shorttitle = {Probabilistic Solutions to Ordinary Differential Equations as Nonlinear {{Bayesian}} Filtering},
  author = {Tronarp, Filip and Kersting, Hans and S{\"a}rkk{\"a}, Simo and Hennig, Philipp},
  year = {2019},
  month = nov,
  journal = {Stat Comput},
  volume = {29},
  number = {6},
  pages = {1297--1315},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-019-09900-1},
  abstract = {We formulate probabilistic numerical approximations to solutions of ordinary differential equations (ODEs) as problems in Gaussian process (GP) regression with nonlinear measurement functions. This is achieved by defining the measurement sequence to consist of the observations of the difference between the derivative of the GP and the vector field evaluated at the GP\textemdash which are all identically zero at the solution of the ODE. When the GP has a state-space representation, the problem can be reduced to a nonlinear Bayesian filtering problem and all widely used approximations to the Bayesian filtering and smoothing problems become applicable. Furthermore, all previous GP-based ODE solvers that are formulated in terms of generating synthetic measurements of the gradient field come out as specific approximations. Based on the nonlinear Bayesian filtering problem posed in this paper, we develop novel Gaussian solvers for which we establish favourable stability properties. Additionally, non-Gaussian approximations to the filtering problem are derived by the particle filter approach. The resulting solvers are compared with other probabilistic solvers in illustrative experiments.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/LCJVHSXK/Tronarp et al. - 2019 - Probabilistic solutions to ordinary differential e.pdf}
}

@article{truongSelectiveReviewOffline2020,
  title = {Selective Review of Offline Change Point Detection Methods},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  year = {2020},
  month = feb,
  journal = {Signal Processing},
  volume = {167},
  pages = {107299},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2019.107299},
  langid = {english},
  file = {/home/yanni/Zotero/storage/FVGKULQI/Truong et al. - 2020 - Selective review of offline change point detection.pdf}
}

@book{tuIntroductionManifolds2011,
  title = {An {{Introduction}} to {{Manifolds}}},
  author = {Tu, Loring W.},
  year = {2011},
  series = {Universitext},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-7400-6},
  isbn = {978-1-4419-7399-3 978-1-4419-7400-6},
  langid = {english},
  file = {/home/yanni/Zotero/storage/J97QXCKN/Tu - 2011 - An Introduction to Manifolds.pdf}
}

@article{turagaStatisticalComputationsGrassmann2011a,
  title = {Statistical {{Computations}} on {{Grassmann}} and {{Stiefel Manifolds}} for {{Image}} and {{Video-Based Recognition}}},
  author = {Turaga, P. and Veeraraghavan, A. and Srivastava, A. and Chellappa, R.},
  year = {2011},
  month = nov,
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {33},
  number = {11},
  pages = {2273--2286},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2011.52},
  abstract = {In this paper, we examine image and video based recognition applications where the underlying models have a special structure the linear subspace structure. We discuss how commonly used parametric models for videos and image-sets can be described using the unified framework of Grassmann and Stiefel manifolds. We first show that the parameters of linear dynamic models are finite dimensional linear subspaces of appropriate dimensions. Unordered image-sets as samples from a finite-dimensional linear subspace naturally fall under this framework. We show that the study of inference over subspaces can be naturally cast as an inference problem on the Grassmann manifold. To perform recognition using subspace-based models, we need tools from the Riemannian geometry of the Grassmann manifold. This involves a study of the geometric properties of the space, appropriate definitions of Riemannian metrics, and definition of geodesics. Further, we derive statistical modeling of inter- and intra-class variations that respect the geometry of the space. We apply techniques such as intrinsic and extrinsic statistics, to enable maximum-likelihood classification. We also provide algorithms for unsupervised clustering derived from the geometry of the manifold.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/FCEJ2B82/Turaga et al. - 2011 - Statistical Computations on Grassmann and Stiefel .pdf}
}

@article{turnerAdaptiveSequentialBayesian,
  title = {Adaptive {{Sequential Bayesian Change Point Detection}}},
  author = {Turner, Ryan and Saatci, Yunus and Rasmussen, Carl Edward},
  pages = {4},
  langid = {english},
  file = {/home/yanni/Zotero/storage/6UB8HIEM/Turner et al. - Adaptive Sequential Bayesian Change Point Detectio.pdf}
}

@article{turnerOnlineVariationalApproximations,
  title = {Online {{Variational Approximations}} to Non-{{Exponential Family Change Point Models}}: {{With Application}} to {{Radar Tracking}}},
  author = {Turner, Ryan D and Bottone, Steven and Stanek, Clay J},
  pages = {9},
  abstract = {The Bayesian online change point detection (BOCPD) algorithm provides an efficient way to do exact inference when the parameters of an underlying model may suddenly change over time. BOCPD requires computation of the underlying model's posterior predictives, which can only be computed online in O(1) time and memory for exponential family models. We develop variational approximations to the posterior on change point times (formulated as run lengths) for efficient inference when the underlying model is not in the exponential family, and does not have tractable posterior predictive distributions. In doing so, we develop improvements to online variational inference. We apply our methodology to a tracking problem using radar data with a signal-to-noise feature that is Rice distributed. We also develop a variational method for inferring the parameters of the (non-exponential family) Rice distribution.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/4BH277ET/Turner et al. - Online Variational Approximations to non-Exponenti.pdf}
}

@misc{vadeboncoeurDeepProbabilisticModels2022,
  title = {Deep {{Probabilistic Models}} for {{Forward}} and {{Inverse Problems}} in {{Parametric PDEs}}},
  author = {Vadeboncoeur, Arnaud and Akyildiz, {\"O}mer Deniz and Kazlauskaite, Ieva and Girolami, Mark and Cirak, Fehmi},
  year = {2022},
  month = aug,
  number = {arXiv:2208.04856},
  eprint = {arXiv:2208.04856},
  publisher = {{arXiv}},
  abstract = {We formulate a class of physics-driven deep latent variable models (PDDLVM) to learn parameter-to-solution (forward) and solution-to-parameter (inverse) maps of parametric partial differential equations (PDEs). Our formulation leverages the finite element method (FEM), deep neural networks, and probabilistic modeling to assemble a deep probabilistic framework in which the forward and inverse maps are approximated with coherent uncertainty quantification. Our probabilistic model explicitly incorporates a parametric PDE-based density and a trainable solution-to-parameter network while the introduced amortized variational family postulates a parameter-to-solution network, all of which are jointly trained. Furthermore, the proposed methodology does not require any expensive PDE solves and is physics-informed only at training time, which allows real-time emulation of PDEs and generation of inverse problem solutions after training, bypassing the need for FEM solve operations with comparable accuracy to FEM solutions. The proposed framework further allows for a seamless integration of observed data for solving inverse problems and building generative models. We demonstrate the effectiveness of our method on a nonlinear Poisson problem, elastic shells with complex 3D geometries, and integrating generic physics-informed neural networks (PINN) architectures. We achieve up to three orders of magnitude speed-ups after training compared to traditional FEM solvers, while outputting coherent uncertainty estimates.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/VX3FTQTF/Vadeboncoeur et al_2022_Deep Probabilistic Models for Forward and Inverse Problems in Parametric PDEs.pdf}
}

@article{vaicenaviciusEvaluatingModelCalibration,
  title = {Evaluating Model Calibration in Classification},
  author = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik},
  pages = {19},
  abstract = {Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safetycritical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/YMG8XTER/Vaicenavicius et al. - Evaluating model calibration in classification.pdf}
}

@article{vanlierParameterUncertaintyBiochemical2013,
  title = {Parameter Uncertainty in Biochemical Models Described by Ordinary Differential Equations},
  author = {Vanlier, J. and Tiemann, C. A. and Hilbers, P. A.J. and {van Riel}, N. A.W.},
  year = {2013},
  journal = {Mathematical Biosciences},
  issn = {00255564},
  doi = {10.1016/j.mbs.2013.03.006},
  abstract = {Improved mechanistic understanding of biochemical networks is one of the driving ambitions of Systems Biology. Computational modeling allows the integration of various sources of experimental data in order to put this conceptual understanding to the test in a quantitative manner. The aim of computational modeling is to obtain both predictive as well as explanatory models for complex phenomena, hereby providing useful approximations of reality with varying levels of detail. As the complexity required to describe different system increases, so does the need for determining how well such predictions can be made. Despite efforts to make tools for uncertainty analysis available to the field, these methods have not yet found widespread use in the field of Systems Biology. Additionally, the suitability of the different methods strongly depends on the problem and system under investigation. This review provides an introduction to some of the techniques available as well as gives an overview of the state-of-the-art methods for parameter uncertainty analysis. \textcopyright{} 2013 Elsevier Inc.},
  keywords = {Bayesian,Dynamical systems,Inference,Ordinary differential equations,Parameter estimation,Uncertainty analysis},
  file = {/home/yanni/Zotero/storage/WSNLD5QF/Vanlier et al. - 2013 - Parameter uncertainty in biochemical models described by ordinary differential equations.pdf}
}

@techreport{varahSPLINELEASTSQUARES1982,
  title = {A {{SPLINE LEAST SQUARES METHOD FOR NUMERICAL PARAMETER ESTIMATION IN DIFFERENTIAL EQUATIONS}}*},
  author = {Varah, J M},
  year = {1982},
  journal = {SIAM J. ScI. STAT. COMPUT},
  volume = {3},
  number = {1},
  abstract = {In this paper, we describe a straightforward least squares approach to the problem of finding numerical values for parameters occurring in differential equations so that the solution best fits some observed data. The method consists of first fitting the given data by least squares using cubic spline functions with knots chosen interactively, and then finding the paramters by least squares solution of the differential equation sampled at a set of points. We illustrate the method by four problems from chemical and biological modeling.},
  keywords = {differential equations,parameter estimation,spline fitting},
  file = {/home/yanni/Zotero/storage/BV8Q8SV9/Varah - 1982 - A SPLINE LEAST SQUARES METHOD FOR NUMERICAL PARAMETER ESTIMATION IN DIFFERENTIAL EQUATIONS.pdf}
}

@book{villaniOptimalTransportOld2009,
  title = {Optimal Transport: Old and New},
  shorttitle = {Optimal Transport},
  author = {Villani, C{\'e}dric},
  year = {2009},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  number = {338},
  publisher = {{Springer}},
  address = {{Berlin}},
  isbn = {978-3-540-71049-3},
  langid = {english},
  lccn = {QA402.5 .V538 2009},
  keywords = {Dynamics,Dynamique,G√©om√©trie diff√©rentielle,Geometry; Differential,Mathematical optimization,Optimisation math√©matique,Probabilit√©s,Probabilities,Probl√®mes de transport (Programmation),Transportation problems (Programming)},
  annotation = {OCLC: ocn244421231},
  file = {/home/yanni/Zotero/storage/QEFP9ZJ5/Villani - 2009 - Optimal transport old and new.pdf}
}

@misc{Vlra206PDFElsevier,
  title = {Vlra206.{{PDF}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1006/jmva.1993.1064},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0047259X8371064X?token=116C68EBC83B04B773945DE6C4367E9E0C16B37619B0A93E45800B6383A43096C135679CD74F1BBE7AA4BA94CD3F0C4A\&originRegion=eu-west-1\&originCreation=20210604110507},
  langid = {english},
  file = {/home/yanni/Zotero/storage/D2A3LZZQ/S0047259X8371064X.html}
}

@techreport{wangGaussianProcessesBayesian,
  title = {Gaussian {{Processes}} for {{Bayesian Estimation}} in {{Ordinary Differential Equations}}},
  author = {Wang, Yali and Barber DAVIDBARBER, David},
  abstract = {Bayesian parameter estimation in coupled ordinary differential equations (ODEs) is challenging due to the high computational cost of numerical integration. In gradient matching a separate data model is introduced with the property that its gradient may be calculated easily. Parameter estimation is then achieved by requiring consistency between the gradients computed from the data model and those specified by the ODE. We propose a Gaussian process model that directly links state derivative information with system observations , simplifying previous approaches and improving estimation accuracy.},
  file = {/home/yanni/Zotero/storage/GGCZJ8GS/Wang, Barber DAVIDBARBER - Unknown - Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations.pdf}
}

@article{wangPARAMETERESTIMATIONODES,
  title = {{{PARAMETER ESTIMATION FOR ODES USING A CROSS-ENTROPY APPROACH}} *},
  author = {Wang, B O and Enright, Wayne},
  doi = {10.1137/120889733},
  abstract = {Parameter estimation for ODEs is an important topic in numerical analysis. In this paper, we present a novel approach to address this inverse problem that can be applied to differential equations that may include delay terms. Cross-entropy algorithms are general algorithms which can be applied to solve global optimization problems. The main steps of cross-entropy methods are first to generate a set of trial samples from a certain distribution and then to update the distribution based on these generated sample trials. To overcome the prohibitive computation of standard cross-entropy algorithms, we develop a modification combining local search techniques. The modified cross-entropy algorithm can improve the convergence rate and reduce the chances of converging to a local optimum. Two different coding schemes (continuous coding and discrete coding) are introduced (to represent the search space that we are optimizing over). Continuous coding uses a truncated multivariate Gaussian to generate trial samples, while discrete coding reduces the search space to consider only a finite (but relatively dense) subset of the feasible parameter values and uses a Bernoulli distribution to generate the trial samples (which are fixed point approximations to the parameters). Extensive numerical experiments are conducted to illustrate the power and advantages of the proposed methods. Compared to other existing state-of-the-art approaches on some benchmark problems for parameter estimation, our methods have three main advantages: (1) they are robust to noise in the data to be fitted; (2) they are not sensitive to the number of observation points; and (3) the modified versions exhibit faster convergence without sacrificing accuracy.},
  keywords = {65L05,65L09,cross-entropy,delay differen-tial equation AMS subject classific,ordinary differential equation,parameter estimation},
  file = {/home/yanni/Zotero/storage/RWC2DZBK/Wang, Enright - Unknown - PARAMETER ESTIMATION FOR ODES USING A CROSS-ENTROPY APPROACH.pdf}
}

@article{wangParameterEstimationOdes2013,
  title = {Parameter Estimation for Odes Using a Cross-Entropy Approach},
  author = {Wang, Bo and Enright, Wayne},
  year = {2013},
  journal = {SIAM Journal on Scientific Computing},
  volume = {35},
  number = {6},
  pages = {A2718----A2737},
  publisher = {{SIAM}}
}

@techreport{wangRoleSymmetryBayesian2019,
  title = {A {{Role}} for {{Symmetry}} in the {{Bayesian Solution}} of {{Differential Equations}}},
  author = {Wang, Junyang and Cockayne, Jon and Oates, Chris J},
  year = {2019},
  abstract = {The interpretation of numerical methods, such as finite difference methods for differential equations, as point estimators suggests that formal uncertainty quantifica-tion can also be performed in this context. Competing statistical paradigms can be considered and Bayesian probabilistic numerical methods (PNMs) are obtained when Bayesian statistical principles are deployed. Bayesian PNM have the appealing property of being closed under composition, such that uncertainty due to different sources of discretisation in a numerical method can be jointly modelled and rigorously propagated. Despite recent attention, no exact Bayesian PNM for the numerical solution of ordinary differential equations (ODEs) has been proposed. This raises the fundamental question of whether exact Bayesian methods for (in general nonlinear) ODEs even exist. The purpose of this paper is to provide a positive answer for a limited class of ODE. To this end, we work at a foundational level, where a novel Bayesian PNM is proposed as a proof-of-concept. Our proposal is a synthesis of classical Lie group methods, to exploit underlying symmetries in the gradient field, and non-parametric regression in a transformed solution space for the ODE. The procedure is presented in detail for first and second order ODEs and relies on a certain strong technical condition existence of a solvable Lie algebra-being satisfied. Numerical illustrations are provided.},
  file = {/home/yanni/Zotero/storage/MWKVPBXF/Wang, Cockayne, Oates - 2019 - A Role for Symmetry in the Bayesian Solution of Differential Equations.pdf}
}

@techreport{weisseGlobalSensitivityAnalysis2009,
  title = {Global {{Sensitivity Analysis}} of {{Ordinary Differential Equations Adaptive Density Propagation Using Approximate Approximations}}},
  author = {Wei{\ss}e, Andrea Yeong},
  year = {2009},
  file = {/home/yanni/Zotero/storage/EBKIW63A/andreaweissethesis.pdf}
}

@misc{wildGeneralizedVariationalInference2022,
  title = {Generalized {{Variational Inference}} in {{Function Spaces}}: {{Gaussian Measures}} Meet {{Bayesian Deep Learning}}},
  shorttitle = {Generalized {{Variational Inference}} in {{Function Spaces}}},
  author = {Wild, Veit D. and Hu, Robert and Sejdinovic, Dino},
  year = {2022},
  month = may,
  number = {arXiv:2205.06342},
  eprint = {arXiv:2205.06342},
  publisher = {{arXiv}},
  abstract = {We develop a framework for generalized variational inference in infinite-dimensional function spaces and use it to construct a method termed Gaussian Wasserstein inference (GWI). GWI leverages the Wasserstein distance between Gaussian measures on the Hilbert space of square-integrable functions in order to determine a variational posterior using a tractable optimisation criterion and avoids pathologies arising in standard variational function space inference. An exciting application of GWI is the ability to use deep neural networks in the variational parametrisation of GWI, combining their superior predictive performance with the principled uncertainty quantification analogous to that of Gaussian processes. The proposed method obtains state-of-the-art performance on several benchmark datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/RD6VMKFA/Wild et al_2022_Generalized Variational Inference in Function Spaces.pdf}
}

@misc{wildVariationalGaussianProcesses2021,
  title = {Variational {{Gaussian Processes}}: {{A Functional Analysis View}}},
  shorttitle = {Variational {{Gaussian Processes}}},
  author = {Wild, Veit and Wynne, George},
  year = {2021},
  month = oct,
  number = {arXiv:2110.12798},
  eprint = {arXiv:2110.12798},
  publisher = {{arXiv}},
  abstract = {Variational Gaussian process (GP) approximations have become a standard tool in fast GP inference. This technique requires a user to select variational features to increase efficiency. So far the common choices in the literature are disparate and lacking generality. We propose to view the GP as lying in a Banach space which then facilitates a unified perspective. This is used to understand the relationship between existing features and to draw a connection between kernel ridge regression and variational GP approximations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/SEJMVY5T/Wild_Wynne_2021_Variational Gaussian Processes.pdf}
}

@article{wilsonGeneralisedWishartProcesses2010,
  title = {Generalised {{Wishart Processes}}},
  author = {Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2010},
  month = dec,
  journal = {arXiv:1101.0240 [math, q-fin, stat]},
  eprint = {1101.0240},
  primaryclass = {math, q-fin, stat},
  abstract = {We introduce a stochastic process with Wishart marginals: the generalised Wishart process (GWP). It is a collection of positive semi-definite random matrices indexed by any arbitrary dependent variable. We use it to model dynamic (e.g. time varying) covariance matrices. Unlike existing models, it can capture a diverse class of covariance structures, it can easily handle missing data, the dependent variable can readily include covariates other than time, and it scales well with dimension; there is no need for free parameters, and optional parameters are easy to interpret. We describe how to construct the GWP, introduce general procedures for inference and predictions, and show that it outperforms its main competitor, multivariate GARCH, even on financial data that especially suits GARCH. We also show how to predict the mean of a multivariate process while accounting for dynamic correlations.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Probability,Quantitative Finance - Computational Finance,Quantitative Finance - Statistical Finance,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/XPTV8F7P/Wilson and Ghahramani - 2010 - Generalised Wishart Processes.pdf;/home/yanni/Zotero/storage/SAL5GM4M/1101.html}
}

@article{wongEmbeddedRidgeApproximations2020,
  title = {Embedded {{Ridge Approximations}}},
  author = {Wong, Chun Yui and Seshadri, Pranay and Parks, Geoffrey and Girolami, Mark},
  year = {2020},
  month = aug,
  journal = {arXiv:1907.07037 [cs, math]},
  eprint = {1907.07037},
  primaryclass = {cs, math},
  abstract = {Many quantities of interest (qois) arising from differential-equation-centric models can be resolved into functions of scalar fields. Examples of such qois include the lift over an airfoil or the displacement of a loaded structure; examples of corresponding fields are the static pressure field in a computational fluid dynamics solution, and the strain field in the finite element elasticity analysis. These scalar fields are evaluated at each node within a discretised computational domain. In certain scenarios, the field at a certain node is only weakly influenced by far-field perturbations; it is likely to be strongly governed by local perturbations, which in turn can be caused by uncertainties in the geometry. One can interpret this as a strong anisotropy of the field with respect to uncertainties in prescribed inputs. We exploit this notion of localised scalar-field influence for approximating global qois, which often are integrals of certain field quantities. We formalise our ideas by assigning ridge approximations for the field at select nodes. This embedded ridge approximation has favorable theoretical properties for approximating a global qoi in terms of the reduced number of computational evaluations required. Parallels are drawn between our proposed approach, active subspaces and vector-valued dimension reduction. Additionally, we study the ridge directions of adjacent nodes and devise algorithms that can recover field quantities at selected nodes, when storing the ridge profiles at a subset of nodes---paving the way for novel reduced order modeling strategies. Our paper offers analytical and simulation-based examples that expose different facets of embedded ridge approximations.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/QKHDB6IB/Wong et al. - 2020 - Embedded Ridge Approximations.pdf}
}

@article{woodSlowMomentumFast2021,
  title = {Slow {{Momentum}} with {{Fast Reversion}}: {{A Trading Strategy Using Deep Learning}} and {{Changepoint Detection}}},
  shorttitle = {Slow {{Momentum}} with {{Fast Reversion}}},
  author = {Wood, Kieran and Roberts, Stephen and Zohren, Stefan},
  year = {2021},
  month = may,
  journal = {arXiv:2105.13727 [cs, q-fin, stat]},
  eprint = {2105.13727},
  primaryclass = {cs, q-fin, stat},
  abstract = {Momentum strategies are an important part of alternative investments and are at the heart of commodity trading advisors (CTAs). These strategies have however been found to have difficulties adjusting to rapid changes in market conditions, such as during the 2020 market crash. In particular, immediately after momentum turning points, where a trend reverses from an uptrend (downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies are prone to making bad bets. To improve the response to regime change, we introduce a novel approach, where we insert an online change-point detection (CPD) module into a Deep Momentum Network (DMN) [1] pipeline, which uses an LSTM deep-learning architecture to simultaneously learn both trend estimation and position sizing. Furthermore, our model is able to optimise the way in which it balances 1) a slow momentum strategy which exploits persisting trends, but does not overreact to localised price moves, and 2) a fast meanreversion strategy regime by quickly flipping its position, then swapping it back again to exploit localised price moves. Our CPD module outputs a changepoint location and severity score, allowing our model to learn to respond to varying degrees of disequilibrium, or smaller and more localised changepoints, in a data driven manner. Using a portfolio of 50, liquid, continuous futures contracts over the period 1990\textendash 2020, the addition of the CPD module leads to an improvement in Sharpe ratio of 33\%. Even more notably, this module is especially beneficial in periods of significant nonstationarity, and in particular, over the most recent years tested (2015-2020) the performance boost is approximately 400\%. This is especially interesting as traditional momentum strategies have been underperforming in this period.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Trading and Market Microstructure,Statistics - Machine Learning},
  file = {/home/yanni/Zotero/storage/LTJ4MCAV/Wood et al. - 2021 - Slow Momentum with Fast Reversion A Trading Strat.pdf}
}

@misc{xiongClusteredActivesubspaceBased2021,
  title = {Clustered Active-Subspace Based Local {{Gaussian Process}} Emulator for High-Dimensional and Complex Computer Models},
  author = {Xiong, Junda and Cai, Xin and Li, Jinglai},
  year = {2021},
  month = nov,
  number = {arXiv:2101.00057},
  eprint = {arXiv:2101.00057},
  publisher = {{arXiv}},
  abstract = {Quantifying uncertainties in physical or engineering systems often requires a large number of simulations of the underlying computer models that are computationally intensive. Emulators or surrogate models are often used to accelerate the computation in such problems, and in this regard the Gaussian Process (GP) emulator is a popular choice for its ability to quantify the approximation error in the emulator itself. However, a major limitation of the GP emulator is that it can not handle problems of very high dimensions, which is often addressed with dimension reduction techniques. In this work we hope to address an issue that the models of interest are so complex that they admit different low dimensional structures in different parameter regimes. Building upon the active subspace method for dimension reduction, we propose a clustered active subspace method which identifies the local low-dimensional structures as well as the parameter regimes they are in (represented as clusters), and then construct low dimensional and local GP emulators within the clusters. Specifically we design a clustering method based on the gradient information to identify these clusters, and a local GP construction procedure to construct the GP emulator within a local cluster. With numerical examples, we demonstrate that the proposed method is effective when the underlying models are of complex low-dimensional structures.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/R8EPMC3I/Xiong et al. - 2021 - Clustered active-subspace based local Gaussian Pro.pdf;/home/yanni/Zotero/storage/64HNQ6RU/2101.html}
}

@book{xiuNumericalMethodsStochastic2010,
  title = {Numerical {{Methods}} for {{Stochastic Computations}}: {{A Spectral Method Approach}}},
  shorttitle = {Numerical {{Methods}} for {{Stochastic Computations}}},
  author = {Xiu, Dongbin},
  year = {2010},
  month = jul,
  journal = {Numerical Methods for Stochastic Computations},
  publisher = {{Princeton University Press}},
  doi = {10.1515/9781400835348},
  abstract = {The@ first graduate-level textbook to focus on fundamental aspects of numerical methods for stochastic computations, this book describes the class of numerical methods based on generalized polynomial chaos (gPC). These fast, efficient, and accurate methods are an extension of the classical spectral methods of high-dimensional random spaces. Designed to simulate complex systems subject to random inputs, these methods are widely used in many areas of computer science and engineering. The book introduces polynomial approximation theory and probability theory; describes the basic theory of gPC methods through numerical examples and rigorous development; details the procedure for converting stochastic equations into deterministic ones; using both the Galerkin and collocation approaches; and discusses the distinct differences and challenges arising from high-dimensional problems. The last section is devoted to the application of gPC methods to critical areas such as inverse problems and data assimilation. Ideal for use by graduate students and researchers both in the classroom and for self-study, Numerical Methods for Stochastic Computations provides the required tools for in-depth research related to stochastic computations. The first graduate-level textbook to focus on the fundamentals of numerical methods for stochastic computations Ideal introduction for graduate courses or self-study Fast, efficient, and accurate numerical methods Polynomial approximation theory and probability theory included Basic gPC methods illustrated through examples},
  isbn = {978-1-4008-3534-8},
  langid = {english},
  keywords = {Accuracy and precision,Algorithm,Approximation,Approximation error,Approximation theory,Basis function,Beta distribution,Big O notation,Boundary value problem,Burgers' equation,Cauchy‚ÄìSchwarz inequality,Central limit theorem,Coefficient,Collocation method,Computation,Computational mathematics,Conditional probability,Continuous function,Continuous function (set theory),Convergence of random variables,Covariance function,Covariance matrix,Curse of dimensionality,Deterministic system,Dimension,Dimension (vector space),Distribution function,Eigenfunction,Eigenvalues and eigenvectors,Equation,Estimation,Estimation theory,Existential quantification,Galerkin method,Gaussian process,Gaussian quadrature,Gibbs phenomenon,Hermite polynomials,Identity matrix,Independence (probability theory),Indicator function,Initial condition,Jacobi polynomials,Kalman filter,Karhunen‚ÄìLo√®ve theorem,Kullback‚ÄìLeibler divergence,Lagrange polynomial,Laguerre polynomials,Legendre polynomials,Lexicographical order,Likelihood function,Linear space (geometry),Marginal distribution,Measurement,Moment-generating function,Monte Carlo method,Normal distribution,Numerical analysis,Numerical error,Numerical integration,Observational error,Orthogonal polynomials,Orthogonality,Parameter,Parametrization,Partial differential equation,Poisson point process,Polynomial,Polynomial chaos,Prediction,Probability,Probability density function,Probability distribution,Probability space,Probability theory,Projection (linear algebra),Quantity,Random field,Random variable,Rate of convergence,Recurrence relation,Sampling (statistics),Scientific notation,Series expansion,Simultaneous equations,Sparse grid,Special case,Spectral method,Standard deviation,Statistic,Stochastic,Stochastic computing,Stochastic process,Stochastic simulation,Subset,Theorem,Uncertainty,Uncertainty quantification,Variable (mathematics),Variance},
  file = {/home/yanni/Zotero/storage/RXNC46AW/Xiu - 2010 - Numerical Methods for Stochastic Computations A S.pdf}
}

@article{xuanBayesianInferenceChange,
  title = {Bayesian {{Inference}} on {{Change Point Problems}}},
  author = {Xuan, Xiang},
  pages = {71},
  langid = {english},
  file = {/home/yanni/Zotero/storage/INMCD7KR/Xuan - Bayesian Inference on Change Point Problems.pdf}
}

@article{yadavODEAugmentedTraining2016,
  title = {{{ODE}} - {{Augmented Training Improves Anomaly Detection}} in {{Sensor Data}} from {{Machines}}},
  author = {Yadav, Mohit and Malhotra, Pankaj and Vig, Lovekesh and Sriram, K and Shroff, Gautam},
  year = {2016},
  pages = {1--5},
  abstract = {Machines of all kinds from vehicles to industrial equipment are increasingly instrumented with hundreds of sensors. Using such data to detect anomalous behaviour is critical for safety and efficient maintenance. However, anomalies occur rarely and with great variety in such systems, so there is often insufficient anomalous data to build reliable detectors. A standard approach to mitigate this problem is to use one class methods relying only on data from normal behaviour. Unfortunately, even these approaches are more likely to fail in the scenario of a dynamical system with manual control input(s). Normal behaviour in response to novel control input(s) might look very different to the learned detector which may be incorrectly detected as anomalous. In this paper, we address this issue by modelling time-series via Ordinary Differential Equations (ODE) and utilising such an ODE model to simulate the behaviour of dynamical systems under varying control inputs. The available data is then augmented with data generated from the ODE, and the anomaly detector is retrained on this augmented dataset. Experiments demonstrate that ODE-augmented training data allows better coverage of possible control input(s) and results in learning more accurate distinctions between normal and anomalous behaviour in time-series.},
  file = {/home/yanni/Zotero/storage/99D2S5GS/1605.01534.pdf}
}

@article{yuanLocalPolynomialRegression2012,
  title = {Local Polynomial Regression for Symmetric Positive Definite Matrices},
  author = {Yuan, Ying and Zhu, Hongtu and Lin, Weili and Marron, J. S.},
  year = {2012},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {74},
  number = {4},
  pages = {697--719},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2011.01022.x},
  abstract = {Summary. Local polynomial regression has received extensive attention for the non-parametric estimation of regression functions when both the response and the covariate are in Euclidean space. However, little has been done when the response is in a Riemannian manifold. We develop an intrinsic local polynomial regression estimate for the analysis of symmetric positive definite matrices as responses that lie in a Riemannian manifold with covariate in Euclidean space. The primary motivation and application of the methodology proposed is in computer vision and medical imaging. We examine two commonly used metrics, including the trace metric and the log-Euclidean metric on the space of symmetric positive definite matrices. For each metric, we develop a cross-validation bandwidth selection method, derive the asymptotic bias, variance and normality of the intrinsic local constant and local linear estimators, and compare their asymptotic mean-square errors. Simulation studies are further used to compare the estimators under the two metrics and to examine their finite sample performance. We use our method to detect diagnostic differences between diffusion tensors along fibre tracts in a study of human immunodeficiency virus.},
  langid = {english},
  keywords = {Bandwidth,Cross-validation,Intrinsic conditional expectation,Local polynomial regression,Symmetric positive definite matrix},
  file = {/home/yanni/Zotero/storage/JZHJNI3S/Yuan et al. - 2012 - Local polynomial regression for symmetric positive.pdf;/home/yanni/Zotero/storage/PXZWUWTI/j.1467-9868.2011.01022.html}
}

@article{yuanModelSelectionEstimation2006,
  title = {Model Selection and Estimation in Regression with Grouped Variables},
  author = {Yuan, Ming and Lin, Yi},
  year = {2006},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {68},
  number = {1},
  pages = {49--67},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2005.00532.x},
  abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods. \textcopyright{} 2006 Royal Statistical Society.},
  keywords = {Analysis of variance,Lasso,Least angle regression,Non-negative garrotte,Piecewise linear solution path},
  file = {/home/yanni/Zotero/storage/2M8MM92P/glasso.final.pdf}
}

@article{yuNoteOnlineChange2020,
  title = {A {{Note}} on {{Online Change Point Detection}}},
  author = {Yu, Yi and Padilla, Oscar Hernan Madrid and Wang, Daren and Rinaldo, Alessandro},
  year = {2020},
  month = nov,
  journal = {arXiv:2006.03283 [math, stat]},
  eprint = {2006.03283},
  primaryclass = {math, stat},
  abstract = {We investigate sequential change point estimation and detection in univariate nonparametric settings, where a stream of independent observations from sub-Gaussian distributions with a common variance factor and piecewise-constant but otherwise unknown means are collected. We develop a simple CUSUM-based methodology that provably control the probability of false alarms or the average run length while minimizing, in a minimax sense, the detection delay. We allow for all the model parameters to vary in order to capture a broad range of levels of statistical hardness for the problem at hand. We further show how our methodology is applicable to the case in which multiple change points are to be estimated sequentially.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/yanni/Zotero/storage/DPE2PPZW/Yu et al. - 2020 - A Note on Online Change Point Detection.pdf}
}

@article{zachosBayesianOnlineChangepoint,
  title = {Bayesian {{On-line Change-point Detection}}},
  author = {Zachos, Ioannis},
  pages = {85},
  langid = {english},
  file = {/home/yanni/Zotero/storage/7JGVMMG8/Zachos - Bayesian On-line Change-point Detection.pdf}
}

@misc{zhangGaussianProcessSubspace2021,
  title = {Gaussian {{Process Subspace Regression}} for {{Model Reduction}}},
  author = {Zhang, Ruda and Mak, Simon and Dunson, David},
  year = {2021},
  month = jul,
  number = {arXiv:2107.04668},
  eprint = {arXiv:2107.04668},
  publisher = {{arXiv}},
  abstract = {Subspace-valued functions arise in a wide range of problems, including parametric reduced order modeling (PROM). In PROM, each parameter point can be associated with a subspace, which is used for Petrov-Galerkin projections of large system matrices. Previous efforts to approximate such functions use interpolations on manifolds, which can be inaccurate and slow. To tackle this, we propose a novel Bayesian nonparametric model for subspace prediction: the Gaussian Process Subspace regression (GPS) model. This method is extrinsic and intrinsic at the same time: with multivariate Gaussian distributions on the Euclidean space, it induces a joint probability model on the Grassmann manifold, the set of fixed-dimensional subspaces. The GPS adopts a simple yet general correlation structure, and a principled approach for model selection. Its predictive distribution admits an analytical form, which allows for efficient subspace prediction over the parameter space. For PROM, the GPS provides a probabilistic prediction at a new parameter point that retains the accuracy of local reduced models, at a computational complexity that does not depend on system dimension, and thus is suitable for online computation. We give four numerical examples to compare our method to subspace interpolation, as well as two methods that interpolate local reduced models. Overall, GPS is the most data efficient, more computationally efficient than subspace interpolation, and gives smooth predictions with uncertainty quantification.},
  archiveprefix = {arxiv},
  keywords = {14M15; 35B30; 37M99; 53-04; 60B20; 60G15,Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/yanni/Zotero/storage/65X4WCQV/Zhang et al_2021_Gaussian Process Subspace Regression for Model Reduction.pdf}
}

@book{zhangNumericalMethodsStochastic2017,
  title = {Numerical {{Methods}} for {{Stochastic Partial Differential Equations}} with {{White Noise}}},
  author = {Zhang, Zhongqiang and Karniadakis, George Em},
  year = {2017},
  series = {Applied {{Mathematical Sciences}}},
  volume = {196},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-57511-7},
  isbn = {978-3-319-57510-0 978-3-319-57511-7},
  keywords = {Deterministic Integration Methods,Ito and Stratonovich Calculus,Long-time Integration,Nonlinear Stochastic Differential Equations,partial differential equations,Space and Time White Noise,Wick-Malliavin Approximation,Wong-Zakai Approximation},
  file = {/home/yanni/Zotero/storage/2VJ22ANV/Zhang and Karniadakis - 2017 - Numerical Methods for Stochastic Partial Different.pdf}
}

@book{zhangNumericalMethodsStochastic2017a,
  title = {Numerical {{Methods}} for {{Stochastic Partial Differential Equations}} with {{White Noise}}},
  author = {Zhang, Zhongqiang and Karniadakis, George Em},
  year = {2017},
  series = {Applied {{Mathematical Sciences}}},
  volume = {196},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-57511-7},
  isbn = {978-3-319-57510-0 978-3-319-57511-7},
  langid = {english},
  file = {/home/yanni/Zotero/storage/ZDTSUZHQ/Zhang and Karniadakis - 2017 - Numerical Methods for Stochastic Partial Different.pdf}
}

@misc{zimmermannManifoldInterpolationModel2019,
  title = {Manifold Interpolation and Model Reduction},
  author = {Zimmermann, Ralf},
  year = {2019},
  month = sep,
  number = {arXiv:1902.06502},
  eprint = {arXiv:1902.06502},
  publisher = {{arXiv}},
  abstract = {One approach to parametric and adaptive model reduction is via the interpolation of orthogonal bases, subspaces or positive definite system matrices. In all these cases, the sampled inputs stem from matrix sets that feature a geometric structure and thus form so-called matrix manifolds. This work will be featured as a chapter in the upcoming Handbook on Model Order Reduction (P. Benner, S. Grivet-Talocia, A. Quarteroni, G. Rozza, W.H.A. Schilders, L.M. Silveira, eds, to appear on DE GRUYTER) and reviews the numerical treatment of the most important matrix manifolds that arise in the context of model reduction. Moreover, the principal approaches to data interpolation and Taylor-like extrapolation on matrix manifolds are outlined and complemented by algorithms in pseudo-code.},
  archiveprefix = {arxiv},
  keywords = {15-01; 15A16; 15B10; 15B48; 53-04; 65F60; 41-01; 41A05; 65F99; 93A15; 93C30,Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/JGGZE2US/Zimmermann_2019_Manifold interpolation and model reduction.pdf}
}

@article{zimmermannManifoldInterpolationModel2019a,
  title = {Manifold Interpolation and Model Reduction},
  author = {Zimmermann, Ralf},
  year = {2019},
  month = sep,
  journal = {arXiv:1902.06502 [cs, math]},
  eprint = {1902.06502},
  primaryclass = {cs, math},
  abstract = {One approach to parametric and adaptive model reduction is via the interpolation of orthogonal bases, subspaces or positive definite system matrices. In all these cases, the sampled inputs stem from matrix sets that feature a geometric structure and thus form so-called matrix manifolds. This work will be featured as a chapter in the upcoming Handbook on Model Order Reduction, (P. Benner, S. Grivet-Talocia, A. Quarteroni, G. Rozza, W. H. A. Schilders, L. M. Silveira, eds, to appear on DE GRUYTER) and reviews the numerical treatment of the most important matrix manifolds that arise in the context of model reduction. Moreover, the principal approaches to data interpolation and Taylor-like extrapolation on matrix manifolds are outlined and complemented by algorithms in pseudo-code.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {15-01; 15A16; 15B10; 15B48; 53-04; 65F60; 41-01; 41A05; 65F99; 93A15; 93C30,Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/DP3GSADH/Zimmermann - 2019 - Manifold interpolation and model reduction.pdf}
}

@article{zimmermannManifoldInterpolationModel2019b,
  title = {Manifold Interpolation and Model Reduction},
  author = {Zimmermann, Ralf},
  year = {2019},
  month = sep,
  journal = {arXiv:1902.06502 [cs, math]},
  eprint = {1902.06502},
  primaryclass = {cs, math},
  abstract = {One approach to parametric and adaptive model reduction is via the interpolation of orthogonal bases, subspaces or positive definite system matrices. In all these cases, the sampled inputs stem from matrix sets that feature a geometric structure and thus form so-called matrix manifolds. This work will be featured as a chapter in the upcoming Handbook on Model Order Reduction, (P. Benner, S. Grivet-Talocia, A. Quarteroni, G. Rozza, W. H. A. Schilders, L. M. Silveira, eds, to appear on DE GRUYTER) and reviews the numerical treatment of the most important matrix manifolds that arise in the context of model reduction. Moreover, the principal approaches to data interpolation and Taylor-like extrapolation on matrix manifolds are outlined and complemented by algorithms in pseudo-code.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {15-01; 15A16; 15B10; 15B48; 53-04; 65F60; 41-01; 41A05; 65F99; 93A15; 93C30,Mathematics - Numerical Analysis},
  file = {/home/yanni/Zotero/storage/NIPWVZMP/Zimmermann - 2019 - Manifold interpolation and model reduction.pdf}
}

@misc{ziogasDeinsumPracticallyOptimal2022,
  title = {Deinsum: {{Practically I}}/{{O Optimal Multilinear Algebra}}},
  shorttitle = {Deinsum},
  author = {Ziogas, Alexandros Nikolaos and Kwasniewski, Grzegorz and {Ben-Nun}, Tal and Schneider, Timo and Hoefler, Torsten},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08301},
  eprint = {arXiv:2206.08301},
  publisher = {{arXiv}},
  abstract = {Multilinear algebra kernel performance on modern massively-parallel systems is determined mainly by data movement. However, deriving data movement-optimal distributed schedules for programs with many high-dimensional inputs is a notoriously hard problem. State-of-the-art libraries rely on heuristics and often fall back to suboptimal tensor folding and BLAS calls. We present Deinsum, an automated framework for distributed multilinear algebra computations expressed in Einstein notation, based on rigorous mathematical tools to address this problem. Our framework automatically derives data movement-optimal tiling and generates corresponding distributed schedules, further optimizing the performance of local computations by increasing their arithmetic intensity. To show the benefits of our approach, we test it on two important tensor kernel classes: Matricized Tensor Times Khatri-Rao Products and Tensor Times Matrix chains. We show performance results and scaling on the Piz Daint supercomputer, with up to 19x speedup over state-of-the-art solutions on 512 nodes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/yanni/Zotero/storage/UPG2J76T/Ziogas et al_2022_Deinsum.pdf}
}

@unpublished{zotero-880,
  type = {Unpublished}
}

@article{zouConsistentSelectionNumber2020,
  title = {Consistent Selection of the Number of Change-Points via Sample-Splitting},
  author = {Zou, Changliang and Wang, Guanghui and Li, Runze},
  year = {2020},
  month = feb,
  journal = {Ann. Statist.},
  volume = {48},
  number = {1},
  issn = {0090-5364},
  doi = {10.1214/19-AOS1814},
  abstract = {In multiple change-point analysis, one of the major challenges is to estimate the number of change-points. Most existing approaches attempt to minimize a Schwarz information criterion which balances a term quantifying model fit with a penalization term accounting for model complexity that increases with the number of change-points and limits overfitting. However, different penalization terms are required to adapt to different contexts of multiple change-point problems and the optimal penalization magnitude usually varies from the model and error distribution. We propose a data-driven selection criterion that is applicable to most kinds of popular change-point detection methods, including binary segmentation and optimal partitioning algorithms. The key idea is to select the number of change-points that minimizes the squared prediction error, which measures the fit of a specified model for a new sample. We develop a cross-validation estimation scheme based on an order-preserved sample-splitting strategy, and establish its asymptotic selection consistency under some mild conditions. Effectiveness of the proposed selection criterion is demonstrated on a variety of numerical experiments and real-data examples.},
  langid = {english},
  file = {/home/yanni/Zotero/storage/VNAJSGUJ/Zou et al. - 2020 - Consistent selection of the number of change-point.pdf}
}
