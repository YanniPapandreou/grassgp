%!TEX root=../article.tex
Dimension reduction is a task of vital importance in the modern day fields of data analysis and scientific computing. Simulations and mathematical models are increasingly important tools in the study of complex physical processes in many scientific and engineering discplines. Unfortunately, complex mathematical models often involve a large number of parameters, leading to increased computational intensity as well as to a degradation in model performance, a phenomenon infamously referred to as \textit{the curse of dimensionality}.

Many statistical/machine learning tasks boil down to approximating some unknown function $f:\R^{d}\rightarrow\R$. In many real world applications the dimension $d$ of the domain is very large and many popular aproaches for estimating $f$, such as \ac{gp} regression, suffer from the aforementioned \textit{curse of dimensionality}. However, suppose that our function $f$ is such that not all of the $d$ input variables are significant, and only a subset $M$ of $\R^{d}$ matters. Learning such a subspace $M$ allows us to perform \textit{model order reduction} which can enable us to emulate the true function $f$ more efficiently. This idea originates in the study of \textit{\acp{as}}\todo{add references to development of actives subspaces in related work section, see Pranay's paper.} \cite{constantineActiveSubspacesEmerging2015a}, where the active subspace can be thought of as the collection of directions along which $f$ is most sensitive to perturbations (on average). This concept will be expanded on in more detail in \cref{sec:background}.

In the original formulation of active subspaces one makes the assumption that there exists a \textbf{single global} dimension reducing subspace and any emulators based on \ac{as} approximations are constructed within this single subspace. In practice such an assumption does not always hold, especially when dealing with highly complex models which can exhibit diverse behaviour across different parameter regimes. This observation naturally motivates one to instead consider finding \textbf{different} low dimensional subspaces which are only valid locally in some subregion of the original input space. This task leads one to consider the formal problem of learning a subspace-valued mapping $P:\R^{d}\rightarrow G(d,n)$, where $G(d,n)$  is the Grassmann manifold of all $n$-dimensional (linear) subspaces of $\R^{d}$. This problem is a member of the more general field of research which deals with the statistical analysis of data lying in a Riemannian manifold. In particular, in this paper we will develop and investigate a probabilistic model based on \acp{gp} which can be used to interpolate a field of Grassmann-valued response variables. I.e.\ given a set of $N$ spatially indexed points in $G(d,n)$, $\{(\mathbf{x}_{i},P(\mathbf{x}_{i}))\}_{i=1}^{N}$, we will develop a probabilistic model capable of making predictions for $P(\tilde{\mathbf{x}})$ at unobserved test points $\tilde{\mathbf{x}}\in\R^{d}$.

