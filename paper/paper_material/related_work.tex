%!TEX root=../article.tex
\todo[inline]{Read these references more closely and expand on them as well as contrasting them with our method.}

The main motivation of this paper stems from the literature surrounding active subspaces. The early development of active subspaces can be attributed to the work of Samarov \cite{samarovExploringRegressionStructure1993}, while more recent developments are due to the works of Constantine et al.\ \cite{constantineActiveSubspacesEmerging2015a,constantineComputingActiveSubspaces2015,constantineActiveSubspaceMethods2014}.

There has been extensive research into the problem of learning a subspaced-valued mapping. These works include non-parametric methods \cite[see also references therein]{yuanLocalPolynomialRegression2012} as well as semi-parametric methods \cite{shiIntrinsicRegressionModels2012}. The approach we investigate in this paper is motivated by that taken in \cite{pigoliKrigingPredictionManifoldvalued2016} where we utilise a \ac{gp} instead of a simple linear regression to allow us to more flexibly discover the spatial dependence of our manifold valued field. The linear model approach taken in \cite{pigoliKrigingPredictionManifoldvalued2016} builds on the work of \cite{thomasfletcherGeodesicRegressionTheory2013}\todo{Read this and make sure this is an accurate statement.}. In \cite{zhangGaussianProcessSubspace2021} another \ac{gp} based probabilistic model is considered for this problem.

There are also several works which lie at the intersection of the study of ridge functions and active subspaces. Such works include \cite{seshadriDimensionReductionGaussian2019,wongEmbeddedRidgeApproximations2020,scillitoePolynomialRidgeFlowfield2021}.

The idea of considering several different low dimensional subspaces instead of one global active subspace is not new and has been investigated in \cite{xiongClusteredActivesubspaceBased2021}.
